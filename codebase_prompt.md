# 项目结构

项目`/media/husrcf/ColorfulSSD/Code/Python/ProjectLily_Z_III/`的目录结构（已排除如`node_modules`等目录）：
```
/media/husrcf/ColorfulSSD/Code/Python/ProjectLily_Z_III//
├── .claude/
│   └── settings.local.json
├── audio_analysis/
│   └── audio_analysis.png
├── backpack/
│   ├── DEBUG_SUMMARY.md
│   ├── FINAL_SUMMARY.md
│   ├── GLOBAL_ENCODER_SUMMARY.md
│   ├── SOLUTION_SUMMARY.md
│   ├── codebase_prompt.md
│   ├── dealer.py
│   ├── inference_backup.py
│   └── modelORI.py
├── checkpoint/
│   ├── audiosr_basic.pt
│   └── audiosr_speech.pt
├── codebase_prompt.md
├── config.yaml
├── convert_trained_model.py
├── data/
│   ├── model.py
│   ├── train/
│   │   ├── high/
│   │   │   ├── 01 - A Thousand Dreams Of You.wav
│   │   │   ├── 01 - Day Dreamer.wav
│   │   │   ├── 01 - Dreaming (2).wav
│   │   │   ├── 01 - Dreaming.wav
│   │   │   ├── 01 - Hot Summer.wav
│   │   │   ├── 01 - MY GOD.wav
│   │   │   ├── 01 - Medley：有谁共鸣-沉默是金.wav
│   │   │   ├── 01 - Opening 风再起时.wav
│   │   │   ├── 01 - Opening-为你钟情.wav
│   │   │   ├── 01 - Overture.wav
│   │   │   ├── 01 - Stand Up.wav
│   │   │   ├── 01 - Tonight and Forever.wav
│   │   │   ├── 01 - マツュマロ.wav
│   │   │   ├── 01 - 一片痴.wav
│   │   │   ├── 01 - 不羁的风.wav
│   │   │   ├── 01 - 为你钟情.wav
│   │   │   ├── 01 - 停止转动(国语).wav
│   │   │   ├── 01 - 全赖有你.wav
│   │   │   ├── 01 - 取暖.wav
│   │   │   ├── 01 - 只怕不再遇上 Live.wav
│   │   │   ├── 01 - 够了.wav
│   │   │   ├── 01 - 始终会行运.wav
│   │   │   ├── 01 - 当爱已成往事.wav
│   │   │   ├── 01 - 当真就好（陈淑桦合唱）.wav
│   │   │   ├── 01 - 情人箭.wav
│   │   │   ├── 01 - 情难再续.wav
│   │   │   ├── 01 - 拒绝再玩 (2).wav
│   │   │   ├── 01 - 拒绝再玩 (3).wav
│   │   │   ├── 01 - 拒绝再玩 (4).wav
│   │   │   ├── 01 - 拒绝再玩.wav
│   │   │   ├── 01 - 有谁共鸣.wav
│   │   │   ├── 01 - 油脂热潮.wav
│   │   │   ├── 01 - 烈火边缘.wav
│   │   │   ├── 01 - 爱慕 (2).wav
│   │   │   ├── 01 - 爱慕.wav
│   │   │   ├── 01 - 爱的凶手 (2).wav
│   │   │   ├── 01 - 爱的凶手.wav
│   │   │   ├── 01 - 由零开始.wav
│   │   │   ├── 01 - 痛心（袁咏仪）.wav
│   │   │   ├── 01 - 红.wav
│   │   │   ├── 01 - 贴身.wav
│   │   │   ├── 01 - 路过蜻蜓.wav
│   │   │   ├── 01 - 这些年来.wav
│   │   │   ├── 01 - 醉死梦生.wav
│   │   │   ├── 01 - 风再起时.wav
│   │   │   ├── 01 - 风继续吹 (2).wav
│   │   │   ├── 01 - 风继续吹.wav
│   │   │   ├── 02 - Hot Summer (2).wav
│   │   │   ├── 02 - Hot Summer.wav
│   │   │   ├── 02 - I miss you much.wav
│   │   │   ├── 02 - Miss You Much.wav
│   │   │   ├── 02 - Monica(Remix).wav
│   │   │   ├── 02 - Monica.wav
│   │   │   ├── 02 - My God (2).wav
│   │   │   ├── 02 - My God.wav
│   │   │   ├── 02 - Stand Up.wav
│   │   │   ├── 02 - We Are All Alone.wav
│   │   │   ├── 02 - 三岁仔.wav
│   │   │   ├── 02 - 上帝.wav
│   │   │   ├── 02 - 为你 (2).wav
│   │   │   ├── 02 - 为你.wav
│   │   │   ├── 02 - 今生今世 (2).wav
│   │   │   ├── 02 - 今生今世.wav
│   │   │   ├── 02 - 你这样恨我.wav
│   │   │   ├── 02 - 侧面.wav
│   │   │   ├── 02 - 侬本多情 (2).wav
│   │   │   ├── 02 - 侬本多情.wav
│   │   │   ├── 02 - 倩女幽魂.wav
│   │   │   ├── 02 - 少女心事.wav
│   │   │   ├── 02 - 左右手.wav
│   │   │   ├── 02 - 当年情.wav
│   │   │   ├── 02 - 当真就好.wav
│   │   │   ├── 02 - 怨男 Live.wav
│   │   │   ├── 02 - 我走我路.wav
│   │   │   ├── 02 - 拒绝再玩.wav
│   │   │   ├── 02 - 无心睡眠 (2).wav
│   │   │   ├── 02 - 无心睡眠.wav
│   │   │   ├── 02 - 枕头.wav
│   │   │   ├── 02 - 梦到内河.wav
│   │   │   ├── 02 - 梦死醉生.wav
│   │   │   ├── 02 - 浣花洗剑录.wav
│   │   │   ├── 02 - 深情相拥（辛晓琪合唱）.wav
│   │   │   ├── 02 - 烈火灯蛾.wav
│   │   │   ├── 02 - 热辣辣.wav
│   │   │   ├── 02 - 痴心的我.wav
│   │   │   ├── 02 - 真相.wav
│   │   │   ├── 02 - 第一次.wav
│   │   │   ├── 02 - 贴身 (2).wav
│   │   │   ├── 02 - 贴身.wav
│   │   │   ├── 02 - 迷惑我.wav
│   │   │   ├── 02 - 那一记耳光 (2).wav
│   │   │   ├── 02 - 那一记耳光.wav
│   │   │   ├── 02 - 风继续吹.wav
│   │   │   ├── 02 - 黑色午夜.wav
│   │   │   ├── 03 - Even Now.wav
│   │   │   ├── 03 - Forever爱你.wav
│   │   │   ├── 03 - H2O.wav
│   │   │   ├── 03 - Love Like Magic.wav
│   │   │   ├── 03 - Medley：恋爱交叉-打开信箱-蓝色忧郁-Monica.wav
│   │   │   ├── 03 - Miss You Much.wav
│   │   │   ├── 03 - Miss you much (2).wav
│   │   │   ├── 03 - Monica.wav
│   │   │   ├── 03 - Stand Up.wav
│   │   │   ├── 03 - 为你钟情.wav
│   │   │   ├── 03 - 以后.wav
│   │   │   ├── 03 - 你在何地 (2).wav
│   │   │   ├── 03 - 你在何地.wav
│   │   │   ├── 03 - 侧面.wav
│   │   │   ├── 03 - 侬本多情.wav
│   │   │   ├── 03 - 分手.wav
│   │   │   ├── 03 - 夜半歌声.wav
│   │   │   ├── 03 - 奔向未来日子.wav
│   │   │   ├── 03 - 宿醉.wav
│   │   │   ├── 03 - 寂寞夜晚.wav
│   │   │   ├── 03 - 寂寞有害.wav
│   │   │   ├── 03 - 左右手(Acoustic Mix).wav
│   │   │   ├── 03 - 左右手.wav
│   │   │   ├── 03 - 当年情.wav
│   │   │   ├── 03 - 情到浓时.wav
│   │   │   ├── 03 - 想你.wav
│   │   │   ├── 03 - 我愿意.wav
│   │   │   ├── 03 - 打开信箱.wav
│   │   │   ├── 03 - 放荡.wav
│   │   │   ├── 03 - 无心睡眠 (2).wav
│   │   │   ├── 03 - 无心睡眠.wav
│   │   │   ├── 03 - 无需要太多.wav
│   │   │   ├── 03 - 春夏秋冬.wav
│   │   │   ├── 03 - 柔情蜜意.wav
│   │   │   ├── 03 - 沈胜衣.wav
│   │   │   ├── 03 - 洁身自爱.wav
│   │   │   ├── 03 - 浣花洗剑录.wav
│   │   │   ├── 03 - 深情相拥（辛晓琪合唱）.wav
│   │   │   ├── 03 - 热辣辣 Live.wav
│   │   │   ├── 03 - 热辣辣.wav
│   │   │   ├── 03 - 爱情路里.wav
│   │   │   ├── 03 - 片段.wav
│   │   │   ├── 03 - 片段（袁咏仪合唱）.wav
│   │   │   ├── 03 - 追.wav
│   │   │   ├── 03 - 透明的你 (2).wav
│   │   │   ├── 03 - 透明的你.wav
│   │   │   ├── 03 - 风继续吹.wav
│   │   │   ├── 04 - Before My Heart Finds Out.wav
│   │   │   ├── 04 - Everybody.wav
│   │   │   ├── 04 - H2O.wav
│   │   │   ├── 04 - Medley：想你-偷情 Live.wav
│   │   │   ├── 04 - Medley：蓝色忧郁-少女心事-不羁的风-Monica.wav
│   │   │   ├── 04 - Stories.wav
│   │   │   ├── 04 - Why.wav
│   │   │   ├── 04 - 不要爱他.wav
│   │   │   ├── 04 - 今生今世.wav
│   │   │   ├── 04 - 侧面 (2).wav
│   │   │   ├── 04 - 侧面.wav
│   │   │   ├── 04 - 偏心.wav
│   │   │   ├── 04 - 共你别离.wav
│   │   │   ├── 04 - 变色龙.wav
│   │   │   ├── 04 - 只怕不再遇上.wav
│   │   │   ├── 04 - 可否多一吻.wav
│   │   │   ├── 04 - 失散的影子.wav
│   │   │   ├── 04 - 寂寞有害.wav
│   │   │   ├── 04 - 愿能比翼飞.wav
│   │   │   ├── 04 - 我(国语).wav
│   │   │   ├── 04 - 我眼中的她.wav
│   │   │   ├── 04 - 打开信箱.wav
│   │   │   ├── 04 - 无形锁扣.wav
│   │   │   ├── 04 - 无心锁扣.wav
│   │   │   ├── 04 - 无需要太多.wav
│   │   │   ├── 04 - 明星.wav
│   │   │   ├── 04 - 暴风一族.wav
│   │   │   ├── 04 - 最冷一天.wav
│   │   │   ├── 04 - 枕头.wav
│   │   │   ├── 04 - 柔情蜜意 (2).wav
│   │   │   ├── 04 - 柔情蜜意.wav
│   │   │   ├── 04 - 没有烟总有花.wav
│   │   │   ├── 04 - 深情相拥.wav
│   │   │   ├── 04 - 爱情离合器.wav
│   │   │   ├── 04 - 爱慕.wav
│   │   │   ├── 04 - 狂野如我 (2).wav
│   │   │   ├── 04 - 狂野如我.wav
│   │   │   ├── 04 - 甜蜜的禁果.wav
│   │   │   ├── 04 - 电风扇.wav
│   │   │   ├── 04 - 痴心的我.wav
│   │   │   ├── 04 - 痴心（袁咏仪）.wav
│   │   │   ├── 04 - 红.wav
│   │   │   ├── 04 - 迷惑我.wav
│   │   │   ├── 04 - 追.wav
│   │   │   ├── 04 - 隐身人.wav
│   │   │   ├── 04 - 雪中情.wav
│   │   │   ├── 04 - 风继续吹(Mix).wav
│   │   │   ├── 04 - 风继续吹.wav
│   │   │   ├── 05 - FROM NOW ON（林忆莲合唱）.wav
│   │   │   ├── 05 - Good Morning Sorrow.wav
│   │   │   ├── 05 - H2O.wav
│   │   │   ├── 05 - Hey!不要玩.wav
│   │   │   ├── 05 - I Honestly Love You.wav
│   │   │   ├── 05 - Medley：H2O-黑色午夜-隐身人-第一次-Stand Up.wav
│   │   │   ├── 05 - 一片痴 (2).wav
│   │   │   ├── 05 - 一片痴.wav
│   │   │   ├── 05 - 从未可以.wav
│   │   │   ├── 05 - 侧面.wav
│   │   │   ├── 05 - 共同渡过.wav
│   │   │   ├── 05 - 到未来日子 (2).wav
│   │   │   ├── 05 - 到未来日子.wav
│   │   │   ├── 05 - 夜半歌声(电影版).wav
│   │   │   ├── 05 - 大亨.wav
│   │   │   ├── 05 - 大报复.wav
│   │   │   ├── 05 - 奔向未来日子.wav
│   │   │   ├── 05 - 妄想 (2).wav
│   │   │   ├── 05 - 妄想.wav
│   │   │   ├── 05 - 宿醉.wav
│   │   │   ├── 05 - 当年情 (2).wav
│   │   │   ├── 05 - 当年情.wav
│   │   │   ├── 05 - 当爱已成往事.wav
│   │   │   ├── 05 - 心中情.wav
│   │   │   ├── 05 - 心跳呼吸正常.wav
│   │   │   ├── 05 - 恋爱交叉.wav
│   │   │   ├── 05 - 我愿意.wav
│   │   │   ├── 05 - 拒绝再玩.wav
│   │   │   ├── 05 - 最爱.wav
│   │   │   ├── 05 - 月亮代表我的心(Live).wav
│   │   │   ├── 05 - 有心人.wav
│   │   │   ├── 05 - 深情相拥 Live.wav
│   │   │   ├── 05 - 燕子的故事.wav
│   │   │   ├── 05 - 爱慕 (2).wav
│   │   │   ├── 05 - 爱慕 (3).wav
│   │   │   ├── 05 - 爱慕.wav
│   │   │   ├── 05 - 由零开始.wav
│   │   │   ├── 05 - 绝不可以.wav
│   │   │   ├── 05 - 缘份.wav
│   │   │   ├── 05 - 缘份有几多 (2).wav
│   │   │   ├── 05 - 缘份有几多.wav
│   │   │   ├── 05 - 缘份（梅艳芳合唱）.wav
│   │   │   ├── 05 - 蓝色忧郁.wav
│   │   │   ├── 05 - 被爱.wav
│   │   │   ├── 05 - 谁令你心痴（陈洁灵合唱）.wav
│   │   │   ├── 05 - 陪你倒数.wav
│   │   │   ├── 05 - 需要你.wav
│   │   │   ├── 06 - FROM NOW ON.wav
│   │   │   ├── 06 - H2O Medley：H2O-少女心事-第一次-不羁的风.wav
│   │   │   ├── 06 - Medley：童年时-似水流年-但愿人长久.wav
│   │   │   ├── 06 - Undercover Angel.wav
│   │   │   ├── 06 - 一片痴.wav
│   │   │   ├── 06 - 一辈子失去了妳.wav
│   │   │   ├── 06 - 为你.wav
│   │   │   ├── 06 - 为你钟情.wav
│   │   │   ├── 06 - 今生今世.wav
│   │   │   ├── 06 - 侧面.wav
│   │   │   ├── 06 - 全身都是爱.wav
│   │   │   ├── 06 - 共同渡过 (2).wav
│   │   │   ├── 06 - 共同渡过 (3).wav
│   │   │   ├── 06 - 共同渡过.wav
│   │   │   ├── 06 - 天使之爱 (2).wav
│   │   │   ├── 06 - 天使之爱.wav
│   │   │   ├── 06 - 小明星.wav
│   │   │   ├── 06 - 当年情.wav
│   │   │   ├── 06 - 情自困.wav
│   │   │   ├── 06 - 想你 (2).wav
│   │   │   ├── 06 - 想你.wav
│   │   │   ├── 06 - 拒绝再玩.wav
│   │   │   ├── 06 - 无需要太多.wav
│   │   │   ├── 06 - 暴风一族 (2).wav
│   │   │   ├── 06 - 暴风一族.wav
│   │   │   ├── 06 - 柔情蜜意 (2).wav
│   │   │   ├── 06 - 柔情蜜意.wav
│   │   │   ├── 06 - 沉默是金.wav
│   │   │   ├── 06 - 流浪 (2).wav
│   │   │   ├── 06 - 流浪.wav
│   │   │   ├── 06 - 深情相拥（辛晓琪合唱）.wav
│   │   │   ├── 06 - 爱情离合器.wav
│   │   │   ├── 06 - 爱慕.wav
│   │   │   ├── 06 - 爱的抉择.wav
│   │   │   ├── 06 - 真相.wav
│   │   │   ├── 06 - 第一次.wav
│   │   │   ├── 06 - 谈情说爱 Live.wav
│   │   │   ├── 06 - 这刻相见后.wav
│   │   │   ├── 06 - 追族.wav
│   │   │   ├── 06 - 追逐.wav
│   │   │   ├── 06 - 风继续吹.wav
│   │   │   ├── 06 - 黑色午夜.wav
│   │   │   ├── 07 - I Like Dreamin'.wav
│   │   │   ├── 07 - Medley：红颜白发-最爱 Live.wav
│   │   │   ├── 07 - Miss You Much.wav
│   │   │   ├── 07 - Monica (2).wav
│   │   │   ├── 07 - 不怕寂寞.wav
│   │   │   ├── 07 - 不羁的风.wav
│   │   │   ├── 07 - 为你钟情.wav
│   │   │   ├── 07 - 作伴.wav
│   │   │   ├── 07 - 你教我点好.wav
│   │   │   ├── 07 - 侬本多情 (2).wav
│   │   │   ├── 07 - 侬本多情.wav
│   │   │   ├── 07 - 偷情.wav
│   │   │   ├── 07 - 共同渡过 (2).wav
│   │   │   ├── 07 - 共同渡过.wav
│   │   │   ├── 07 - 千千阕歌.wav
│   │   │   ├── 07 - 同道中人.wav
│   │   │   ├── 07 - 夜半歌声.wav
│   │   │   ├── 07 - 奔向未来日子.wav
│   │   │   ├── 07 - 寂寞夜晚.wav
│   │   │   ├── 07 - 少女心事.wav
│   │   │   ├── 07 - 恋爱交叉.wav
│   │   │   ├── 07 - 情人箭.wav
│   │   │   ├── 07 - 情难自控.wav
│   │   │   ├── 07 - 我愿意.wav
│   │   │   ├── 07 - 打开信箱 (2).wav
│   │   │   ├── 07 - 打开信箱.wav
│   │   │   ├── 07 - 放荡.wav
│   │   │   ├── 07 - 无心睡眠.wav
│   │   │   ├── 07 - 有谁共鸣.wav
│   │   │   ├── 07 - 烧毁我眼睛.wav
│   │   │   ├── 07 - 爱情路里.wav
│   │   │   ├── 07 - 留住昨天.wav
│   │   │   ├── 07 - 眉来眼去（辛晓琪合唱）.wav
│   │   │   ├── 07 - 继续跳舞.wav
│   │   │   ├── 07 - 触电.wav
│   │   │   ├── 07 - 让我飞 (2).wav
│   │   │   ├── 07 - 让我飞.wav
│   │   │   ├── 07 - 路过蜻蜒.wav
│   │   │   ├── 07 - 追.wav
│   │   │   ├── 07 - 野火.wav
│   │   │   ├── 07 - 风再起时.wav
│   │   │   ├── 07 - 风继续吹.wav
│   │   │   ├── 07 - 黑色午夜.wav
│   │   │   ├── 08 - H2O.wav
│   │   │   ├── 08 - I Need You.wav
│   │   │   ├── 08 - Stand Up-Twist & Shout-Stand Up.wav
│   │   │   ├── 08 - 一辈子失去了妳.wav
│   │   │   ├── 08 - 不怕寂寞.wav
│   │   │   ├── 08 - 不管您是谁.wav
│   │   │   ├── 08 - 不羁的风.wav
│   │   │   ├── 08 - 不要爱他.wav
│   │   │   ├── 08 - 为妳钟情.wav
│   │   │   ├── 08 - 你是我一半.wav
│   │   │   ├── 08 - 你的一切.wav
│   │   │   ├── 08 - 侧面-放荡.wav
│   │   │   ├── 08 - 侬本多情.wav
│   │   │   ├── 08 - 兜风心情.wav
│   │   │   ├── 08 - 兜风心情（柏安妮合唱）.wav
│   │   │   ├── 08 - 共同渡过.wav
│   │   │   ├── 08 - 刻骨铭心.wav
│   │   │   ├── 08 - 取暖.wav
│   │   │   ├── 08 - 够了 (2).wav
│   │   │   ├── 08 - 够了.wav
│   │   │   ├── 08 - 妳教我点好.wav
│   │   │   ├── 08 - 客途秋恨.wav
│   │   │   ├── 08 - 寂寞夜晚.wav
│   │   │   ├── 08 - 少女心事.wav
│   │   │   ├── 08 - 我愿意.wav
│   │   │   ├── 08 - 无心睡眠.wav
│   │   │   ├── 08 - 明星 Live.wav
│   │   │   ├── 08 - 明星.wav
│   │   │   ├── 08 - 暴风一族.wav
│   │   │   ├── 08 - 最爱.wav
│   │   │   ├── 08 - 有心人 (2).wav
│   │   │   ├── 08 - 有心人.wav
│   │   │   ├── 08 - 浓情.wav
│   │   │   ├── 08 - 烈火边缘.wav
│   │   │   ├── 08 - 热辣辣.wav
│   │   │   ├── 08 - 爱有万万千.wav
│   │   │   ├── 08 - 爱火.wav
│   │   │   ├── 08 - 痴心的我.wav
│   │   │   ├── 08 - 眉来眼去.wav
│   │   │   ├── 08 - 眉来眼去（辛晓琪合唱）.wav
│   │   │   ├── 08 - 知道爱.wav
│   │   │   ├── 08 - 继续跳舞.wav
│   │   │   ├── 08 - 请勿越轨.wav
│   │   │   ├── 08 - 谁负了谁.wav
│   │   │   ├── 08 - 难以再说对不起.wav
│   │   │   ├── 09 - Medley：阿飞正传-梦-A Thousand Dreams of You.wav
│   │   │   ├── 09 - Monica.wav
│   │   │   ├── 09 - The Way We Were.wav
│   │   │   ├── 09 - WHY.wav
│   │   │   ├── 09 - You Made Me Believe In Magic.wav
│   │   │   ├── 09 - 七色的爱.wav
│   │   │   ├── 09 - 三岁仔.wav
│   │   │   ├── 09 - 不怕寂寞.wav
│   │   │   ├── 09 - 不管你是谁.wav
│   │   │   ├── 09 - 不羁的风.wav
│   │   │   ├── 09 - 为你钟情.wav
│   │   │   ├── 09 - 你在何地.wav
│   │   │   ├── 09 - 倩女幽魂 (2).wav
│   │   │   ├── 09 - 内心争斗.wav
│   │   │   ├── 09 - 别话.wav
│   │   │   ├── 09 - 妒忌.wav
│   │   │   ├── 09 - 寂寞猎人.wav
│   │   │   ├── 09 - 找一个地方.wav
│   │   │   ├── 09 - 无需要太多 (2).wav
│   │   │   ├── 09 - 无需要太多.wav
│   │   │   ├── 09 - 暴风一族.wav
│   │   │   ├── 09 - 有心人.wav
│   │   │   ├── 09 - 枕头.wav
│   │   │   ├── 09 - 死心（袁咏仪）.wav
│   │   │   ├── 09 - 沉默是金.wav
│   │   │   ├── 09 - 爱慕.wav
│   │   │   ├── 09 - 爱有万万千.wav
│   │   │   ├── 09 - 爱火.wav
│   │   │   ├── 09 - 由零开始.wav
│   │   │   ├── 09 - 电风扇.wav
│   │   │   ├── 09 - 禁片.wav
│   │   │   ├── 09 - 红 Live.wav
│   │   │   ├── 09 - 红颜白发.wav
│   │   │   ├── 09 - 胭脂扣.wav
│   │   │   ├── 09 - 蓝色忧郁 (2).wav
│   │   │   ├── 09 - 蓝色忧郁.wav
│   │   │   ├── 09 - 请勿越轨 (2).wav
│   │   │   ├── 09 - 请勿越轨.wav
│   │   │   ├── 09 - 谈恋爱.wav
│   │   │   ├── 09 - 谈情说爱.wav
│   │   │   ├── 09 - 闯进新领域.wav
│   │   │   ├── 09 - 陪你倒数.wav
│   │   │   ├── 09 - 难以再说对不起.wav
│   │   │   ├── 09 - 默默向上游(Mix).wav
│   │   │   ├── 09 - 默默向上游.wav
│   │   │   ├── 10 - American Pie.wav
│   │   │   ├── 10 - CRAZY ROCK.wav
│   │   │   ├── 10 - I Honestly Love You.wav
│   │   │   ├── 10 - Just The Way You Are.wav
│   │   │   ├── 10 - Medley：啼笑姻缘-当爱已成往事-啼笑姻缘.wav
│   │   │   ├── 10 - Thank You.wav
│   │   │   ├── 10 - 一片痴.wav
│   │   │   ├── 10 - 为你钟情 Live.wav
│   │   │   ├── 10 - 人生的鼓手 (2).wav
│   │   │   ├── 10 - 人生的鼓手.wav
│   │   │   ├── 10 - 何去何从之阿飞正传.wav
│   │   │   ├── 10 - 你是明星(国).wav
│   │   │   ├── 10 - 倩女幽魂 (2).wav
│   │   │   ├── 10 - 倩女幽魂 (4).wav
│   │   │   ├── 10 - 全赖有你.wav
│   │   │   ├── 10 - 再恋.wav
│   │   │   ├── 10 - 刻骨铭心.wav
│   │   │   ├── 10 - 可人儿.wav
│   │   │   ├── 10 - 尽情地爱.wav
│   │   │   ├── 10 - 心跳呼吸正常.wav
│   │   │   ├── 10 - 想你 (2).wav
│   │   │   ├── 10 - 想你.wav
│   │   │   ├── 10 - 拒绝再玩.wav
│   │   │   ├── 10 - 无心睡眠.wav
│   │   │   ├── 10 - 无需要太多.wav
│   │   │   ├── 10 - 最爱.wav
│   │   │   ├── 10 - 有心人.wav
│   │   │   ├── 10 - 未来之歌.wav
│   │   │   ├── 10 - 柔情蜜意.wav
│   │   │   ├── 10 - 棉花糖.wav
│   │   │   ├── 10 - 油脂热潮.wav
│   │   │   ├── 10 - 爱的狂徒 (2).wav
│   │   │   ├── 10 - 眉来眼去（辛晓琪合唱）.wav
│   │   │   ├── 10 - 缘份.wav
│   │   │   ├── 10 - 蓝色忧郁.wav
│   │   │   ├── 10 - 被爱.wav
│   │   │   ├── 10 - 谁令你心痴（陈洁灵合唱）.wav
│   │   │   ├── 10 - 雨中的浪漫.wav
│   │   │   ├── 10 - 风再起时.wav
│   │   │   ├── 10 - 黑色午夜(Remix).wav
│   │   │   ├── 11 - A Little Bit More.wav
│   │   │   ├── 11 - Every body.wav
│   │   │   ├── 11 - H2O.wav
│   │   │   ├── 11 - MIRACLE（麦洁文合唱）.wav
│   │   │   ├── 11 - マツュマロ.wav
│   │   │   ├── 11 - 为你钟情.wav
│   │   │   ├── 11 - 何去何从之阿飞正传.wav
│   │   │   ├── 11 - 全世界只想你来爱我(国).wav
│   │   │   ├── 11 - 全身都是爱.wav
│   │   │   ├── 11 - 分手.wav
│   │   │   ├── 11 - 大亨.wav
│   │   │   ├── 11 - 少女心事.wav
│   │   │   ├── 11 - 左右手.wav
│   │   │   ├── 11 - 怪你过分美丽.wav
│   │   │   ├── 11 - 恋爱交叉.wav
│   │   │   ├── 11 - 情难自控.wav
│   │   │   ├── 11 - 无心睡眠.wav
│   │   │   ├── 11 - 明月夜 (2).wav
│   │   │   ├── 11 - 明月夜.wav
│   │   │   ├── 11 - 暴风一族.wav
│   │   │   ├── 11 - 月亮代表我的心 Live.wav
│   │   │   ├── 11 - 柔情蜜意.wav
│   │   │   ├── 11 - 永远记得.wav
│   │   │   ├── 11 - 沈胜衣.wav
│   │   │   ├── 11 - 烈火边缘.wav
│   │   │   ├── 11 - 至少还有你.wav
│   │   │   ├── 11 - 访英台.wav
│   │   │   ├── 11 - 谈情说爱.wav
│   │   │   ├── 11 - 迷惑我.wav
│   │   │   ├── 11 - 隐身人.wav
│   │   │   ├── 11 - 默默向上游 (2).wav
│   │   │   ├── 11 - 默默向上游 (3).wav
│   │   │   ├── 11 - 默默向上游.wav
│   │   │   ├── 12 - Love like magic.wav
│   │   │   ├── 12 - Pistol Packin' Melody.wav
│   │   │   ├── 12 - 一盏小明灯.wav
│   │   │   ├── 12 - 倩女幽魂.wav
│   │   │   ├── 12 - 共同渡过.wav
│   │   │   ├── 12 - 分手.wav
│   │   │   ├── 12 - 够了.wav
│   │   │   ├── 12 - 大报复.wav
│   │   │   ├── 12 - 奔向未来日子.wav
│   │   │   ├── 12 - 始终会行运.wav
│   │   │   ├── 12 - 守住风口 (2).wav
│   │   │   ├── 12 - 守住风口.wav
│   │   │   ├── 12 - 少女心事.wav
│   │   │   ├── 12 - 当年情.wav
│   │   │   ├── 12 - 我(国).wav
│   │   │   ├── 12 - 我愿意.wav
│   │   │   ├── 12 - 我要逆风去 (2).wav
│   │   │   ├── 12 - 我要逆风去.wav
│   │   │   ├── 12 - 无心睡眠.wav
│   │   │   ├── 12 - 没有爱.wav
│   │   │   ├── 12 - 爱火.wav
│   │   │   ├── 12 - 第一次.wav
│   │   │   ├── 12 - 谈恋爱.wav
│   │   │   ├── 12 - 谈情说爱 (2).wav
│   │   │   ├── 12 - 谈情说爱.wav
│   │   │   ├── 12 - 贴身.wav
│   │   │   ├── 12 - 迷路.wav
│   │   │   ├── 12 - 追 Live.wav
│   │   │   ├── 12 - 风继续吹 (2).wav
│   │   │   ├── 12 - 风继续吹.wav
│   │   │   ├── 13 - A Thousand Dreams Of You.wav
│   │   │   ├── 13 - Twist & Shout.wav
│   │   │   ├── 13 - 作伴.wav
│   │   │   ├── 13 - 侧面.wav
│   │   │   ├── 13 - 共同度过.wav
│   │   │   ├── 13 - 共同渡过.wav
│   │   │   ├── 13 - 最爱.wav
│   │   │   ├── 13 - 燕子的故事.wav
│   │   │   ├── 13 - 片段.wav
│   │   │   ├── 13 - 痴心的我.wav
│   │   │   ├── 13 - 第一次 (2).wav
│   │   │   ├── 13 - 第一次.wav
│   │   │   ├── 13 - 路过蜻蜓.wav
│   │   │   ├── 13 - 野火 (2).wav
│   │   │   ├── 13 - 野火.wav
│   │   │   ├── 13 - 陪你倒数.wav
│   │   │   ├── 13 - 隐身人.wav
│   │   │   ├── 14 - Daydreamer.wav
│   │   │   ├── 14 - Stand Up.wav
│   │   │   ├── 14 - 共同度过.wav
│   │   │   ├── 14 - 分手.wav
│   │   │   ├── 14 - 失散的影子 (2).wav
│   │   │   ├── 14 - 失散的影子.wav
│   │   │   ├── 14 - 当爱已成往事.wav
│   │   │   ├── 14 - 无心睡眠.wav
│   │   │   ├── 14 - 沉默是金(独唱版).wav
│   │   │   ├── 14 - 爱情路里.wav
│   │   │   ├── 14 - 迷惑我.wav
│   │   │   ├── 14 - 迷路.wav
│   │   │   ├── 14 - 默默向上游.wav
│   │   │   ├── 15 - H2O.wav
│   │   │   ├── 15 - I Need You.wav
│   │   │   ├── 15 - 为你钟情.wav
│   │   │   ├── 15 - 侬本多情.wav
│   │   │   ├── 15 - 全赖有你.wav
│   │   │   ├── 15 - 只怕不再遇上（陈洁灵合唱）.wav
│   │   │   ├── 15 - 恋爱交叉.wav
│   │   │   ├── 15 - 惊梦 (2).wav
│   │   │   ├── 15 - 惊梦.wav
│   │   │   ├── 15 - 我的心里没有他.wav
│   │   │   ├── 15 - 无胆入情关.wav
│   │   │   ├── 15 - 红颜白发.wav
│   │   │   ├── 15 - 风再起时 (2).wav
│   │   │   ├── 16 - Everybody.wav
│   │   │   ├── 16 - I Like Dreamin'.wav
│   │   │   ├── 16 - 一盏小明灯.wav
│   │   │   ├── 16 - 倩女幽魂.wav
│   │   │   ├── 16 - 热情的沙漠.wav
│   │   │   ├── 16 - 背着命运(国语).wav
│   │   │   ├── 16 - 闯进新领域.wav
│   │   │   ├── 16 - 风再起时.wav
│   │   │   ├── 17 - Love Like Magic.wav
│   │   │   ├── 17 - 不确定的年纪(国语).wav
│   │   │   ├── 17 - 大热.wav
│   │   │   ├── 17 - 当年情(国语).wav
│   │   │   ├── 17 - 有谁共鸣.wav
│   │   │   ├── 18 - 沉默是金.wav
│   │   │   ├── 18 - 痴心的我(国语).wav
│   │   │   ├── SSB0005.wav
│   │   │   ├── SSB0009.wav
│   │   │   ├── SSB0011.wav
│   │   │   ├── SSB0012.wav
│   │   │   ├── SSB0016.wav
│   │   │   ├── SSB0018.wav
│   │   │   ├── SSB0038.wav
│   │   │   ├── SSB0043.wav
│   │   │   ├── SSB0057.wav
│   │   │   ├── SSB0073.wav
│   │   │   ├── SSB0080.wav
│   │   │   ├── SSB0112.wav
│   │   │   ├── SSB0122.wav
│   │   │   ├── SSB0133.wav
│   │   │   ├── SSB0145.wav
│   │   │   ├── SSB0149.wav
│   │   │   ├── SSB0193.wav
│   │   │   ├── SSB0197.wav
│   │   │   ├── SSB0200.wav
│   │   │   ├── SSB0241.wav
│   │   │   ├── SSB0246.wav
│   │   │   ├── SSB0261.wav
│   │   │   ├── SSB0267.wav
│   │   │   ├── SSB0273.wav
│   │   │   ├── SSB0287.wav
│   │   │   ├── SSB0288.wav
│   │   │   ├── SSB0299.wav
│   │   │   ├── SSB0307.wav
│   │   │   ├── SSB0309.wav
│   │   │   ├── SSB0315.wav
│   │   │   ├── SSB0316.wav
│   │   │   ├── SSB0323.wav
│   │   │   ├── SSB0338.wav
│   │   │   ├── SSB0339.wav
│   │   │   ├── SSB0341.wav
│   │   │   ├── SSB0342.wav
│   │   │   ├── SSB0354.wav
│   │   │   ├── SSB0366.wav
│   │   │   ├── SSB0375.wav
│   │   │   ├── SSB0379.wav
│   │   │   ├── SSB0380.wav
│   │   │   ├── SSB0382.wav
│   │   │   ├── SSB0385.wav
│   │   │   ├── SSB0393.wav
│   │   │   ├── SSB0394.wav
│   │   │   ├── SSB0395.wav
│   │   │   ├── SSB0407.wav
│   │   │   ├── SSB0415.wav
│   │   │   ├── SSB0426.wav
│   │   │   ├── SSB0427.wav
│   │   │   ├── SSB0434.wav
│   │   │   ├── SSB0435.wav
│   │   │   ├── SSB0470.wav
│   │   │   ├── SSB0482.wav
│   │   │   ├── SSB0502.wav
│   │   │   ├── SSB0534.wav
│   │   │   ├── SSB0535.wav
│   │   │   ├── SSB0539.wav
│   │   │   ├── SSB0544.wav
│   │   │   ├── SSB0565.wav
│   │   │   ├── SSB0570.wav
│   │   │   ├── SSB0578.wav
│   │   │   ├── SSB0588.wav
│   │   │   ├── SSB0590.wav
│   │   │   ├── SSB0594.wav
│   │   │   ├── SSB0599.wav
│   │   │   ├── SSB0601.wav
│   │   │   ├── SSB0603.wav
│   │   │   ├── SSB0606.wav
│   │   │   ├── SSB0607.wav
│   │   │   ├── SSB0614.wav
│   │   │   ├── SSB0623.wav
│   │   │   ├── SSB0629.wav
│   │   │   ├── SSB0631.wav
│   │   │   ├── SSB0632.wav
│   │   │   ├── SSB0666.wav
│   │   │   ├── SSB0668.wav
│   │   │   ├── SSB0671.wav
│   │   │   ├── SSB0686.wav
│   │   │   ├── SSB0700.wav
│   │   │   ├── SSB0710.wav
│   │   │   ├── SSB0720.wav
│   │   │   ├── SSB0723.wav
│   │   │   ├── SSB0746.wav
│   │   │   ├── SSB0751.wav
│   │   │   ├── SSB0758.wav
│   │   │   ├── SSB0760.wav
│   │   │   ├── SSB0762.wav
│   │   │   ├── SSB0778.wav
│   │   │   ├── SSB0780.wav
│   │   │   ├── SSB0784.wav
│   │   │   ├── SSB0786.wav
│   │   │   ├── SSB0794.wav
│   │   │   ├── SSB0817.wav
│   │   │   ├── SSB0851.wav
│   │   │   ├── SSB0863.wav
│   │   │   ├── SSB0871.wav
│   │   │   ├── SSB0887.wav
│   │   │   ├── SSB0913.wav
│   │   │   ├── SSB0915.wav
│   │   │   ├── SSB0935.wav
│   │   │   ├── SSB0966.wav
│   │   │   ├── SSB0987.wav
│   │   │   ├── SSB1008.wav
│   │   │   ├── SSB1020.wav
│   │   │   ├── SSB1024.wav
│   │   │   ├── SSB1050.wav
│   │   │   ├── SSB1055.wav
│   │   │   ├── SSB1056.wav
│   │   │   ├── SSB1064.wav
│   │   │   ├── SSB1072.wav
│   │   │   ├── SSB1091.wav
│   │   │   ├── SSB1096.wav
│   │   │   ├── SSB1100.wav
│   │   │   ├── SSB1108.wav
│   │   │   ├── SSB1115.wav
│   │   │   ├── SSB1125.wav
│   │   │   ├── SSB1131.wav
│   │   │   ├── SSB1136.wav
│   │   │   ├── SSB1138.wav
│   │   │   ├── SSB1161.wav
│   │   │   ├── SSB1203.wav
│   │   │   ├── SSB1204.wav
│   │   │   ├── SSB1218.wav
│   │   │   ├── SSB1221.wav
│   │   │   ├── SSB1253.wav
│   │   │   ├── SSB1320.wav
│   │   │   ├── SSB1341.wav
│   │   │   ├── SSB1366.wav
│   │   │   ├── SSB1377.wav
│   │   │   ├── SSB1383.wav
│   │   │   ├── SSB1385.wav
│   │   │   ├── SSB1392.wav
│   │   │   ├── SSB1393.wav
│   │   │   ├── SSB1408.wav
│   │   │   ├── SSB1431.wav
│   │   │   ├── SSB1437.wav
│   │   │   ├── SSB1448.wav
│   │   │   ├── SSB1555.wav
│   │   │   ├── SSB1563.wav
│   │   │   ├── SSB1567.wav
│   │   │   ├── SSB1575.wav
│   │   │   ├── SSB1585.wav
│   │   │   ├── SSB1593.wav
│   │   │   ├── SSB1607.wav
│   │   │   ├── SSB1624.wav
│   │   │   ├── SSB1625.wav
│   │   │   ├── SSB1630.wav
│   │   │   ├── SSB1650.wav
│   │   │   ├── SSB1670.wav
│   │   │   ├── SSB1684.wav
│   │   │   ├── SSB1686.wav
│   │   │   ├── SSB1699.wav
│   │   │   ├── SSB1711.wav
│   │   │   ├── SSB1759.wav
│   │   │   ├── SSB1806.wav
│   │   │   ├── SSB1831.wav
│   │   │   ├── SSB1832.wav
│   │   │   ├── SSB1837.wav
│   │   │   ├── SSB1846.wav
│   │   │   ├── SSB1863.wav
│   │   │   ├── SSB1878.wav
│   │   │   ├── SSB1891.wav
│   │   │   ├── SSB1918.wav
│   │   │   ├── SSB1935.wav
│   │   │   ├── SSB1939.wav
│   │   │   ├── p225.wav
│   │   │   ├── p226.wav
│   │   │   ├── p227.wav
│   │   │   ├── p228.wav
│   │   │   ├── p229.wav
│   │   │   ├── p231.wav
│   │   │   ├── p232.wav
│   │   │   ├── p233.wav
│   │   │   ├── p234.wav
│   │   │   ├── p236.wav
│   │   │   ├── p237.wav
│   │   │   ├── p238.wav
│   │   │   ├── p239.wav
│   │   │   ├── p240.wav
│   │   │   ├── p241.wav
│   │   │   ├── p243.wav
│   │   │   ├── p244.wav
│   │   │   ├── p245.wav
│   │   │   ├── p246.wav
│   │   │   ├── p247.wav
│   │   │   ├── p248.wav
│   │   │   ├── p249.wav
│   │   │   ├── p250.wav
│   │   │   ├── p251.wav
│   │   │   ├── p252.wav
│   │   │   ├── p253.wav
│   │   │   ├── p255.wav
│   │   │   ├── p256.wav
│   │   │   ├── p258.wav
│   │   │   ├── p259.wav
│   │   │   ├── p260.wav
│   │   │   ├── p261.wav
│   │   │   ├── p262.wav
│   │   │   ├── p263.wav
│   │   │   ├── p264.wav
│   │   │   ├── p265.wav
│   │   │   ├── p266.wav
│   │   │   ├── p267.wav
│   │   │   ├── p268.wav
│   │   │   ├── p269.wav
│   │   │   ├── p270.wav
│   │   │   ├── p272.wav
│   │   │   ├── p273.wav
│   │   │   ├── p274.wav
│   │   │   ├── p275.wav
│   │   │   ├── p276.wav
│   │   │   ├── p277.wav
│   │   │   ├── p278.wav
│   │   │   ├── p279.wav
│   │   │   ├── p280.wav
│   │   │   ├── p281.wav
│   │   │   ├── p282.wav
│   │   │   ├── p283.wav
│   │   │   ├── p284.wav
│   │   │   ├── p285.wav
│   │   │   ├── p286.wav
│   │   │   ├── p287.wav
│   │   │   ├── p288.wav
│   │   │   ├── p292.wav
│   │   │   ├── p293.wav
│   │   │   ├── p294.wav
│   │   │   ├── p295.wav
│   │   │   ├── p297.wav
│   │   │   ├── p298.wav
│   │   │   ├── p299.wav
│   │   │   ├── p300.wav
│   │   │   ├── p301.wav
│   │   │   ├── p302.wav
│   │   │   ├── p303.wav
│   │   │   ├── p304.wav
│   │   │   ├── p305.wav
│   │   │   ├── p306.wav
│   │   │   ├── p307.wav
│   │   │   ├── p308.wav
│   │   │   ├── p310.wav
│   │   │   ├── p312.wav
│   │   │   ├── p313.wav
│   │   │   ├── p314.wav
│   │   │   ├── p315.wav
│   │   │   ├── p316.wav
│   │   │   ├── p317.wav
│   │   │   ├── p318.wav
│   │   │   ├── p323.wav
│   │   │   ├── p326.wav
│   │   │   ├── p329.wav
│   │   │   ├── p330.wav
│   │   │   ├── p333.wav
│   │   │   ├── p334.wav
│   │   │   ├── p335.wav
│   │   │   ├── p336.wav
│   │   │   ├── p339.wav
│   │   │   ├── p340.wav
│   │   │   ├── p341.wav
│   │   │   ├── p345.wav
│   │   │   ├── p347.wav
│   │   │   ├── p351.wav
│   │   │   ├── p360.wav
│   │   │   ├── p361.wav
│   │   │   ├── p362.wav
│   │   │   ├── p363.wav
│   │   │   ├── p364.wav
│   │   │   ├── p374.wav
│   │   │   ├── p376.wav
│   │   │   ├── s5.wav
│   │   │   ├── 张国荣-A SONG FOR YOU.wav
│   │   │   ├── 张国荣-DREAMING My Other Half.wav
│   │   │   ├── 张国荣-Don't Lie To Me.wav
│   │   │   ├── 张国荣-Dreaming (Remix).wav
│   │   │   ├── 张国荣-Hot Summer (Live).wav
│   │   │   ├── 张国荣-Hot Summer (Remix).wav
│   │   │   ├── 张国荣-I Honestly Love You.wav
│   │   │   ├── 张国荣-MISS YOU MUCH Missing you.wav
│   │   │   ├── 张国荣-Medley ： 不怕寂寞 、 我愿意 (Live).wav
│   │   │   ├── 张国荣-Medley ： 抵抗夜寒、 黑色午夜 、 热辣辣 (Live).wav
│   │   │   ├── 张国荣-Medley ： 有谁共鸣 、 沉默是金 (Live).wav
│   │   │   ├── 张国荣-Medley ： 童年时 、 似水流年、 但愿人长久 (Live).wav
│   │   │   ├── 张国荣-Medley ： 蓝色忧郁 、 少女心事 、 不羁的风 、 Monica (Live).wav
│   │   │   ├── 张国荣-Medley： H2O 、黑色午夜、 隐身人、第一次、Stand Up (Live).wav
│   │   │   ├── 张国荣-Miss You Much (Live).wav
│   │   │   ├── 张国荣-Miss You Much (Remix).wav
│   │   │   ├── 张国荣-Monica (Live).wav
│   │   │   ├── 张国荣-Opening 、 贴身 (Live).wav
│   │   │   ├── 张国荣-PROLOGUE.wav
│   │   │   ├── 张国荣-Stories (Live).wav
│   │   │   ├── 张国荣-The Way We Were (Live).wav
│   │   │   ├── 张国荣-Why (Remix).wav
│   │   │   ├── 张国荣-不怕寂寞 (Live).wav
│   │   │   ├── 张国荣-不想拥抱我的人.wav
│   │   │   ├── 张国荣-不羁的风 (Live).wav
│   │   │   ├── 张国荣-为你钟情 (Live).wav
│   │   │   ├── 张国荣-你在何地 (Live).wav
│   │   │   ├── 张国荣-你我之间.wav
│   │   │   ├── 张国荣-侧面 (Live).wav
│   │   │   ├── 张国荣-侧面 (Remix).wav
│   │   │   ├── 张国荣-侧面 Silhouette.wav
│   │   │   ├── 张国荣-侬本多情 (Live).wav
│   │   │   ├── 张国荣-侯斯顿之恋.wav
│   │   │   ├── 张国荣-倩女幽魂 (Live) (2).wav
│   │   │   ├── 张国荣-倩女幽魂 (Live).wav
│   │   │   ├── 张国荣-偏心 (Live).wav
│   │   │   ├── 张国荣-偷情.wav
│   │   │   ├── 张国荣-共创真善美.wav
│   │   │   ├── 张国荣-共同渡过 (Live) (2).wav
│   │   │   ├── 张国荣-共同渡过 (Live).wav
│   │   │   ├── 张国荣-共同渡过 Together Forever.wav
│   │   │   ├── 张国荣-千千阕歌 (Live).wav
│   │   │   ├── 张国荣-千娇百美.wav
│   │   │   ├── 张国荣-午后红茶.wav
│   │   │   ├── 张国荣-发烧.wav
│   │   │   ├── 张国荣-够了 (Live).wav
│   │   │   ├── 张国荣-够了 (Remix).wav
│   │   │   ├── 张国荣-大热.wav
│   │   │   ├── 张国荣-奇迹.wav
│   │   │   ├── 张国荣-奔向未来日子 (Live).wav
│   │   │   ├── 张国荣-客途秋恨 (Live).wav
│   │   │   ├── 张国荣-寂寞夜晚 (Live).wav
│   │   │   ├── 张国荣-当年情 (Live) (2).wav
│   │   │   ├── 张国荣-怨男.wav
│   │   │   ├── 张国荣-怪你过份美丽.wav
│   │   │   ├── 张国荣-想你 (Live).wav
│   │   │   ├── 张国荣-想妳 (Live).wav
│   │   │   ├── 张国荣-想妳 On My Mind.wav
│   │   │   ├── 张国荣-意犹未尽.wav
│   │   │   ├── 张国荣-我 (Mandarin Version) (2).wav
│   │   │   ├── 张国荣-我 (Mandarin Version).wav
│   │   │   ├── 张国荣-我.wav
│   │   │   ├── 张国荣-我知你好.wav
│   │   │   ├── 张国荣-打开信箱 (Live).wav
│   │   │   ├── 张国荣-拒绝再玩 (Live).wav
│   │   │   ├── 张国荣-拒绝再玩 (Remix).wav
│   │   │   ├── 张国荣-挪亚方舟.wav
│   │   │   ├── 张国荣-放荡 (Live).wav
│   │   │   ├── 张国荣-放荡 (Remix).wav
│   │   │   ├── 张国荣-敢爱.wav
│   │   │   ├── 张国荣-无心睡眠 (Live) (2).wav
│   │   │   ├── 张国荣-无心睡眠 (Live).wav
│   │   │   ├── 张国荣-无心睡眠 (Live丨reprise).wav
│   │   │   ├── 张国荣-无心睡眠 (Remix).wav
│   │   │   ├── 张国荣-无心睡眠 Sleepless nights Restless heart.wav
│   │   │   ├── 张国荣-无需要太多 (Live) (2).wav
│   │   │   ├── 张国荣-无需要太多 (Live).wav
│   │   │   ├── 张国荣-无需要太多 Love is Enough.wav
│   │   │   ├── 张国荣-明星 (Live).wav
│   │   │   ├── 张国荣-暴风一族 (Live).wav
│   │   │   ├── 张国荣-暴风一族 (Remix).wav
│   │   │   ├── 张国荣-最爱 (Live) (2).wav
│   │   │   ├── 张国荣-最爱 (Live).wav
│   │   │   ├── 张国荣-最爱是谁 My Dearest.wav
│   │   │   ├── 张国荣-有心人.wav
│   │   │   ├── 张国荣-有谁共鸣 (Live).wav
│   │   │   ├── 张国荣-沉默是金 (Live).wav
│   │   │   ├── 张国荣-没有烟总有花.wav
│   │   │   ├── 张国荣-没有爱.wav
│   │   │   ├── 张国荣-烈火边缘 (Live).wav
│   │   │   ├── 张国荣-热辣辣 (Live).wav
│   │   │   ├── 张国荣-热辣辣 (Remix).wav
│   │   │   ├── 张国荣-爱慕 (Live) (2).wav
│   │   │   ├── 张国荣-爱慕 (Live).wav
│   │   │   ├── 张国荣-爱的凶手 (Live).wav
│   │   │   ├── 张国荣-爱的凶手.wav
│   │   │   ├── 张国荣-玻璃之情.wav
│   │   │   ├── 张国荣-由零开始 (Live).wav
│   │   │   ├── 张国荣-由零开始 Will You Remember Me.wav
│   │   │   ├── 张国荣-禁片 (Remix).wav
│   │   │   ├── 张国荣-红.wav
│   │   │   ├── 张国荣-红蝴蝶.wav
│   │   │   ├── 张国荣-继续跳舞 (Live).wav
│   │   │   ├── 张国荣-继续跳舞 (Remix).wav
│   │   │   ├── 张国荣-胭脂扣 (Live).wav
│   │   │   ├── 张国荣-蝶变.wav
│   │   │   ├── 张国荣-访英台 (Live).wav
│   │   │   ├── 张国荣-请勿越轨 (Live).wav
│   │   │   ├── 张国荣-谈情说爱.wav
│   │   │   ├── 张国荣-身边有人.wav
│   │   │   ├── 张国荣-还有谁.wav
│   │   │   ├── 张国荣-随心.wav
│   │   │   ├── 张国荣-风再起时 (Live).wav
│   │   │   ├── 张国荣-风继续吹 (Live) (2).wav
│   │   │   ├── 张国荣-风继续吹 (Live).wav
│   │   │   ├── 張國榮.LESLIE CHEUNG LPCD45II.wav
│   │   │   └── 柏安妮-不想再拥有 (Live).wav
│   │   └── low/
│   │       ├── 01 - A Thousand Dreams Of You.wav
│   │       ├── 01 - Day Dreamer.wav
│   │       ├── 01 - Dreaming (2).wav
│   │       ├── 01 - Dreaming.wav
│   │       ├── 01 - Hot Summer.wav
│   │       ├── 01 - MY GOD.wav
│   │       ├── 01 - Medley：有谁共鸣-沉默是金.wav
│   │       ├── 01 - Opening 风再起时.wav
│   │       ├── 01 - Opening-为你钟情.wav
│   │       ├── 01 - Overture.wav
│   │       ├── 01 - Stand Up.wav
│   │       ├── 01 - Tonight and Forever.wav
│   │       ├── 01 - マツュマロ.wav
│   │       ├── 01 - 一片痴.wav
│   │       ├── 01 - 不羁的风.wav
│   │       ├── 01 - 为你钟情.wav
│   │       ├── 01 - 停止转动(国语).wav
│   │       ├── 01 - 全赖有你.wav
│   │       ├── 01 - 取暖.wav
│   │       ├── 01 - 只怕不再遇上 Live.wav
│   │       ├── 01 - 够了.wav
│   │       ├── 01 - 始终会行运.wav
│   │       ├── 01 - 当爱已成往事.wav
│   │       ├── 01 - 当真就好（陈淑桦合唱）.wav
│   │       ├── 01 - 情人箭.wav
│   │       ├── 01 - 情难再续.wav
│   │       ├── 01 - 拒绝再玩 (2).wav
│   │       ├── 01 - 拒绝再玩 (3).wav
│   │       ├── 01 - 拒绝再玩 (4).wav
│   │       ├── 01 - 拒绝再玩.wav
│   │       ├── 01 - 有谁共鸣.wav
│   │       ├── 01 - 油脂热潮.wav
│   │       ├── 01 - 烈火边缘.wav
│   │       ├── 01 - 爱慕 (2).wav
│   │       ├── 01 - 爱慕.wav
│   │       ├── 01 - 爱的凶手 (2).wav
│   │       ├── 01 - 爱的凶手.wav
│   │       ├── 01 - 由零开始.wav
│   │       ├── 01 - 痛心（袁咏仪）.wav
│   │       ├── 01 - 红.wav
│   │       ├── 01 - 贴身.wav
│   │       ├── 01 - 路过蜻蜓.wav
│   │       ├── 01 - 这些年来.wav
│   │       ├── 01 - 醉死梦生.wav
│   │       ├── 01 - 风再起时.wav
│   │       ├── 01 - 风继续吹 (2).wav
│   │       ├── 01 - 风继续吹.wav
│   │       ├── 02 - Hot Summer (2).wav
│   │       ├── 02 - Hot Summer.wav
│   │       ├── 02 - I miss you much.wav
│   │       ├── 02 - Miss You Much.wav
│   │       ├── 02 - Monica(Remix).wav
│   │       ├── 02 - Monica.wav
│   │       ├── 02 - My God (2).wav
│   │       ├── 02 - My God.wav
│   │       ├── 02 - Stand Up.wav
│   │       ├── 02 - We Are All Alone.wav
│   │       ├── 02 - 三岁仔.wav
│   │       ├── 02 - 上帝.wav
│   │       ├── 02 - 为你 (2).wav
│   │       ├── 02 - 为你.wav
│   │       ├── 02 - 今生今世 (2).wav
│   │       ├── 02 - 今生今世.wav
│   │       ├── 02 - 你这样恨我.wav
│   │       ├── 02 - 侧面.wav
│   │       ├── 02 - 侬本多情 (2).wav
│   │       ├── 02 - 侬本多情.wav
│   │       ├── 02 - 倩女幽魂.wav
│   │       ├── 02 - 少女心事.wav
│   │       ├── 02 - 左右手.wav
│   │       ├── 02 - 当年情.wav
│   │       ├── 02 - 当真就好.wav
│   │       ├── 02 - 怨男 Live.wav
│   │       ├── 02 - 我走我路.wav
│   │       ├── 02 - 拒绝再玩.wav
│   │       ├── 02 - 无心睡眠 (2).wav
│   │       ├── 02 - 无心睡眠.wav
│   │       ├── 02 - 枕头.wav
│   │       ├── 02 - 梦到内河.wav
│   │       ├── 02 - 梦死醉生.wav
│   │       ├── 02 - 浣花洗剑录.wav
│   │       ├── 02 - 深情相拥（辛晓琪合唱）.wav
│   │       ├── 02 - 烈火灯蛾.wav
│   │       ├── 02 - 热辣辣.wav
│   │       ├── 02 - 痴心的我.wav
│   │       ├── 02 - 真相.wav
│   │       ├── 02 - 第一次.wav
│   │       ├── 02 - 贴身 (2).wav
│   │       ├── 02 - 贴身.wav
│   │       ├── 02 - 迷惑我.wav
│   │       ├── 02 - 那一记耳光 (2).wav
│   │       ├── 02 - 那一记耳光.wav
│   │       ├── 02 - 风继续吹.wav
│   │       ├── 02 - 黑色午夜.wav
│   │       ├── 03 - Even Now.wav
│   │       ├── 03 - Forever爱你.wav
│   │       ├── 03 - H2O.wav
│   │       ├── 03 - Love Like Magic.wav
│   │       ├── 03 - Medley：恋爱交叉-打开信箱-蓝色忧郁-Monica.wav
│   │       ├── 03 - Miss You Much.wav
│   │       ├── 03 - Miss you much (2).wav
│   │       ├── 03 - Monica.wav
│   │       ├── 03 - Stand Up.wav
│   │       ├── 03 - 为你钟情.wav
│   │       ├── 03 - 以后.wav
│   │       ├── 03 - 你在何地 (2).wav
│   │       ├── 03 - 你在何地.wav
│   │       ├── 03 - 侧面.wav
│   │       ├── 03 - 侬本多情.wav
│   │       ├── 03 - 分手.wav
│   │       ├── 03 - 夜半歌声.wav
│   │       ├── 03 - 奔向未来日子.wav
│   │       ├── 03 - 宿醉.wav
│   │       ├── 03 - 寂寞夜晚.wav
│   │       ├── 03 - 寂寞有害.wav
│   │       ├── 03 - 左右手(Acoustic Mix).wav
│   │       ├── 03 - 左右手.wav
│   │       ├── 03 - 当年情.wav
│   │       ├── 03 - 情到浓时.wav
│   │       ├── 03 - 想你.wav
│   │       ├── 03 - 我愿意.wav
│   │       ├── 03 - 打开信箱.wav
│   │       ├── 03 - 放荡.wav
│   │       ├── 03 - 无心睡眠 (2).wav
│   │       ├── 03 - 无心睡眠.wav
│   │       ├── 03 - 无需要太多.wav
│   │       ├── 03 - 春夏秋冬.wav
│   │       ├── 03 - 柔情蜜意.wav
│   │       ├── 03 - 沈胜衣.wav
│   │       ├── 03 - 洁身自爱.wav
│   │       ├── 03 - 浣花洗剑录.wav
│   │       ├── 03 - 深情相拥（辛晓琪合唱）.wav
│   │       ├── 03 - 热辣辣 Live.wav
│   │       ├── 03 - 热辣辣.wav
│   │       ├── 03 - 爱情路里.wav
│   │       ├── 03 - 片段.wav
│   │       ├── 03 - 片段（袁咏仪合唱）.wav
│   │       ├── 03 - 追.wav
│   │       ├── 03 - 透明的你 (2).wav
│   │       ├── 03 - 透明的你.wav
│   │       ├── 03 - 风继续吹.wav
│   │       ├── 04 - Before My Heart Finds Out.wav
│   │       ├── 04 - Everybody.wav
│   │       ├── 04 - H2O.wav
│   │       ├── 04 - Medley：想你-偷情 Live.wav
│   │       ├── 04 - Medley：蓝色忧郁-少女心事-不羁的风-Monica.wav
│   │       ├── 04 - Stories.wav
│   │       ├── 04 - Why.wav
│   │       ├── 04 - 不要爱他.wav
│   │       ├── 04 - 今生今世.wav
│   │       ├── 04 - 侧面 (2).wav
│   │       ├── 04 - 侧面.wav
│   │       ├── 04 - 偏心.wav
│   │       ├── 04 - 共你别离.wav
│   │       ├── 04 - 变色龙.wav
│   │       ├── 04 - 只怕不再遇上.wav
│   │       ├── 04 - 可否多一吻.wav
│   │       ├── 04 - 失散的影子.wav
│   │       ├── 04 - 寂寞有害.wav
│   │       ├── 04 - 愿能比翼飞.wav
│   │       ├── 04 - 我(国语).wav
│   │       ├── 04 - 我眼中的她.wav
│   │       ├── 04 - 打开信箱.wav
│   │       ├── 04 - 无形锁扣.wav
│   │       ├── 04 - 无心锁扣.wav
│   │       ├── 04 - 无需要太多.wav
│   │       ├── 04 - 明星.wav
│   │       ├── 04 - 暴风一族.wav
│   │       ├── 04 - 最冷一天.wav
│   │       ├── 04 - 枕头.wav
│   │       ├── 04 - 柔情蜜意 (2).wav
│   │       ├── 04 - 柔情蜜意.wav
│   │       ├── 04 - 没有烟总有花.wav
│   │       ├── 04 - 深情相拥.wav
│   │       ├── 04 - 爱情离合器.wav
│   │       ├── 04 - 爱慕.wav
│   │       ├── 04 - 狂野如我 (2).wav
│   │       ├── 04 - 狂野如我.wav
│   │       ├── 04 - 甜蜜的禁果.wav
│   │       ├── 04 - 电风扇.wav
│   │       ├── 04 - 痴心的我.wav
│   │       ├── 04 - 痴心（袁咏仪）.wav
│   │       ├── 04 - 红.wav
│   │       ├── 04 - 迷惑我.wav
│   │       ├── 04 - 追.wav
│   │       ├── 04 - 隐身人.wav
│   │       ├── 04 - 雪中情.wav
│   │       ├── 04 - 风继续吹(Mix).wav
│   │       ├── 04 - 风继续吹.wav
│   │       ├── 05 - FROM NOW ON（林忆莲合唱）.wav
│   │       ├── 05 - Good Morning Sorrow.wav
│   │       ├── 05 - H2O.wav
│   │       ├── 05 - Hey!不要玩.wav
│   │       ├── 05 - I Honestly Love You.wav
│   │       ├── 05 - Medley：H2O-黑色午夜-隐身人-第一次-Stand Up.wav
│   │       ├── 05 - 一片痴 (2).wav
│   │       ├── 05 - 一片痴.wav
│   │       ├── 05 - 从未可以.wav
│   │       ├── 05 - 侧面.wav
│   │       ├── 05 - 共同渡过.wav
│   │       ├── 05 - 到未来日子 (2).wav
│   │       ├── 05 - 到未来日子.wav
│   │       ├── 05 - 夜半歌声(电影版).wav
│   │       ├── 05 - 大亨.wav
│   │       ├── 05 - 大报复.wav
│   │       ├── 05 - 奔向未来日子.wav
│   │       ├── 05 - 妄想 (2).wav
│   │       ├── 05 - 妄想.wav
│   │       ├── 05 - 宿醉.wav
│   │       ├── 05 - 当年情 (2).wav
│   │       ├── 05 - 当年情.wav
│   │       ├── 05 - 当爱已成往事.wav
│   │       ├── 05 - 心中情.wav
│   │       ├── 05 - 心跳呼吸正常.wav
│   │       ├── 05 - 恋爱交叉.wav
│   │       ├── 05 - 我愿意.wav
│   │       ├── 05 - 拒绝再玩.wav
│   │       ├── 05 - 最爱.wav
│   │       ├── 05 - 月亮代表我的心(Live).wav
│   │       ├── 05 - 有心人.wav
│   │       ├── 05 - 深情相拥 Live.wav
│   │       ├── 05 - 燕子的故事.wav
│   │       ├── 05 - 爱慕 (2).wav
│   │       ├── 05 - 爱慕 (3).wav
│   │       ├── 05 - 爱慕.wav
│   │       ├── 05 - 由零开始.wav
│   │       ├── 05 - 绝不可以.wav
│   │       ├── 05 - 缘份.wav
│   │       ├── 05 - 缘份有几多 (2).wav
│   │       ├── 05 - 缘份有几多.wav
│   │       ├── 05 - 缘份（梅艳芳合唱）.wav
│   │       ├── 05 - 蓝色忧郁.wav
│   │       ├── 05 - 被爱.wav
│   │       ├── 05 - 谁令你心痴（陈洁灵合唱）.wav
│   │       ├── 05 - 陪你倒数.wav
│   │       ├── 05 - 需要你.wav
│   │       ├── 06 - FROM NOW ON.wav
│   │       ├── 06 - H2O Medley：H2O-少女心事-第一次-不羁的风.wav
│   │       ├── 06 - Medley：童年时-似水流年-但愿人长久.wav
│   │       ├── 06 - Undercover Angel.wav
│   │       ├── 06 - 一片痴.wav
│   │       ├── 06 - 一辈子失去了妳.wav
│   │       ├── 06 - 为你.wav
│   │       ├── 06 - 为你钟情.wav
│   │       ├── 06 - 今生今世.wav
│   │       ├── 06 - 侧面.wav
│   │       ├── 06 - 全身都是爱.wav
│   │       ├── 06 - 共同渡过 (2).wav
│   │       ├── 06 - 共同渡过 (3).wav
│   │       ├── 06 - 共同渡过.wav
│   │       ├── 06 - 天使之爱 (2).wav
│   │       ├── 06 - 天使之爱.wav
│   │       ├── 06 - 小明星.wav
│   │       ├── 06 - 当年情.wav
│   │       ├── 06 - 情自困.wav
│   │       ├── 06 - 想你 (2).wav
│   │       ├── 06 - 想你.wav
│   │       ├── 06 - 拒绝再玩.wav
│   │       ├── 06 - 无需要太多.wav
│   │       ├── 06 - 暴风一族 (2).wav
│   │       ├── 06 - 暴风一族.wav
│   │       ├── 06 - 柔情蜜意 (2).wav
│   │       ├── 06 - 柔情蜜意.wav
│   │       ├── 06 - 沉默是金.wav
│   │       ├── 06 - 流浪 (2).wav
│   │       ├── 06 - 流浪.wav
│   │       ├── 06 - 深情相拥（辛晓琪合唱）.wav
│   │       ├── 06 - 爱情离合器.wav
│   │       ├── 06 - 爱慕.wav
│   │       ├── 06 - 爱的抉择.wav
│   │       ├── 06 - 真相.wav
│   │       ├── 06 - 第一次.wav
│   │       ├── 06 - 谈情说爱 Live.wav
│   │       ├── 06 - 这刻相见后.wav
│   │       ├── 06 - 追族.wav
│   │       ├── 06 - 追逐.wav
│   │       ├── 06 - 风继续吹.wav
│   │       ├── 06 - 黑色午夜.wav
│   │       ├── 07 - I Like Dreamin'.wav
│   │       ├── 07 - Medley：红颜白发-最爱 Live.wav
│   │       ├── 07 - Miss You Much.wav
│   │       ├── 07 - Monica (2).wav
│   │       ├── 07 - 不怕寂寞.wav
│   │       ├── 07 - 不羁的风.wav
│   │       ├── 07 - 为你钟情.wav
│   │       ├── 07 - 作伴.wav
│   │       ├── 07 - 你教我点好.wav
│   │       ├── 07 - 侬本多情 (2).wav
│   │       ├── 07 - 侬本多情.wav
│   │       ├── 07 - 偷情.wav
│   │       ├── 07 - 共同渡过 (2).wav
│   │       ├── 07 - 共同渡过.wav
│   │       ├── 07 - 千千阕歌.wav
│   │       ├── 07 - 同道中人.wav
│   │       ├── 07 - 夜半歌声.wav
│   │       ├── 07 - 奔向未来日子.wav
│   │       ├── 07 - 寂寞夜晚.wav
│   │       ├── 07 - 少女心事.wav
│   │       ├── 07 - 恋爱交叉.wav
│   │       ├── 07 - 情人箭.wav
│   │       ├── 07 - 情难自控.wav
│   │       ├── 07 - 我愿意.wav
│   │       ├── 07 - 打开信箱 (2).wav
│   │       ├── 07 - 打开信箱.wav
│   │       ├── 07 - 放荡.wav
│   │       ├── 07 - 无心睡眠.wav
│   │       ├── 07 - 有谁共鸣.wav
│   │       ├── 07 - 烧毁我眼睛.wav
│   │       ├── 07 - 爱情路里.wav
│   │       ├── 07 - 留住昨天.wav
│   │       ├── 07 - 眉来眼去（辛晓琪合唱）.wav
│   │       ├── 07 - 继续跳舞.wav
│   │       ├── 07 - 触电.wav
│   │       ├── 07 - 让我飞 (2).wav
│   │       ├── 07 - 让我飞.wav
│   │       ├── 07 - 路过蜻蜒.wav
│   │       ├── 07 - 追.wav
│   │       ├── 07 - 野火.wav
│   │       ├── 07 - 风再起时.wav
│   │       ├── 07 - 风继续吹.wav
│   │       ├── 07 - 黑色午夜.wav
│   │       ├── 08 - H2O.wav
│   │       ├── 08 - I Need You.wav
│   │       ├── 08 - Stand Up-Twist & Shout-Stand Up.wav
│   │       ├── 08 - 一辈子失去了妳.wav
│   │       ├── 08 - 不怕寂寞.wav
│   │       ├── 08 - 不管您是谁.wav
│   │       ├── 08 - 不羁的风.wav
│   │       ├── 08 - 不要爱他.wav
│   │       ├── 08 - 为妳钟情.wav
│   │       ├── 08 - 你是我一半.wav
│   │       ├── 08 - 你的一切.wav
│   │       ├── 08 - 侧面-放荡.wav
│   │       ├── 08 - 侬本多情.wav
│   │       ├── 08 - 兜风心情.wav
│   │       ├── 08 - 兜风心情（柏安妮合唱）.wav
│   │       ├── 08 - 共同渡过.wav
│   │       ├── 08 - 刻骨铭心.wav
│   │       ├── 08 - 取暖.wav
│   │       ├── 08 - 够了 (2).wav
│   │       ├── 08 - 够了.wav
│   │       ├── 08 - 妳教我点好.wav
│   │       ├── 08 - 客途秋恨.wav
│   │       ├── 08 - 寂寞夜晚.wav
│   │       ├── 08 - 少女心事.wav
│   │       ├── 08 - 我愿意.wav
│   │       ├── 08 - 无心睡眠.wav
│   │       ├── 08 - 明星 Live.wav
│   │       ├── 08 - 明星.wav
│   │       ├── 08 - 暴风一族.wav
│   │       ├── 08 - 最爱.wav
│   │       ├── 08 - 有心人 (2).wav
│   │       ├── 08 - 有心人.wav
│   │       ├── 08 - 浓情.wav
│   │       ├── 08 - 烈火边缘.wav
│   │       ├── 08 - 热辣辣.wav
│   │       ├── 08 - 爱有万万千.wav
│   │       ├── 08 - 爱火.wav
│   │       ├── 08 - 痴心的我.wav
│   │       ├── 08 - 眉来眼去.wav
│   │       ├── 08 - 眉来眼去（辛晓琪合唱）.wav
│   │       ├── 08 - 知道爱.wav
│   │       ├── 08 - 继续跳舞.wav
│   │       ├── 08 - 请勿越轨.wav
│   │       ├── 08 - 谁负了谁.wav
│   │       ├── 08 - 难以再说对不起.wav
│   │       ├── 09 - Medley：阿飞正传-梦-A Thousand Dreams of You.wav
│   │       ├── 09 - Monica.wav
│   │       ├── 09 - The Way We Were.wav
│   │       ├── 09 - WHY.wav
│   │       ├── 09 - You Made Me Believe In Magic.wav
│   │       ├── 09 - 七色的爱.wav
│   │       ├── 09 - 三岁仔.wav
│   │       ├── 09 - 不怕寂寞.wav
│   │       ├── 09 - 不管你是谁.wav
│   │       ├── 09 - 不羁的风.wav
│   │       ├── 09 - 为你钟情.wav
│   │       ├── 09 - 你在何地.wav
│   │       ├── 09 - 倩女幽魂 (2).wav
│   │       ├── 09 - 内心争斗.wav
│   │       ├── 09 - 别话.wav
│   │       ├── 09 - 妒忌.wav
│   │       ├── 09 - 寂寞猎人.wav
│   │       ├── 09 - 找一个地方.wav
│   │       ├── 09 - 无需要太多 (2).wav
│   │       ├── 09 - 无需要太多.wav
│   │       ├── 09 - 暴风一族.wav
│   │       ├── 09 - 有心人.wav
│   │       ├── 09 - 枕头.wav
│   │       ├── 09 - 死心（袁咏仪）.wav
│   │       ├── 09 - 沉默是金.wav
│   │       ├── 09 - 爱慕.wav
│   │       ├── 09 - 爱有万万千.wav
│   │       ├── 09 - 爱火.wav
│   │       ├── 09 - 由零开始.wav
│   │       ├── 09 - 电风扇.wav
│   │       ├── 09 - 禁片.wav
│   │       ├── 09 - 红 Live.wav
│   │       ├── 09 - 红颜白发.wav
│   │       ├── 09 - 胭脂扣.wav
│   │       ├── 09 - 蓝色忧郁 (2).wav
│   │       ├── 09 - 蓝色忧郁.wav
│   │       ├── 09 - 请勿越轨 (2).wav
│   │       ├── 09 - 请勿越轨.wav
│   │       ├── 09 - 谈恋爱.wav
│   │       ├── 09 - 谈情说爱.wav
│   │       ├── 09 - 闯进新领域.wav
│   │       ├── 09 - 陪你倒数.wav
│   │       ├── 09 - 难以再说对不起.wav
│   │       ├── 09 - 默默向上游(Mix).wav
│   │       ├── 09 - 默默向上游.wav
│   │       ├── 10 - American Pie.wav
│   │       ├── 10 - CRAZY ROCK.wav
│   │       ├── 10 - I Honestly Love You.wav
│   │       ├── 10 - Just The Way You Are.wav
│   │       ├── 10 - Medley：啼笑姻缘-当爱已成往事-啼笑姻缘.wav
│   │       ├── 10 - Thank You.wav
│   │       ├── 10 - 一片痴.wav
│   │       ├── 10 - 为你钟情 Live.wav
│   │       ├── 10 - 人生的鼓手 (2).wav
│   │       ├── 10 - 人生的鼓手.wav
│   │       ├── 10 - 何去何从之阿飞正传.wav
│   │       ├── 10 - 你是明星(国).wav
│   │       ├── 10 - 倩女幽魂 (2).wav
│   │       ├── 10 - 倩女幽魂 (4).wav
│   │       ├── 10 - 全赖有你.wav
│   │       ├── 10 - 再恋.wav
│   │       ├── 10 - 刻骨铭心.wav
│   │       ├── 10 - 可人儿.wav
│   │       ├── 10 - 尽情地爱.wav
│   │       ├── 10 - 心跳呼吸正常.wav
│   │       ├── 10 - 想你 (2).wav
│   │       ├── 10 - 想你.wav
│   │       ├── 10 - 拒绝再玩.wav
│   │       ├── 10 - 无心睡眠.wav
│   │       ├── 10 - 无需要太多.wav
│   │       ├── 10 - 最爱.wav
│   │       ├── 10 - 有心人.wav
│   │       ├── 10 - 未来之歌.wav
│   │       ├── 10 - 柔情蜜意.wav
│   │       ├── 10 - 棉花糖.wav
│   │       ├── 10 - 油脂热潮.wav
│   │       ├── 10 - 爱的狂徒 (2).wav
│   │       ├── 10 - 眉来眼去（辛晓琪合唱）.wav
│   │       ├── 10 - 缘份.wav
│   │       ├── 10 - 蓝色忧郁.wav
│   │       ├── 10 - 被爱.wav
│   │       ├── 10 - 谁令你心痴（陈洁灵合唱）.wav
│   │       ├── 10 - 雨中的浪漫.wav
│   │       ├── 10 - 风再起时.wav
│   │       ├── 10 - 黑色午夜(Remix).wav
│   │       ├── 11 - A Little Bit More.wav
│   │       ├── 11 - Every body.wav
│   │       ├── 11 - H2O.wav
│   │       ├── 11 - MIRACLE（麦洁文合唱）.wav
│   │       ├── 11 - マツュマロ.wav
│   │       ├── 11 - 为你钟情.wav
│   │       ├── 11 - 何去何从之阿飞正传.wav
│   │       ├── 11 - 全世界只想你来爱我(国).wav
│   │       ├── 11 - 全身都是爱.wav
│   │       ├── 11 - 分手.wav
│   │       ├── 11 - 大亨.wav
│   │       ├── 11 - 少女心事.wav
│   │       ├── 11 - 左右手.wav
│   │       ├── 11 - 怪你过分美丽.wav
│   │       ├── 11 - 恋爱交叉.wav
│   │       ├── 11 - 情难自控.wav
│   │       ├── 11 - 无心睡眠.wav
│   │       ├── 11 - 明月夜 (2).wav
│   │       ├── 11 - 明月夜.wav
│   │       ├── 11 - 暴风一族.wav
│   │       ├── 11 - 月亮代表我的心 Live.wav
│   │       ├── 11 - 柔情蜜意.wav
│   │       ├── 11 - 永远记得.wav
│   │       ├── 11 - 沈胜衣.wav
│   │       ├── 11 - 烈火边缘.wav
│   │       ├── 11 - 至少还有你.wav
│   │       ├── 11 - 访英台.wav
│   │       ├── 11 - 谈情说爱.wav
│   │       ├── 11 - 迷惑我.wav
│   │       ├── 11 - 隐身人.wav
│   │       ├── 11 - 默默向上游 (2).wav
│   │       ├── 11 - 默默向上游 (3).wav
│   │       ├── 11 - 默默向上游.wav
│   │       ├── 12 - Love like magic.wav
│   │       ├── 12 - Pistol Packin' Melody.wav
│   │       ├── 12 - 一盏小明灯.wav
│   │       ├── 12 - 倩女幽魂.wav
│   │       ├── 12 - 共同渡过.wav
│   │       ├── 12 - 分手.wav
│   │       ├── 12 - 够了.wav
│   │       ├── 12 - 大报复.wav
│   │       ├── 12 - 奔向未来日子.wav
│   │       ├── 12 - 始终会行运.wav
│   │       ├── 12 - 守住风口 (2).wav
│   │       ├── 12 - 守住风口.wav
│   │       ├── 12 - 少女心事.wav
│   │       ├── 12 - 当年情.wav
│   │       ├── 12 - 我(国).wav
│   │       ├── 12 - 我愿意.wav
│   │       ├── 12 - 我要逆风去 (2).wav
│   │       ├── 12 - 我要逆风去.wav
│   │       ├── 12 - 无心睡眠.wav
│   │       ├── 12 - 没有爱.wav
│   │       ├── 12 - 爱火.wav
│   │       ├── 12 - 第一次.wav
│   │       ├── 12 - 谈恋爱.wav
│   │       ├── 12 - 谈情说爱 (2).wav
│   │       ├── 12 - 谈情说爱.wav
│   │       ├── 12 - 贴身.wav
│   │       ├── 12 - 迷路.wav
│   │       ├── 12 - 追 Live.wav
│   │       ├── 12 - 风继续吹 (2).wav
│   │       ├── 12 - 风继续吹.wav
│   │       ├── 13 - A Thousand Dreams Of You.wav
│   │       ├── 13 - Twist & Shout.wav
│   │       ├── 13 - 作伴.wav
│   │       ├── 13 - 侧面.wav
│   │       ├── 13 - 共同度过.wav
│   │       ├── 13 - 共同渡过.wav
│   │       ├── 13 - 最爱.wav
│   │       ├── 13 - 燕子的故事.wav
│   │       ├── 13 - 片段.wav
│   │       ├── 13 - 痴心的我.wav
│   │       ├── 13 - 第一次 (2).wav
│   │       ├── 13 - 第一次.wav
│   │       ├── 13 - 路过蜻蜓.wav
│   │       ├── 13 - 野火 (2).wav
│   │       ├── 13 - 野火.wav
│   │       ├── 13 - 陪你倒数.wav
│   │       ├── 13 - 隐身人.wav
│   │       ├── 14 - Daydreamer.wav
│   │       ├── 14 - Stand Up.wav
│   │       ├── 14 - 共同度过.wav
│   │       ├── 14 - 分手.wav
│   │       ├── 14 - 失散的影子 (2).wav
│   │       ├── 14 - 失散的影子.wav
│   │       ├── 14 - 当爱已成往事.wav
│   │       ├── 14 - 无心睡眠.wav
│   │       ├── 14 - 沉默是金(独唱版).wav
│   │       ├── 14 - 爱情路里.wav
│   │       ├── 14 - 迷惑我.wav
│   │       ├── 14 - 迷路.wav
│   │       ├── 14 - 默默向上游.wav
│   │       ├── 15 - H2O.wav
│   │       ├── 15 - I Need You.wav
│   │       ├── 15 - 为你钟情.wav
│   │       ├── 15 - 侬本多情.wav
│   │       ├── 15 - 全赖有你.wav
│   │       ├── 15 - 只怕不再遇上（陈洁灵合唱）.wav
│   │       ├── 15 - 恋爱交叉.wav
│   │       ├── 15 - 惊梦 (2).wav
│   │       ├── 15 - 惊梦.wav
│   │       ├── 15 - 我的心里没有他.wav
│   │       ├── 15 - 无胆入情关.wav
│   │       ├── 15 - 红颜白发.wav
│   │       ├── 15 - 风再起时 (2).wav
│   │       ├── 16 - Everybody.wav
│   │       ├── 16 - I Like Dreamin'.wav
│   │       ├── 16 - 一盏小明灯.wav
│   │       ├── 16 - 倩女幽魂.wav
│   │       ├── 16 - 热情的沙漠.wav
│   │       ├── 16 - 背着命运(国语).wav
│   │       ├── 16 - 闯进新领域.wav
│   │       ├── 16 - 风再起时.wav
│   │       ├── 17 - Love Like Magic.wav
│   │       ├── 17 - 不确定的年纪(国语).wav
│   │       ├── 17 - 大热.wav
│   │       ├── 17 - 当年情(国语).wav
│   │       ├── 17 - 有谁共鸣.wav
│   │       ├── 18 - 沉默是金.wav
│   │       ├── 18 - 痴心的我(国语).wav
│   │       ├── SSB0005.wav
│   │       ├── SSB0009.wav
│   │       ├── SSB0011.wav
│   │       ├── SSB0012.wav
│   │       ├── SSB0016.wav
│   │       ├── SSB0018.wav
│   │       ├── SSB0038.wav
│   │       ├── SSB0043.wav
│   │       ├── SSB0057.wav
│   │       ├── SSB0073.wav
│   │       ├── SSB0080.wav
│   │       ├── SSB0112.wav
│   │       ├── SSB0122.wav
│   │       ├── SSB0133.wav
│   │       ├── SSB0145.wav
│   │       ├── SSB0149.wav
│   │       ├── SSB0193.wav
│   │       ├── SSB0197.wav
│   │       ├── SSB0200.wav
│   │       ├── SSB0241.wav
│   │       ├── SSB0246.wav
│   │       ├── SSB0261.wav
│   │       ├── SSB0267.wav
│   │       ├── SSB0273.wav
│   │       ├── SSB0287.wav
│   │       ├── SSB0288.wav
│   │       ├── SSB0299.wav
│   │       ├── SSB0307.wav
│   │       ├── SSB0309.wav
│   │       ├── SSB0315.wav
│   │       ├── SSB0316.wav
│   │       ├── SSB0323.wav
│   │       ├── SSB0338.wav
│   │       ├── SSB0339.wav
│   │       ├── SSB0341.wav
│   │       ├── SSB0342.wav
│   │       ├── SSB0354.wav
│   │       ├── SSB0366.wav
│   │       ├── SSB0375.wav
│   │       ├── SSB0379.wav
│   │       ├── SSB0380.wav
│   │       ├── SSB0382.wav
│   │       ├── SSB0385.wav
│   │       ├── SSB0393.wav
│   │       ├── SSB0394.wav
│   │       ├── SSB0395.wav
│   │       ├── SSB0407.wav
│   │       ├── SSB0415.wav
│   │       ├── SSB0426.wav
│   │       ├── SSB0427.wav
│   │       ├── SSB0434.wav
│   │       ├── SSB0435.wav
│   │       ├── SSB0470.wav
│   │       ├── SSB0482.wav
│   │       ├── SSB0502.wav
│   │       ├── SSB0534.wav
│   │       ├── SSB0535.wav
│   │       ├── SSB0539.wav
│   │       ├── SSB0544.wav
│   │       ├── SSB0565.wav
│   │       ├── SSB0570.wav
│   │       ├── SSB0578.wav
│   │       ├── SSB0588.wav
│   │       ├── SSB0590.wav
│   │       ├── SSB0594.wav
│   │       ├── SSB0599.wav
│   │       ├── SSB0601.wav
│   │       ├── SSB0603.wav
│   │       ├── SSB0606.wav
│   │       ├── SSB0607.wav
│   │       ├── SSB0614.wav
│   │       ├── SSB0623.wav
│   │       ├── SSB0629.wav
│   │       ├── SSB0631.wav
│   │       ├── SSB0632.wav
│   │       ├── SSB0666.wav
│   │       ├── SSB0668.wav
│   │       ├── SSB0671.wav
│   │       ├── SSB0686.wav
│   │       ├── SSB0700.wav
│   │       ├── SSB0710.wav
│   │       ├── SSB0720.wav
│   │       ├── SSB0723.wav
│   │       ├── SSB0746.wav
│   │       ├── SSB0751.wav
│   │       ├── SSB0758.wav
│   │       ├── SSB0760.wav
│   │       ├── SSB0762.wav
│   │       ├── SSB0778.wav
│   │       ├── SSB0780.wav
│   │       ├── SSB0784.wav
│   │       ├── SSB0786.wav
│   │       ├── SSB0794.wav
│   │       ├── SSB0817.wav
│   │       ├── SSB0851.wav
│   │       ├── SSB0863.wav
│   │       ├── SSB0871.wav
│   │       ├── SSB0887.wav
│   │       ├── SSB0913.wav
│   │       ├── SSB0915.wav
│   │       ├── SSB0935.wav
│   │       ├── SSB0966.wav
│   │       ├── SSB0987.wav
│   │       ├── SSB1008.wav
│   │       ├── SSB1020.wav
│   │       ├── SSB1024.wav
│   │       ├── SSB1050.wav
│   │       ├── SSB1055.wav
│   │       ├── SSB1056.wav
│   │       ├── SSB1064.wav
│   │       ├── SSB1072.wav
│   │       ├── SSB1091.wav
│   │       ├── SSB1096.wav
│   │       ├── SSB1100.wav
│   │       ├── SSB1108.wav
│   │       ├── SSB1115.wav
│   │       ├── SSB1125.wav
│   │       ├── SSB1131.wav
│   │       ├── SSB1136.wav
│   │       ├── SSB1138.wav
│   │       ├── SSB1161.wav
│   │       ├── SSB1203.wav
│   │       ├── SSB1204.wav
│   │       ├── SSB1218.wav
│   │       ├── SSB1221.wav
│   │       ├── SSB1253.wav
│   │       ├── SSB1320.wav
│   │       ├── SSB1341.wav
│   │       ├── SSB1366.wav
│   │       ├── SSB1377.wav
│   │       ├── SSB1383.wav
│   │       ├── SSB1385.wav
│   │       ├── SSB1392.wav
│   │       ├── SSB1393.wav
│   │       ├── SSB1408.wav
│   │       ├── SSB1431.wav
│   │       ├── SSB1437.wav
│   │       ├── SSB1448.wav
│   │       ├── SSB1555.wav
│   │       ├── SSB1563.wav
│   │       ├── SSB1567.wav
│   │       ├── SSB1575.wav
│   │       ├── SSB1585.wav
│   │       ├── SSB1593.wav
│   │       ├── SSB1607.wav
│   │       ├── SSB1624.wav
│   │       ├── SSB1625.wav
│   │       ├── SSB1630.wav
│   │       ├── SSB1650.wav
│   │       ├── SSB1670.wav
│   │       ├── SSB1684.wav
│   │       ├── SSB1686.wav
│   │       ├── SSB1699.wav
│   │       ├── SSB1711.wav
│   │       ├── SSB1759.wav
│   │       ├── SSB1806.wav
│   │       ├── SSB1831.wav
│   │       ├── SSB1832.wav
│   │       ├── SSB1837.wav
│   │       ├── SSB1846.wav
│   │       ├── SSB1863.wav
│   │       ├── SSB1878.wav
│   │       ├── SSB1891.wav
│   │       ├── SSB1918.wav
│   │       ├── SSB1935.wav
│   │       ├── SSB1939.wav
│   │       ├── p225.wav
│   │       ├── p226.wav
│   │       ├── p227.wav
│   │       ├── p228.wav
│   │       ├── p229.wav
│   │       ├── p231.wav
│   │       ├── p232.wav
│   │       ├── p233.wav
│   │       ├── p234.wav
│   │       ├── p236.wav
│   │       ├── p237.wav
│   │       ├── p238.wav
│   │       ├── p239.wav
│   │       ├── p240.wav
│   │       ├── p241.wav
│   │       ├── p243.wav
│   │       ├── p244.wav
│   │       ├── p245.wav
│   │       ├── p246.wav
│   │       ├── p247.wav
│   │       ├── p248.wav
│   │       ├── p249.wav
│   │       ├── p250.wav
│   │       ├── p251.wav
│   │       ├── p252.wav
│   │       ├── p253.wav
│   │       ├── p255.wav
│   │       ├── p256.wav
│   │       ├── p258.wav
│   │       ├── p259.wav
│   │       ├── p260.wav
│   │       ├── p261.wav
│   │       ├── p262.wav
│   │       ├── p263.wav
│   │       ├── p264.wav
│   │       ├── p265.wav
│   │       ├── p266.wav
│   │       ├── p267.wav
│   │       ├── p268.wav
│   │       ├── p269.wav
│   │       ├── p270.wav
│   │       ├── p272.wav
│   │       ├── p273.wav
│   │       ├── p274.wav
│   │       ├── p275.wav
│   │       ├── p276.wav
│   │       ├── p277.wav
│   │       ├── p278.wav
│   │       ├── p279.wav
│   │       ├── p280.wav
│   │       ├── p281.wav
│   │       ├── p282.wav
│   │       ├── p283.wav
│   │       ├── p284.wav
│   │       ├── p285.wav
│   │       ├── p286.wav
│   │       ├── p287.wav
│   │       ├── p288.wav
│   │       ├── p292.wav
│   │       ├── p293.wav
│   │       ├── p294.wav
│   │       ├── p295.wav
│   │       ├── p297.wav
│   │       ├── p298.wav
│   │       ├── p299.wav
│   │       ├── p300.wav
│   │       ├── p301.wav
│   │       ├── p302.wav
│   │       ├── p303.wav
│   │       ├── p304.wav
│   │       ├── p305.wav
│   │       ├── p306.wav
│   │       ├── p307.wav
│   │       ├── p308.wav
│   │       ├── p310.wav
│   │       ├── p312.wav
│   │       ├── p313.wav
│   │       ├── p314.wav
│   │       ├── p315.wav
│   │       ├── p316.wav
│   │       ├── p317.wav
│   │       ├── p318.wav
│   │       ├── p323.wav
│   │       ├── p326.wav
│   │       ├── p329.wav
│   │       ├── p330.wav
│   │       ├── p333.wav
│   │       ├── p334.wav
│   │       ├── p335.wav
│   │       ├── p336.wav
│   │       ├── p339.wav
│   │       ├── p340.wav
│   │       ├── p341.wav
│   │       ├── p345.wav
│   │       ├── p347.wav
│   │       ├── p351.wav
│   │       ├── p360.wav
│   │       ├── p361.wav
│   │       ├── p362.wav
│   │       ├── p363.wav
│   │       ├── p364.wav
│   │       ├── p374.wav
│   │       ├── p376.wav
│   │       ├── s5.wav
│   │       ├── 张国荣-A SONG FOR YOU.wav
│   │       ├── 张国荣-DREAMING My Other Half.wav
│   │       ├── 张国荣-Don't Lie To Me.wav
│   │       ├── 张国荣-Dreaming (Remix).wav
│   │       ├── 张国荣-Hot Summer (Live).wav
│   │       ├── 张国荣-Hot Summer (Remix).wav
│   │       ├── 张国荣-I Honestly Love You.wav
│   │       ├── 张国荣-MISS YOU MUCH Missing you.wav
│   │       ├── 张国荣-Medley ： 不怕寂寞 、 我愿意 (Live).wav
│   │       ├── 张国荣-Medley ： 抵抗夜寒、 黑色午夜 、 热辣辣 (Live).wav
│   │       ├── 张国荣-Medley ： 有谁共鸣 、 沉默是金 (Live).wav
│   │       ├── 张国荣-Medley ： 童年时 、 似水流年、 但愿人长久 (Live).wav
│   │       ├── 张国荣-Medley ： 蓝色忧郁 、 少女心事 、 不羁的风 、 Monica (Live).wav
│   │       ├── 张国荣-Medley： H2O 、黑色午夜、 隐身人、第一次、Stand Up (Live).wav
│   │       ├── 张国荣-Miss You Much (Live).wav
│   │       ├── 张国荣-Miss You Much (Remix).wav
│   │       ├── 张国荣-Monica (Live).wav
│   │       ├── 张国荣-Opening 、 贴身 (Live).wav
│   │       ├── 张国荣-PROLOGUE.wav
│   │       ├── 张国荣-Stories (Live).wav
│   │       ├── 张国荣-The Way We Were (Live).wav
│   │       ├── 张国荣-Why (Remix).wav
│   │       ├── 张国荣-不怕寂寞 (Live).wav
│   │       ├── 张国荣-不想拥抱我的人.wav
│   │       ├── 张国荣-不羁的风 (Live).wav
│   │       ├── 张国荣-为你钟情 (Live).wav
│   │       ├── 张国荣-你在何地 (Live).wav
│   │       ├── 张国荣-你我之间.wav
│   │       ├── 张国荣-侧面 (Live).wav
│   │       ├── 张国荣-侧面 (Remix).wav
│   │       ├── 张国荣-侧面 Silhouette.wav
│   │       ├── 张国荣-侬本多情 (Live).wav
│   │       ├── 张国荣-侯斯顿之恋.wav
│   │       ├── 张国荣-倩女幽魂 (Live) (2).wav
│   │       ├── 张国荣-倩女幽魂 (Live).wav
│   │       ├── 张国荣-偏心 (Live).wav
│   │       ├── 张国荣-偷情.wav
│   │       ├── 张国荣-共创真善美.wav
│   │       ├── 张国荣-共同渡过 (Live) (2).wav
│   │       ├── 张国荣-共同渡过 (Live).wav
│   │       ├── 张国荣-共同渡过 Together Forever.wav
│   │       ├── 张国荣-千千阕歌 (Live).wav
│   │       ├── 张国荣-千娇百美.wav
│   │       ├── 张国荣-午后红茶.wav
│   │       ├── 张国荣-发烧.wav
│   │       ├── 张国荣-够了 (Live).wav
│   │       ├── 张国荣-够了 (Remix).wav
│   │       ├── 张国荣-大热.wav
│   │       ├── 张国荣-奇迹.wav
│   │       ├── 张国荣-奔向未来日子 (Live).wav
│   │       ├── 张国荣-客途秋恨 (Live).wav
│   │       ├── 张国荣-寂寞夜晚 (Live).wav
│   │       ├── 张国荣-当年情 (Live) (2).wav
│   │       ├── 张国荣-怨男.wav
│   │       ├── 张国荣-怪你过份美丽.wav
│   │       ├── 张国荣-想你 (Live).wav
│   │       ├── 张国荣-想妳 (Live).wav
│   │       ├── 张国荣-想妳 On My Mind.wav
│   │       ├── 张国荣-意犹未尽.wav
│   │       ├── 张国荣-我 (Mandarin Version) (2).wav
│   │       ├── 张国荣-我 (Mandarin Version).wav
│   │       ├── 张国荣-我.wav
│   │       ├── 张国荣-我知你好.wav
│   │       ├── 张国荣-打开信箱 (Live).wav
│   │       ├── 张国荣-拒绝再玩 (Live).wav
│   │       ├── 张国荣-拒绝再玩 (Remix).wav
│   │       ├── 张国荣-挪亚方舟.wav
│   │       ├── 张国荣-放荡 (Live).wav
│   │       ├── 张国荣-放荡 (Remix).wav
│   │       ├── 张国荣-敢爱.wav
│   │       ├── 张国荣-无心睡眠 (Live) (2).wav
│   │       ├── 张国荣-无心睡眠 (Live).wav
│   │       ├── 张国荣-无心睡眠 (Live丨reprise).wav
│   │       ├── 张国荣-无心睡眠 (Remix).wav
│   │       ├── 张国荣-无心睡眠 Sleepless nights Restless heart.wav
│   │       ├── 张国荣-无需要太多 (Live) (2).wav
│   │       ├── 张国荣-无需要太多 (Live).wav
│   │       ├── 张国荣-无需要太多 Love is Enough.wav
│   │       ├── 张国荣-明星 (Live).wav
│   │       ├── 张国荣-暴风一族 (Live).wav
│   │       ├── 张国荣-暴风一族 (Remix).wav
│   │       ├── 张国荣-最爱 (Live) (2).wav
│   │       ├── 张国荣-最爱 (Live).wav
│   │       ├── 张国荣-最爱是谁 My Dearest.wav
│   │       ├── 张国荣-有心人.wav
│   │       ├── 张国荣-有谁共鸣 (Live).wav
│   │       ├── 张国荣-沉默是金 (Live).wav
│   │       ├── 张国荣-没有烟总有花.wav
│   │       ├── 张国荣-没有爱.wav
│   │       ├── 张国荣-烈火边缘 (Live).wav
│   │       ├── 张国荣-热辣辣 (Live).wav
│   │       ├── 张国荣-热辣辣 (Remix).wav
│   │       ├── 张国荣-爱慕 (Live) (2).wav
│   │       ├── 张国荣-爱慕 (Live).wav
│   │       ├── 张国荣-爱的凶手 (Live).wav
│   │       ├── 张国荣-爱的凶手.wav
│   │       ├── 张国荣-玻璃之情.wav
│   │       ├── 张国荣-由零开始 (Live).wav
│   │       ├── 张国荣-由零开始 Will You Remember Me.wav
│   │       ├── 张国荣-禁片 (Remix).wav
│   │       ├── 张国荣-红.wav
│   │       ├── 张国荣-红蝴蝶.wav
│   │       ├── 张国荣-继续跳舞 (Live).wav
│   │       ├── 张国荣-继续跳舞 (Remix).wav
│   │       ├── 张国荣-胭脂扣 (Live).wav
│   │       ├── 张国荣-蝶变.wav
│   │       ├── 张国荣-访英台 (Live).wav
│   │       ├── 张国荣-请勿越轨 (Live).wav
│   │       ├── 张国荣-谈情说爱.wav
│   │       ├── 张国荣-身边有人.wav
│   │       ├── 张国荣-还有谁.wav
│   │       ├── 张国荣-随心.wav
│   │       ├── 张国荣-风再起时 (Live).wav
│   │       ├── 张国荣-风继续吹 (Live) (2).wav
│   │       ├── 张国荣-风继续吹 (Live).wav
│   │       ├── 張國榮.LESLIE CHEUNG LPCD45II.wav
│   │       └── 柏安妮-不想再拥有 (Live).wav
│   └── valid/
│       ├── high/
│       │   ├── 01 - 侧面.wav
│       │   ├── 02 - 共同渡过.wav
│       │   ├── 03 - 片段 (2).wav
│       │   ├── 03 - 红颜白发.wav
│       │   ├── 04 - Love Me More.wav
│       │   ├── 04 - 共您别离.wav
│       │   ├── 05 - 为谁疯癫.wav
│       │   ├── 06 - Dreaming.wav
│       │   ├── 06 - 无心睡眠.wav
│       │   ├── 06 - 月正亮.wav
│       │   ├── 06 - 知道爱.wav
│       │   ├── 06 - 蓝色忧郁.wav
│       │   ├── 07 - Monica.wav
│       │   ├── 07 - 情难自控 (2).wav
│       │   ├── 07 - 禁片.wav
│       │   ├── 07 - 风继续吹 (2).wav
│       │   ├── 08 - 惊梦.wav
│       │   ├── 09 - Stand Up(Remix).wav
│       │   ├── 09 - 以后.wav
│       │   ├── 09 - 倩女幽魂.wav
│       │   ├── 10 - 不羁的风.wav
│       │   ├── 10 - 侬本多情.wav
│       │   ├── 10 - 倩女幽魂 (3).wav
│       │   ├── 10 - 倩女幽魂.wav
│       │   ├── 10 - 爱的狂徒.wav
│       │   ├── 10 - 这些年来.wav
│       │   ├── 11 - 无胆入情关.wav
│       │   ├── 11 - 春夏秋冬.wav
│       │   ├── 13 - 一片痴.wav
│       │   ├── 14 - 恋爱交叉.wav
│       │   ├── 14 - 甜蜜的禁果.wav
│       │   ├── 15 - 风再起时.wav
│       │   ├── 18 - 午夜奔驰(国语).wav
│       │   ├── SSB0033.wav
│       │   ├── SSB0139.wav
│       │   ├── SSB0609.wav
│       │   ├── SSB0737.wav
│       │   ├── SSB0748.wav
│       │   ├── SSB0919.wav
│       │   ├── SSB1828.wav
│       │   ├── p230.wav
│       │   ├── p254.wav
│       │   ├── p257.wav
│       │   ├── p271.wav
│       │   ├── p311.wav
│       │   ├── p343.wav
│       │   ├── 张国荣-Opening 、 为你钟情 (Live).wav
│       │   ├── 张国荣-侬本多情 (Live) (2).wav
│       │   ├── 张国荣-当年情 (Live).wav
│       │   └── 张国荣-愿你决定.wav
│       └── low/
│           ├── 01 - 侧面.wav
│           ├── 02 - 共同渡过.wav
│           ├── 03 - 片段 (2).wav
│           ├── 03 - 红颜白发.wav
│           ├── 04 - Love Me More.wav
│           ├── 04 - 共您别离.wav
│           ├── 05 - 为谁疯癫.wav
│           ├── 06 - Dreaming.wav
│           ├── 06 - 无心睡眠.wav
│           ├── 06 - 月正亮.wav
│           ├── 06 - 知道爱.wav
│           ├── 06 - 蓝色忧郁.wav
│           ├── 07 - Monica.wav
│           ├── 07 - 情难自控 (2).wav
│           ├── 07 - 禁片.wav
│           ├── 07 - 风继续吹 (2).wav
│           ├── 08 - 惊梦.wav
│           ├── 09 - Stand Up(Remix).wav
│           ├── 09 - 以后.wav
│           ├── 09 - 倩女幽魂.wav
│           ├── 10 - 不羁的风.wav
│           ├── 10 - 侬本多情.wav
│           ├── 10 - 倩女幽魂 (3).wav
│           ├── 10 - 倩女幽魂.wav
│           ├── 10 - 爱的狂徒.wav
│           ├── 10 - 这些年来.wav
│           ├── 11 - 无胆入情关.wav
│           ├── 11 - 春夏秋冬.wav
│           ├── 13 - 一片痴.wav
│           ├── 14 - 恋爱交叉.wav
│           ├── 14 - 甜蜜的禁果.wav
│           ├── 15 - 风再起时.wav
│           ├── 18 - 午夜奔驰(国语).wav
│           ├── SSB0033.wav
│           ├── SSB0139.wav
│           ├── SSB0609.wav
│           ├── SSB0737.wav
│           ├── SSB0748.wav
│           ├── SSB0919.wav
│           ├── SSB1828.wav
│           ├── p230.wav
│           ├── p254.wav
│           ├── p257.wav
│           ├── p271.wav
│           ├── p311.wav
│           ├── p343.wav
│           ├── 张国荣-Opening 、 为你钟情 (Live).wav
│           ├── 张国荣-侬本多情 (Live) (2).wav
│           ├── 张国荣-当年情 (Live).wav
│           └── 张国荣-愿你决定.wav
├── dataset_loader.py
├── inference.py
├── inspector.py
├── model.py
├── package.py
├── preprocess_audio.py
├── raw_audio/
│   ├── SSB0005.wav
│   ├── SSB0009.wav
│   ├── SSB0011.wav
│   ├── SSB0012.wav
│   ├── SSB0016.wav
│   ├── SSB0018.wav
│   ├── SSB0033.wav
│   ├── SSB0038.wav
│   ├── SSB0043.wav
│   ├── SSB0057.wav
│   ├── SSB0073.wav
│   ├── SSB0080.wav
│   ├── SSB0112.wav
│   ├── SSB0122.wav
│   ├── SSB0133.wav
│   ├── SSB0139.wav
│   ├── SSB0145.wav
│   ├── SSB0149.wav
│   ├── SSB0193.wav
│   ├── SSB0197.wav
│   ├── SSB0200.wav
│   ├── SSB0241.wav
│   ├── SSB0246.wav
│   ├── SSB0261.wav
│   ├── SSB0267.wav
│   ├── SSB0273.wav
│   ├── SSB0287.wav
│   ├── SSB0288.wav
│   ├── SSB0299.wav
│   ├── SSB0307.wav
│   ├── SSB0309.wav
│   ├── SSB0315.wav
│   ├── SSB0316.wav
│   ├── SSB0323.wav
│   ├── SSB0338.wav
│   ├── SSB0339.wav
│   ├── SSB0341.wav
│   ├── SSB0342.wav
│   ├── SSB0354.wav
│   ├── SSB0366.wav
│   ├── SSB0375.wav
│   ├── SSB0379.wav
│   ├── SSB0380.wav
│   ├── SSB0382.wav
│   ├── SSB0385.wav
│   ├── SSB0393.wav
│   ├── SSB0394.wav
│   ├── SSB0395.wav
│   ├── SSB0407.wav
│   ├── SSB0415.wav
│   ├── SSB0426.wav
│   ├── SSB0427.wav
│   ├── SSB0434.wav
│   ├── SSB0435.wav
│   ├── SSB0470.wav
│   ├── SSB0482.wav
│   ├── SSB0502.wav
│   ├── SSB0534.wav
│   ├── SSB0535.wav
│   ├── SSB0539.wav
│   ├── SSB0544.wav
│   ├── SSB0565.wav
│   ├── SSB0570.wav
│   ├── SSB0578.wav
│   ├── SSB0588.wav
│   ├── SSB0590.wav
│   ├── SSB0594.wav
│   ├── SSB0599.wav
│   ├── SSB0601.wav
│   ├── SSB0603.wav
│   ├── SSB0606.wav
│   ├── SSB0607.wav
│   ├── SSB0609.wav
│   ├── SSB0614.wav
│   ├── SSB0623.wav
│   ├── SSB0629.wav
│   ├── SSB0631.wav
│   ├── SSB0632.wav
│   ├── SSB0666.wav
│   ├── SSB0668.wav
│   ├── SSB0671.wav
│   ├── SSB0686.wav
│   ├── SSB0700.wav
│   ├── SSB0710.wav
│   ├── SSB0720.wav
│   ├── SSB0723.wav
│   ├── SSB0737.wav
│   ├── SSB0746.wav
│   ├── SSB0748.wav
│   ├── SSB0751.wav
│   ├── SSB0758.wav
│   ├── SSB0760.wav
│   ├── SSB0762.wav
│   ├── SSB0778.wav
│   ├── SSB0780.wav
│   ├── SSB0784.wav
│   ├── SSB0786.wav
│   ├── SSB0794.wav
│   ├── SSB0817.wav
│   ├── SSB0851.wav
│   ├── SSB0863.wav
│   ├── SSB0871.wav
│   ├── SSB0887.wav
│   ├── SSB0913.wav
│   ├── SSB0915.wav
│   ├── SSB0919.wav
│   ├── SSB0935.wav
│   ├── SSB0966.wav
│   ├── SSB0987.wav
│   ├── SSB1008.wav
│   ├── SSB1020.wav
│   ├── SSB1024.wav
│   ├── SSB1050.wav
│   ├── SSB1055.wav
│   ├── SSB1056.wav
│   ├── SSB1064.wav
│   ├── SSB1072.wav
│   ├── SSB1091.wav
│   ├── SSB1096.wav
│   ├── SSB1100.wav
│   ├── SSB1108.wav
│   ├── SSB1115.wav
│   ├── SSB1125.wav
│   ├── SSB1131.wav
│   ├── SSB1136.wav
│   ├── SSB1138.wav
│   ├── SSB1161.wav
│   ├── SSB1203.wav
│   ├── SSB1204.wav
│   ├── SSB1218.wav
│   ├── SSB1221.wav
│   ├── SSB1253.wav
│   ├── SSB1320.wav
│   ├── SSB1341.wav
│   ├── SSB1366.wav
│   ├── SSB1377.wav
│   ├── SSB1383.wav
│   ├── SSB1385.wav
│   ├── SSB1392.wav
│   ├── SSB1393.wav
│   ├── SSB1408.wav
│   ├── SSB1431.wav
│   ├── SSB1437.wav
│   ├── SSB1448.wav
│   ├── SSB1555.wav
│   ├── SSB1563.wav
│   ├── SSB1567.wav
│   ├── SSB1575.wav
│   ├── SSB1585.wav
│   ├── SSB1593.wav
│   ├── SSB1607.wav
│   ├── SSB1624.wav
│   ├── SSB1625.wav
│   ├── SSB1630.wav
│   ├── SSB1650.wav
│   ├── SSB1670.wav
│   ├── SSB1684.wav
│   ├── SSB1686.wav
│   ├── SSB1699.wav
│   ├── SSB1711.wav
│   ├── SSB1759.wav
│   ├── SSB1806.wav
│   ├── SSB1828.wav
│   ├── SSB1831.wav
│   ├── SSB1832.wav
│   ├── SSB1837.wav
│   ├── SSB1846.wav
│   ├── SSB1863.wav
│   ├── SSB1878.wav
│   ├── SSB1891.wav
│   ├── SSB1918.wav
│   ├── SSB1935.wav
│   ├── SSB1939.wav
│   └── SSB1956.wav
├── raw_audioII/
│   ├── 01 - A Thousand Dreams Of You.wav
│   ├── 01 - Day Dreamer.wav
│   ├── 01 - Dreaming (2).wav
│   ├── 01 - Dreaming.wav
│   ├── 01 - Hot Summer.wav
│   ├── 01 - MY GOD.wav
│   ├── 01 - Medley：有谁共鸣-沉默是金.wav
│   ├── 01 - Opening 风再起时.wav
│   ├── 01 - Opening-为你钟情.wav
│   ├── 01 - Overture.wav
│   ├── 01 - Stand Up.wav
│   ├── 01 - Tonight and Forever.wav
│   ├── 01 - マツュマロ.wav
│   ├── 01 - 一片痴.wav
│   ├── 01 - 不羁的风.wav
│   ├── 01 - 为你钟情.wav
│   ├── 01 - 侧面.wav
│   ├── 01 - 停止转动(国语).wav
│   ├── 01 - 全赖有你.wav
│   ├── 01 - 取暖.wav
│   ├── 01 - 只怕不再遇上 Live.wav
│   ├── 01 - 够了.wav
│   ├── 01 - 始终会行运.wav
│   ├── 01 - 当爱已成往事.wav
│   ├── 01 - 当真就好（陈淑桦合唱）.wav
│   ├── 01 - 情人箭.wav
│   ├── 01 - 情难再续.wav
│   ├── 01 - 拒绝再玩 (2).wav
│   ├── 01 - 拒绝再玩 (3).wav
│   ├── 01 - 拒绝再玩 (4).wav
│   ├── 01 - 拒绝再玩.wav
│   ├── 01 - 有谁共鸣.wav
│   ├── 01 - 油脂热潮.wav
│   ├── 01 - 烈火边缘.wav
│   ├── 01 - 爱慕 (2).wav
│   ├── 01 - 爱慕.wav
│   ├── 01 - 爱的凶手 (2).wav
│   ├── 01 - 爱的凶手.wav
│   ├── 01 - 由零开始.wav
│   ├── 01 - 痛心（袁咏仪）.wav
│   ├── 01 - 红.wav
│   ├── 01 - 贴身.wav
│   ├── 01 - 路过蜻蜓.wav
│   ├── 01 - 这些年来.wav
│   ├── 01 - 醉死梦生.wav
│   ├── 01 - 风再起时.wav
│   ├── 01 - 风继续吹 (2).wav
│   ├── 01 - 风继续吹.wav
│   ├── 02 - Hot Summer (2).wav
│   ├── 02 - Hot Summer.wav
│   ├── 02 - I miss you much.wav
│   ├── 02 - Miss You Much.wav
│   ├── 02 - Monica(Remix).wav
│   ├── 02 - Monica.wav
│   ├── 02 - My God (2).wav
│   ├── 02 - My God.wav
│   ├── 02 - Stand Up.wav
│   ├── 02 - We Are All Alone.wav
│   ├── 02 - 三岁仔.wav
│   ├── 02 - 上帝.wav
│   ├── 02 - 为你 (2).wav
│   ├── 02 - 为你.wav
│   ├── 02 - 今生今世 (2).wav
│   ├── 02 - 今生今世.wav
│   ├── 02 - 你这样恨我.wav
│   ├── 02 - 侧面.wav
│   ├── 02 - 侬本多情 (2).wav
│   ├── 02 - 侬本多情.wav
│   ├── 02 - 倩女幽魂.wav
│   ├── 02 - 共同渡过.wav
│   ├── 02 - 少女心事.wav
│   ├── 02 - 左右手.wav
│   ├── 02 - 当年情.wav
│   ├── 02 - 当真就好.wav
│   ├── 02 - 怨男 Live.wav
│   ├── 02 - 我走我路.wav
│   ├── 02 - 拒绝再玩.wav
│   ├── 02 - 无心睡眠 (2).wav
│   ├── 02 - 无心睡眠.wav
│   ├── 02 - 枕头.wav
│   ├── 02 - 梦到内河.wav
│   ├── 02 - 梦死醉生.wav
│   ├── 02 - 浣花洗剑录.wav
│   ├── 02 - 深情相拥（辛晓琪合唱）.wav
│   ├── 02 - 烈火灯蛾.wav
│   ├── 02 - 热辣辣.wav
│   ├── 02 - 痴心的我.wav
│   ├── 02 - 真相.wav
│   ├── 02 - 第一次.wav
│   ├── 02 - 贴身 (2).wav
│   ├── 02 - 贴身.wav
│   ├── 02 - 迷惑我.wav
│   ├── 02 - 那一记耳光 (2).wav
│   ├── 02 - 那一记耳光.wav
│   ├── 02 - 风继续吹.wav
│   ├── 02 - 黑色午夜.wav
│   ├── 03 - Even Now.wav
│   ├── 03 - Forever爱你.wav
│   ├── 03 - H2O.wav
│   ├── 03 - Love Like Magic.wav
│   ├── 03 - Medley：恋爱交叉-打开信箱-蓝色忧郁-Monica.wav
│   ├── 03 - Miss You Much.wav
│   ├── 03 - Miss you much (2).wav
│   ├── 03 - Monica.wav
│   ├── 03 - Stand Up.wav
│   ├── 03 - 为你钟情.wav
│   ├── 03 - 以后.wav
│   ├── 03 - 你在何地 (2).wav
│   ├── 03 - 你在何地.wav
│   ├── 03 - 侧面.wav
│   ├── 03 - 侬本多情.wav
│   ├── 03 - 分手.wav
│   ├── 03 - 夜半歌声.wav
│   ├── 03 - 奔向未来日子.wav
│   ├── 03 - 宿醉.wav
│   ├── 03 - 寂寞夜晚.wav
│   ├── 03 - 寂寞有害.wav
│   ├── 03 - 左右手(Acoustic Mix).wav
│   ├── 03 - 左右手.wav
│   ├── 03 - 当年情.wav
│   ├── 03 - 情到浓时.wav
│   ├── 03 - 想你.wav
│   ├── 03 - 我愿意.wav
│   ├── 03 - 打开信箱.wav
│   ├── 03 - 放荡.wav
│   ├── 03 - 无心睡眠 (2).wav
│   ├── 03 - 无心睡眠.wav
│   ├── 03 - 无需要太多.wav
│   ├── 03 - 春夏秋冬.wav
│   ├── 03 - 柔情蜜意.wav
│   ├── 03 - 沈胜衣.wav
│   ├── 03 - 洁身自爱.wav
│   ├── 03 - 浣花洗剑录.wav
│   ├── 03 - 深情相拥（辛晓琪合唱）.wav
│   ├── 03 - 热辣辣 Live.wav
│   ├── 03 - 热辣辣.wav
│   ├── 03 - 爱情路里.wav
│   ├── 03 - 片段 (2).wav
│   ├── 03 - 片段.wav
│   ├── 03 - 片段（袁咏仪合唱）.wav
│   ├── 03 - 红颜白发.wav
│   ├── 03 - 追.wav
│   ├── 03 - 透明的你 (2).wav
│   ├── 03 - 透明的你.wav
│   ├── 03 - 风继续吹.wav
│   ├── 04 - Before My Heart Finds Out.wav
│   ├── 04 - Everybody.wav
│   ├── 04 - H2O.wav
│   ├── 04 - Love Me More.wav
│   ├── 04 - Medley：想你-偷情 Live.wav
│   ├── 04 - Medley：蓝色忧郁-少女心事-不羁的风-Monica.wav
│   ├── 04 - Stories.wav
│   ├── 04 - Why.wav
│   ├── 04 - 不要爱他.wav
│   ├── 04 - 今生今世.wav
│   ├── 04 - 侧面 (2).wav
│   ├── 04 - 侧面.wav
│   ├── 04 - 偏心.wav
│   ├── 04 - 共你别离.wav
│   ├── 04 - 共您别离.wav
│   ├── 04 - 变色龙.wav
│   ├── 04 - 只怕不再遇上.wav
│   ├── 04 - 可否多一吻.wav
│   ├── 04 - 失散的影子.wav
│   ├── 04 - 寂寞有害.wav
│   ├── 04 - 愿能比翼飞.wav
│   ├── 04 - 我(国语).wav
│   ├── 04 - 我眼中的她.wav
│   ├── 04 - 打开信箱.wav
│   ├── 04 - 无形锁扣.wav
│   ├── 04 - 无心锁扣.wav
│   ├── 04 - 无需要太多.wav
│   ├── 04 - 明星.wav
│   ├── 04 - 暴风一族.wav
│   ├── 04 - 最冷一天.wav
│   ├── 04 - 枕头.wav
│   ├── 04 - 柔情蜜意 (2).wav
│   ├── 04 - 柔情蜜意.wav
│   ├── 04 - 没有烟总有花.wav
│   ├── 04 - 深情相拥.wav
│   ├── 04 - 爱情离合器.wav
│   ├── 04 - 爱慕.wav
│   ├── 04 - 狂野如我 (2).wav
│   ├── 04 - 狂野如我.wav
│   ├── 04 - 甜蜜的禁果.wav
│   ├── 04 - 电风扇.wav
│   ├── 04 - 痴心的我.wav
│   ├── 04 - 痴心（袁咏仪）.wav
│   ├── 04 - 红.wav
│   ├── 04 - 迷惑我.wav
│   ├── 04 - 追.wav
│   ├── 04 - 隐身人.wav
│   ├── 04 - 雪中情.wav
│   ├── 04 - 风继续吹(Mix).wav
│   ├── 04 - 风继续吹.wav
│   ├── 05 - FROM NOW ON（林忆莲合唱）.wav
│   ├── 05 - Good Morning Sorrow.wav
│   ├── 05 - H2O.wav
│   ├── 05 - Hey!不要玩.wav
│   ├── 05 - I Honestly Love You.wav
│   ├── 05 - Medley：H2O-黑色午夜-隐身人-第一次-Stand Up.wav
│   ├── 05 - 一片痴 (2).wav
│   ├── 05 - 一片痴.wav
│   ├── 05 - 为谁疯癫.wav
│   ├── 05 - 从未可以.wav
│   ├── 05 - 侧面.wav
│   ├── 05 - 共同渡过.wav
│   ├── 05 - 到未来日子 (2).wav
│   ├── 05 - 到未来日子.wav
│   ├── 05 - 夜半歌声(电影版).wav
│   ├── 05 - 大亨.wav
│   ├── 05 - 大报复.wav
│   ├── 05 - 奔向未来日子.wav
│   ├── 05 - 妄想 (2).wav
│   ├── 05 - 妄想.wav
│   ├── 05 - 宿醉.wav
│   ├── 05 - 当年情 (2).wav
│   ├── 05 - 当年情.wav
│   ├── 05 - 当爱已成往事.wav
│   ├── 05 - 心中情.wav
│   ├── 05 - 心跳呼吸正常.wav
│   ├── 05 - 恋爱交叉.wav
│   ├── 05 - 我愿意.wav
│   ├── 05 - 拒绝再玩.wav
│   ├── 05 - 最爱.wav
│   ├── 05 - 月亮代表我的心(Live).wav
│   ├── 05 - 有心人.wav
│   ├── 05 - 深情相拥 Live.wav
│   ├── 05 - 燕子的故事.wav
│   ├── 05 - 爱慕 (2).wav
│   ├── 05 - 爱慕 (3).wav
│   ├── 05 - 爱慕.wav
│   ├── 05 - 由零开始.wav
│   ├── 05 - 绝不可以.wav
│   ├── 05 - 缘份.wav
│   ├── 05 - 缘份有几多 (2).wav
│   ├── 05 - 缘份有几多.wav
│   ├── 05 - 缘份（梅艳芳合唱）.wav
│   ├── 05 - 蓝色忧郁.wav
│   ├── 05 - 被爱.wav
│   ├── 05 - 谁令你心痴（陈洁灵合唱）.wav
│   ├── 05 - 陪你倒数.wav
│   ├── 05 - 需要你.wav
│   ├── 06 - Dreaming.wav
│   ├── 06 - FROM NOW ON.wav
│   ├── 06 - H2O Medley：H2O-少女心事-第一次-不羁的风.wav
│   ├── 06 - Medley：童年时-似水流年-但愿人长久.wav
│   ├── 06 - Undercover Angel.wav
│   ├── 06 - 一片痴.wav
│   ├── 06 - 一辈子失去了妳.wav
│   ├── 06 - 为你.wav
│   ├── 06 - 为你钟情.wav
│   ├── 06 - 今生今世.wav
│   ├── 06 - 侧面.wav
│   ├── 06 - 全身都是爱.wav
│   ├── 06 - 共同渡过 (2).wav
│   ├── 06 - 共同渡过 (3).wav
│   ├── 06 - 共同渡过.wav
│   ├── 06 - 天使之爱 (2).wav
│   ├── 06 - 天使之爱.wav
│   ├── 06 - 小明星.wav
│   ├── 06 - 当年情.wav
│   ├── 06 - 情自困.wav
│   ├── 06 - 想你 (2).wav
│   ├── 06 - 想你.wav
│   ├── 06 - 拒绝再玩.wav
│   ├── 06 - 无心睡眠.wav
│   ├── 06 - 无需要太多.wav
│   ├── 06 - 暴风一族 (2).wav
│   ├── 06 - 暴风一族.wav
│   ├── 06 - 月正亮.wav
│   ├── 06 - 柔情蜜意 (2).wav
│   ├── 06 - 柔情蜜意.wav
│   ├── 06 - 沉默是金.wav
│   ├── 06 - 流浪 (2).wav
│   ├── 06 - 流浪.wav
│   ├── 06 - 深情相拥（辛晓琪合唱）.wav
│   ├── 06 - 爱情离合器.wav
│   ├── 06 - 爱慕.wav
│   ├── 06 - 爱的抉择.wav
│   ├── 06 - 真相.wav
│   ├── 06 - 知道爱.wav
│   ├── 06 - 第一次.wav
│   ├── 06 - 蓝色忧郁.wav
│   ├── 06 - 谈情说爱 Live.wav
│   ├── 06 - 这刻相见后.wav
│   ├── 06 - 追族.wav
│   ├── 06 - 追逐.wav
│   ├── 06 - 风继续吹.wav
│   ├── 06 - 黑色午夜.wav
│   ├── 07 - I Like Dreamin'.wav
│   ├── 07 - Medley：红颜白发-最爱 Live.wav
│   ├── 07 - Miss You Much.wav
│   ├── 07 - Monica (2).wav
│   ├── 07 - Monica.wav
│   ├── 07 - 不怕寂寞.wav
│   ├── 07 - 不羁的风.wav
│   ├── 07 - 为你钟情.wav
│   ├── 07 - 作伴.wav
│   ├── 07 - 你教我点好.wav
│   ├── 07 - 侬本多情 (2).wav
│   ├── 07 - 侬本多情.wav
│   ├── 07 - 偷情.wav
│   ├── 07 - 共同渡过 (2).wav
│   ├── 07 - 共同渡过.wav
│   ├── 07 - 千千阕歌.wav
│   ├── 07 - 同道中人.wav
│   ├── 07 - 夜半歌声.wav
│   ├── 07 - 奔向未来日子.wav
│   ├── 07 - 寂寞夜晚.wav
│   ├── 07 - 少女心事.wav
│   ├── 07 - 恋爱交叉.wav
│   ├── 07 - 情人箭.wav
│   ├── 07 - 情难自控 (2).wav
│   ├── 07 - 情难自控.wav
│   ├── 07 - 我愿意.wav
│   ├── 07 - 打开信箱 (2).wav
│   ├── 07 - 打开信箱.wav
│   ├── 07 - 放荡.wav
│   ├── 07 - 无心睡眠.wav
│   ├── 07 - 有谁共鸣.wav
│   ├── 07 - 烧毁我眼睛.wav
│   ├── 07 - 爱情路里.wav
│   ├── 07 - 留住昨天.wav
│   ├── 07 - 眉来眼去（辛晓琪合唱）.wav
│   ├── 07 - 禁片.wav
│   ├── 07 - 继续跳舞.wav
│   ├── 07 - 触电.wav
│   ├── 07 - 让我飞 (2).wav
│   ├── 07 - 让我飞.wav
│   ├── 07 - 路过蜻蜒.wav
│   ├── 07 - 追.wav
│   ├── 07 - 野火.wav
│   ├── 07 - 风再起时.wav
│   ├── 07 - 风继续吹 (2).wav
│   ├── 07 - 风继续吹.wav
│   ├── 07 - 黑色午夜.wav
│   ├── 08 - H2O.wav
│   ├── 08 - I Need You.wav
│   ├── 08 - Stand Up-Twist & Shout-Stand Up.wav
│   ├── 08 - 一辈子失去了妳.wav
│   ├── 08 - 不怕寂寞.wav
│   ├── 08 - 不管您是谁.wav
│   ├── 08 - 不羁的风.wav
│   ├── 08 - 不要爱他.wav
│   ├── 08 - 为妳钟情.wav
│   ├── 08 - 你是我一半.wav
│   ├── 08 - 你的一切.wav
│   ├── 08 - 侧面-放荡.wav
│   ├── 08 - 侬本多情.wav
│   ├── 08 - 兜风心情.wav
│   ├── 08 - 兜风心情（柏安妮合唱）.wav
│   ├── 08 - 共同渡过.wav
│   ├── 08 - 刻骨铭心.wav
│   ├── 08 - 取暖.wav
│   ├── 08 - 够了 (2).wav
│   ├── 08 - 够了.wav
│   ├── 08 - 妳教我点好.wav
│   ├── 08 - 客途秋恨.wav
│   ├── 08 - 寂寞夜晚.wav
│   ├── 08 - 少女心事.wav
│   ├── 08 - 惊梦.wav
│   ├── 08 - 我愿意.wav
│   ├── 08 - 无心睡眠.wav
│   ├── 08 - 明星 Live.wav
│   ├── 08 - 明星.wav
│   ├── 08 - 暴风一族.wav
│   ├── 08 - 最爱.wav
│   ├── 08 - 有心人 (2).wav
│   ├── 08 - 有心人.wav
│   ├── 08 - 浓情.wav
│   ├── 08 - 烈火边缘.wav
│   ├── 08 - 热辣辣.wav
│   ├── 08 - 爱有万万千.wav
│   ├── 08 - 爱火.wav
│   ├── 08 - 痴心的我.wav
│   ├── 08 - 眉来眼去.wav
│   ├── 08 - 眉来眼去（辛晓琪合唱）.wav
│   ├── 08 - 知道爱.wav
│   ├── 08 - 继续跳舞.wav
│   ├── 08 - 请勿越轨.wav
│   ├── 08 - 谁负了谁.wav
│   ├── 08 - 难以再说对不起.wav
│   ├── 09 - Medley：阿飞正传-梦-A Thousand Dreams of You.wav
│   ├── 09 - Monica.wav
│   ├── 09 - Stand Up(Remix).wav
│   ├── 09 - The Way We Were.wav
│   ├── 09 - WHY.wav
│   ├── 09 - You Made Me Believe In Magic.wav
│   ├── 09 - 七色的爱.wav
│   ├── 09 - 三岁仔.wav
│   ├── 09 - 不怕寂寞.wav
│   ├── 09 - 不管你是谁.wav
│   ├── 09 - 不羁的风.wav
│   ├── 09 - 为你钟情.wav
│   ├── 09 - 以后.wav
│   ├── 09 - 你在何地.wav
│   ├── 09 - 倩女幽魂 (2).wav
│   ├── 09 - 倩女幽魂.wav
│   ├── 09 - 内心争斗.wav
│   ├── 09 - 别话.wav
│   ├── 09 - 妒忌.wav
│   ├── 09 - 寂寞猎人.wav
│   ├── 09 - 找一个地方.wav
│   ├── 09 - 无需要太多 (2).wav
│   ├── 09 - 无需要太多.wav
│   ├── 09 - 暴风一族.wav
│   ├── 09 - 有心人.wav
│   ├── 09 - 枕头.wav
│   ├── 09 - 死心（袁咏仪）.wav
│   ├── 09 - 沉默是金.wav
│   ├── 09 - 爱慕.wav
│   ├── 09 - 爱有万万千.wav
│   ├── 09 - 爱火.wav
│   ├── 09 - 由零开始.wav
│   ├── 09 - 电风扇.wav
│   ├── 09 - 禁片.wav
│   ├── 09 - 红 Live.wav
│   ├── 09 - 红颜白发.wav
│   ├── 09 - 胭脂扣.wav
│   ├── 09 - 蓝色忧郁 (2).wav
│   ├── 09 - 蓝色忧郁.wav
│   ├── 09 - 请勿越轨 (2).wav
│   ├── 09 - 请勿越轨.wav
│   ├── 09 - 谈恋爱.wav
│   ├── 09 - 谈情说爱.wav
│   ├── 09 - 闯进新领域.wav
│   ├── 09 - 陪你倒数.wav
│   ├── 09 - 难以再说对不起.wav
│   ├── 09 - 默默向上游(Mix).wav
│   ├── 09 - 默默向上游.wav
│   ├── 10 - American Pie.wav
│   ├── 10 - CRAZY ROCK.wav
│   ├── 10 - I Honestly Love You.wav
│   ├── 10 - Just The Way You Are.wav
│   ├── 10 - Medley：啼笑姻缘-当爱已成往事-啼笑姻缘.wav
│   ├── 10 - Thank You.wav
│   ├── 10 - 一片痴.wav
│   ├── 10 - 不羁的风.wav
│   ├── 10 - 为你钟情 Live.wav
│   ├── 10 - 人生的鼓手 (2).wav
│   ├── 10 - 人生的鼓手.wav
│   ├── 10 - 何去何从之阿飞正传.wav
│   ├── 10 - 你是明星(国).wav
│   ├── 10 - 侬本多情.wav
│   ├── 10 - 倩女幽魂 (2).wav
│   ├── 10 - 倩女幽魂 (3).wav
│   ├── 10 - 倩女幽魂 (4).wav
│   ├── 10 - 倩女幽魂.wav
│   ├── 10 - 全赖有你.wav
│   ├── 10 - 再恋.wav
│   ├── 10 - 刻骨铭心.wav
│   ├── 10 - 可人儿.wav
│   ├── 10 - 尽情地爱.wav
│   ├── 10 - 心跳呼吸正常.wav
│   ├── 10 - 想你 (2).wav
│   ├── 10 - 想你.wav
│   ├── 10 - 拒绝再玩.wav
│   ├── 10 - 无心睡眠.wav
│   ├── 10 - 无需要太多.wav
│   ├── 10 - 最爱.wav
│   ├── 10 - 有心人.wav
│   ├── 10 - 未来之歌.wav
│   ├── 10 - 柔情蜜意.wav
│   ├── 10 - 棉花糖.wav
│   ├── 10 - 油脂热潮.wav
│   ├── 10 - 爱的狂徒 (2).wav
│   ├── 10 - 爱的狂徒.wav
│   ├── 10 - 眉来眼去（辛晓琪合唱）.wav
│   ├── 10 - 缘份.wav
│   ├── 10 - 蓝色忧郁.wav
│   ├── 10 - 被爱.wav
│   ├── 10 - 谁令你心痴（陈洁灵合唱）.wav
│   ├── 10 - 这些年来.wav
│   ├── 10 - 雨中的浪漫.wav
│   ├── 10 - 风再起时.wav
│   ├── 10 - 黑色午夜(Remix).wav
│   ├── 11 - A Little Bit More.wav
│   ├── 11 - Every body.wav
│   ├── 11 - H2O.wav
│   ├── 11 - MIRACLE（麦洁文合唱）.wav
│   ├── 11 - マツュマロ.wav
│   ├── 11 - 为你钟情.wav
│   ├── 11 - 何去何从之阿飞正传.wav
│   ├── 11 - 全世界只想你来爱我(国).wav
│   ├── 11 - 全身都是爱.wav
│   ├── 11 - 分手.wav
│   ├── 11 - 大亨.wav
│   ├── 11 - 少女心事.wav
│   ├── 11 - 左右手.wav
│   ├── 11 - 怪你过分美丽.wav
│   ├── 11 - 恋爱交叉.wav
│   ├── 11 - 情难自控.wav
│   ├── 11 - 无心睡眠.wav
│   ├── 11 - 无胆入情关.wav
│   ├── 11 - 明月夜 (2).wav
│   ├── 11 - 明月夜.wav
│   ├── 11 - 春夏秋冬.wav
│   ├── 11 - 暴风一族.wav
│   ├── 11 - 月亮代表我的心 Live.wav
│   ├── 11 - 柔情蜜意.wav
│   ├── 11 - 永远记得.wav
│   ├── 11 - 沈胜衣.wav
│   ├── 11 - 烈火边缘.wav
│   ├── 11 - 至少还有你.wav
│   ├── 11 - 访英台.wav
│   ├── 11 - 谈情说爱.wav
│   ├── 11 - 迷惑我.wav
│   ├── 11 - 隐身人.wav
│   ├── 11 - 默默向上游 (2).wav
│   ├── 11 - 默默向上游 (3).wav
│   ├── 11 - 默默向上游.wav
│   ├── 12 - Love like magic.wav
│   ├── 12 - Pistol Packin' Melody.wav
│   ├── 12 - 一盏小明灯.wav
│   ├── 12 - 倩女幽魂.wav
│   ├── 12 - 共同渡过.wav
│   ├── 12 - 分手.wav
│   ├── 12 - 够了.wav
│   ├── 12 - 大报复.wav
│   ├── 12 - 奔向未来日子.wav
│   ├── 12 - 始终会行运.wav
│   ├── 12 - 守住风口 (2).wav
│   ├── 12 - 守住风口.wav
│   ├── 12 - 少女心事.wav
│   ├── 12 - 当年情.wav
│   ├── 12 - 我(国).wav
│   ├── 12 - 我愿意.wav
│   ├── 12 - 我要逆风去 (2).wav
│   ├── 12 - 我要逆风去.wav
│   ├── 12 - 无心睡眠.wav
│   ├── 12 - 没有爱.wav
│   ├── 12 - 爱火.wav
│   ├── 12 - 第一次.wav
│   ├── 12 - 谈恋爱.wav
│   ├── 12 - 谈情说爱 (2).wav
│   ├── 12 - 谈情说爱.wav
│   ├── 12 - 贴身.wav
│   ├── 12 - 迷路.wav
│   ├── 12 - 追 Live.wav
│   ├── 12 - 风继续吹 (2).wav
│   ├── 12 - 风继续吹.wav
│   ├── 13 - A Thousand Dreams Of You.wav
│   ├── 13 - Twist & Shout.wav
│   ├── 13 - 一片痴.wav
│   ├── 13 - 作伴.wav
│   ├── 13 - 侧面.wav
│   ├── 13 - 共同度过.wav
│   ├── 13 - 共同渡过.wav
│   ├── 13 - 最爱.wav
│   ├── 13 - 燕子的故事.wav
│   ├── 13 - 片段.wav
│   ├── 13 - 痴心的我.wav
│   ├── 13 - 第一次 (2).wav
│   ├── 13 - 第一次.wav
│   ├── 13 - 路过蜻蜓.wav
│   ├── 13 - 野火 (2).wav
│   ├── 13 - 野火.wav
│   ├── 13 - 陪你倒数.wav
│   ├── 13 - 隐身人.wav
│   ├── 14 - Daydreamer.wav
│   ├── 14 - Stand Up.wav
│   ├── 14 - 共同度过.wav
│   ├── 14 - 分手.wav
│   ├── 14 - 失散的影子 (2).wav
│   ├── 14 - 失散的影子.wav
│   ├── 14 - 当爱已成往事.wav
│   ├── 14 - 恋爱交叉.wav
│   ├── 14 - 无心睡眠.wav
│   ├── 14 - 沉默是金(独唱版).wav
│   ├── 14 - 爱情路里.wav
│   ├── 14 - 甜蜜的禁果.wav
│   ├── 14 - 迷惑我.wav
│   ├── 14 - 迷路.wav
│   ├── 14 - 默默向上游.wav
│   ├── 15 - H2O.wav
│   ├── 15 - I Need You.wav
│   ├── 15 - 为你钟情.wav
│   ├── 15 - 侬本多情.wav
│   ├── 15 - 全赖有你.wav
│   ├── 15 - 只怕不再遇上（陈洁灵合唱）.wav
│   ├── 15 - 恋爱交叉.wav
│   ├── 15 - 惊梦 (2).wav
│   ├── 15 - 惊梦.wav
│   ├── 15 - 我的心里没有他.wav
│   ├── 15 - 无胆入情关.wav
│   ├── 15 - 红颜白发.wav
│   ├── 15 - 风再起时 (2).wav
│   ├── 15 - 风再起时.wav
│   ├── 16 - Everybody.wav
│   ├── 16 - I Like Dreamin'.wav
│   ├── 16 - 一盏小明灯.wav
│   ├── 16 - 倩女幽魂.wav
│   ├── 16 - 热情的沙漠.wav
│   ├── 16 - 背着命运(国语).wav
│   ├── 16 - 闯进新领域.wav
│   ├── 16 - 风再起时.wav
│   ├── 17 - Love Like Magic.wav
│   ├── 17 - 不确定的年纪(国语).wav
│   ├── 17 - 大热.wav
│   ├── 17 - 当年情(国语).wav
│   ├── 17 - 有谁共鸣.wav
│   ├── 18 - 午夜奔驰(国语).wav
│   ├── 18 - 沉默是金.wav
│   ├── 18 - 痴心的我(国语).wav
│   ├── p225.wav
│   ├── p226.wav
│   ├── p227.wav
│   ├── p228.wav
│   ├── p229.wav
│   ├── p230.wav
│   ├── p231.wav
│   ├── p232.wav
│   ├── p233.wav
│   ├── p234.wav
│   ├── p236.wav
│   ├── p237.wav
│   ├── p238.wav
│   ├── p239.wav
│   ├── p240.wav
│   ├── p241.wav
│   ├── p243.wav
│   ├── p244.wav
│   ├── p245.wav
│   ├── p246.wav
│   ├── p247.wav
│   ├── p248.wav
│   ├── p249.wav
│   ├── p250.wav
│   ├── p251.wav
│   ├── p252.wav
│   ├── p253.wav
│   ├── p254.wav
│   ├── p255.wav
│   ├── p256.wav
│   ├── p257.wav
│   ├── p258.wav
│   ├── p259.wav
│   ├── p260.wav
│   ├── p261.wav
│   ├── p262.wav
│   ├── p263.wav
│   ├── p264.wav
│   ├── p265.wav
│   ├── p266.wav
│   ├── p267.wav
│   ├── p268.wav
│   ├── p269.wav
│   ├── p270.wav
│   ├── p271.wav
│   ├── p272.wav
│   ├── p273.wav
│   ├── p274.wav
│   ├── p275.wav
│   ├── p276.wav
│   ├── p277.wav
│   ├── p278.wav
│   ├── p279.wav
│   ├── p280.wav
│   ├── p281.wav
│   ├── p282.wav
│   ├── p283.wav
│   ├── p284.wav
│   ├── p285.wav
│   ├── p286.wav
│   ├── p287.wav
│   ├── p288.wav
│   ├── p292.wav
│   ├── p293.wav
│   ├── p294.wav
│   ├── p295.wav
│   ├── p297.wav
│   ├── p298.wav
│   ├── p299.wav
│   ├── p300.wav
│   ├── p301.wav
│   ├── p302.wav
│   ├── p303.wav
│   ├── p304.wav
│   ├── p305.wav
│   ├── p306.wav
│   ├── p307.wav
│   ├── p308.wav
│   ├── p310.wav
│   ├── p311.wav
│   ├── p312.wav
│   ├── p313.wav
│   ├── p314.wav
│   ├── p315.wav
│   ├── p316.wav
│   ├── p317.wav
│   ├── p318.wav
│   ├── p323.wav
│   ├── p326.wav
│   ├── p329.wav
│   ├── p330.wav
│   ├── p333.wav
│   ├── p334.wav
│   ├── p335.wav
│   ├── p336.wav
│   ├── p339.wav
│   ├── p340.wav
│   ├── p341.wav
│   ├── p343.wav
│   ├── p345.wav
│   ├── p347.wav
│   ├── p351.wav
│   ├── p360.wav
│   ├── p361.wav
│   ├── p362.wav
│   ├── p363.wav
│   ├── p364.wav
│   ├── p374.wav
│   ├── p376.wav
│   ├── s5.wav
│   ├── 张国荣-A SONG FOR YOU.wav
│   ├── 张国荣-DREAMING My Other Half.wav
│   ├── 张国荣-Don't Lie To Me.wav
│   ├── 张国荣-Dreaming (Remix).wav
│   ├── 张国荣-Hot Summer (Live).wav
│   ├── 张国荣-Hot Summer (Remix).wav
│   ├── 张国荣-I Honestly Love You.wav
│   ├── 张国荣-MISS YOU MUCH Missing you.wav
│   ├── 张国荣-Medley ： 不怕寂寞 、 我愿意 (Live).wav
│   ├── 张国荣-Medley ： 抵抗夜寒、 黑色午夜 、 热辣辣 (Live).wav
│   ├── 张国荣-Medley ： 有谁共鸣 、 沉默是金 (Live).wav
│   ├── 张国荣-Medley ： 童年时 、 似水流年、 但愿人长久 (Live).wav
│   ├── 张国荣-Medley ： 蓝色忧郁 、 少女心事 、 不羁的风 、 Monica (Live).wav
│   ├── 张国荣-Medley： H2O 、黑色午夜、 隐身人、第一次、Stand Up (Live).wav
│   ├── 张国荣-Miss You Much (Live).wav
│   ├── 张国荣-Miss You Much (Remix).wav
│   ├── 张国荣-Monica (Live).wav
│   ├── 张国荣-Opening 、 为你钟情 (Live).wav
│   ├── 张国荣-Opening 、 贴身 (Live).wav
│   ├── 张国荣-PROLOGUE.wav
│   ├── 张国荣-Stories (Live).wav
│   ├── 张国荣-The Way We Were (Live).wav
│   ├── 张国荣-Why (Remix).wav
│   ├── 张国荣-不怕寂寞 (Live).wav
│   ├── 张国荣-不想拥抱我的人.wav
│   ├── 张国荣-不羁的风 (Live).wav
│   ├── 张国荣-为你钟情 (Live).wav
│   ├── 张国荣-你在何地 (Live).wav
│   ├── 张国荣-你我之间.wav
│   ├── 张国荣-侧面 (Live).wav
│   ├── 张国荣-侧面 (Remix).wav
│   ├── 张国荣-侧面 Silhouette.wav
│   ├── 张国荣-侬本多情 (Live) (2).wav
│   ├── 张国荣-侬本多情 (Live).wav
│   ├── 张国荣-侯斯顿之恋.wav
│   ├── 张国荣-倩女幽魂 (Live) (2).wav
│   ├── 张国荣-倩女幽魂 (Live).wav
│   ├── 张国荣-偏心 (Live).wav
│   ├── 张国荣-偷情.wav
│   ├── 张国荣-共创真善美.wav
│   ├── 张国荣-共同渡过 (Live) (2).wav
│   ├── 张国荣-共同渡过 (Live).wav
│   ├── 张国荣-共同渡过 Together Forever.wav
│   ├── 张国荣-千千阕歌 (Live).wav
│   ├── 张国荣-千娇百美.wav
│   ├── 张国荣-午后红茶.wav
│   ├── 张国荣-发烧.wav
│   ├── 张国荣-够了 (Live).wav
│   ├── 张国荣-够了 (Remix).wav
│   ├── 张国荣-大热.wav
│   ├── 张国荣-奇迹.wav
│   ├── 张国荣-奔向未来日子 (Live).wav
│   ├── 张国荣-客途秋恨 (Live).wav
│   ├── 张国荣-寂寞夜晚 (Live).wav
│   ├── 张国荣-当年情 (Live) (2).wav
│   ├── 张国荣-当年情 (Live).wav
│   ├── 张国荣-怨男.wav
│   ├── 张国荣-怪你过份美丽.wav
│   ├── 张国荣-想你 (Live).wav
│   ├── 张国荣-想妳 (Live).wav
│   ├── 张国荣-想妳 On My Mind.wav
│   ├── 张国荣-意犹未尽.wav
│   ├── 张国荣-愿你决定.wav
│   ├── 张国荣-我 (Mandarin Version) (2).wav
│   ├── 张国荣-我 (Mandarin Version).wav
│   ├── 张国荣-我.wav
│   ├── 张国荣-我知你好.wav
│   ├── 张国荣-打开信箱 (Live).wav
│   ├── 张国荣-拒绝再玩 (Live).wav
│   ├── 张国荣-拒绝再玩 (Remix).wav
│   ├── 张国荣-挪亚方舟.wav
│   ├── 张国荣-放荡 (Live).wav
│   ├── 张国荣-放荡 (Remix).wav
│   ├── 张国荣-敢爱.wav
│   ├── 张国荣-无心睡眠 (Live) (2).wav
│   ├── 张国荣-无心睡眠 (Live).wav
│   ├── 张国荣-无心睡眠 (Live丨reprise).wav
│   ├── 张国荣-无心睡眠 (Remix).wav
│   ├── 张国荣-无心睡眠 Sleepless nights Restless heart.wav
│   ├── 张国荣-无需要太多 (Live) (2).wav
│   ├── 张国荣-无需要太多 (Live).wav
│   ├── 张国荣-无需要太多 Love is Enough.wav
│   ├── 张国荣-明星 (Live).wav
│   ├── 张国荣-暴风一族 (Live).wav
│   ├── 张国荣-暴风一族 (Remix).wav
│   ├── 张国荣-最爱 (Live) (2).wav
│   ├── 张国荣-最爱 (Live).wav
│   ├── 张国荣-最爱是谁 My Dearest.wav
│   ├── 张国荣-有心人.wav
│   ├── 张国荣-有谁共鸣 (Live).wav
│   ├── 张国荣-沉默是金 (Live).wav
│   ├── 张国荣-没有烟总有花.wav
│   ├── 张国荣-没有爱.wav
│   ├── 张国荣-烈火边缘 (Live).wav
│   ├── 张国荣-热辣辣 (Live).wav
│   ├── 张国荣-热辣辣 (Remix).wav
│   ├── 张国荣-爱慕 (Live) (2).wav
│   ├── 张国荣-爱慕 (Live).wav
│   ├── 张国荣-爱的凶手 (Live).wav
│   ├── 张国荣-爱的凶手.wav
│   ├── 张国荣-玻璃之情.wav
│   ├── 张国荣-由零开始 (Live).wav
│   ├── 张国荣-由零开始 Will You Remember Me.wav
│   ├── 张国荣-禁片 (Remix).wav
│   ├── 张国荣-红.wav
│   ├── 张国荣-红蝴蝶.wav
│   ├── 张国荣-继续跳舞 (Live).wav
│   ├── 张国荣-继续跳舞 (Remix).wav
│   ├── 张国荣-胭脂扣 (Live).wav
│   ├── 张国荣-蝶变.wav
│   ├── 张国荣-访英台 (Live).wav
│   ├── 张国荣-请勿越轨 (Live).wav
│   ├── 张国荣-谈情说爱.wav
│   ├── 张国荣-身边有人.wav
│   ├── 张国荣-还有谁.wav
│   ├── 张国荣-随心.wav
│   ├── 张国荣-风再起时 (Live).wav
│   ├── 张国荣-风继续吹 (Live) (2).wav
│   ├── 张国荣-风继续吹 (Live).wav
│   ├── 張國榮.LESLIE CHEUNG LPCD45II.wav
│   └── 柏安妮-不想再拥有 (Live).wav
├── run_gui.py
└── train_gui.py
```

---

# 代码内容

## 文件: `dataset_loader.py`

```python
import os
import random
from typing import List, Tuple, Optional
import numpy as np
import soundfile as sf
import torch
import torch.nn.functional as F
from torch.utils.data import Dataset
import torchaudio
from rich.progress import track

# --- AudioSR lowpass filtering utility ---
from audiosr.lowpass import lowpass
from audiosr.utils import _locate_cutoff_freq

AUDIO_EXTS = (".wav", ".flac", ".ogg", ".mp3")

def load_audio_mono(path: str, target_sr: int):
    audio, sr = torchaudio.load(path)
    if audio.shape[0] > 1:
        audio = torch.mean(audio, dim=0, keepdim=True)
    
    if sr != target_sr:
        audio = torchaudio.functional.resample(audio, orig_freq=sr, new_freq=target_sr)
        
    return audio.squeeze(0).numpy().astype(np.float32)

def create_lowpass_version(waveform, sr):
    """Creates a low-pass filtered version of the audio, simulating a low-quality input."""
    waveform = waveform.to(torch.float32)
    window = torch.hann_window(2048, device=waveform.device).to(torch.float32)
    
    stft_for_cutoff = torch.stft(waveform, n_fft=2048, hop_length=480, win_length=2048, window=window, return_complex=True, center=True)
    
    cutoff_freq = (_locate_cutoff_freq(stft_for_cutoff.abs(), percentile=0.985) / 1024) * (sr / 2)
    if cutoff_freq < 1000:
        cutoff_freq = sr / 2

    filtered_waveform = lowpass(
        waveform.cpu().numpy(),
        highcut=cutoff_freq,
        fs=sr,
        order=8,
        _type="butter",
    )
    return torch.from_numpy(filtered_waveform.copy()).to(torch.float32)

class PairedMelDataset(Dataset):
    def __init__(
        self,
        dataset_root: str,
        categories: List[str],
        high_dir_name: str,
        low_dir_name: str,
        sample_rate: int,
        segment_seconds: float,
        split: str = "train",
        valid_ratio: float = 0.05,
        split_seed: int = 1337,
        n_fft: int = 2048,
        hop_length: int = 480,
        win_length: int = 2048,
        n_mels: int = 256,
        fmin: float = 20.0,
        fmax: Optional[float] = 24000.0,
        preload_to_ram: bool = False, # New parameter
        **kwargs,
    ):
        super().__init__()
        self.sample_rate = int(sample_rate)
        self.segment_len = int(round(segment_seconds * sample_rate))
        self.is_train = (split == 'train')
        self.preload_to_ram = preload_to_ram
        
        self.mel_transform = torchaudio.transforms.MelSpectrogram(
            sample_rate=self.sample_rate,
            n_fft=n_fft,
            win_length=win_length,
            hop_length=hop_length,
            f_min=fmin,
            f_max=fmax,
            n_mels=n_mels,
            power=2.0,
            norm='slaney',
            mel_scale='slaney'
        )

        pairs: List[str] = []
        for cat in categories:
            high_dir = os.path.join(dataset_root, cat, high_dir_name)
            if not os.path.isdir(high_dir):
                print(f"Warning: Directory not found {high_dir}")
                continue
            
            files = {f for f in os.listdir(high_dir) if f.lower().endswith(AUDIO_EXTS)}
            for name in sorted(files):
                pairs.append(os.path.join(high_dir, name))
        
        rng = random.Random(split_seed)
        rng.shuffle(pairs)
        n_valid = int(round(len(pairs) * float(valid_ratio)))
        self.files = pairs[n_valid:] if self.is_train else pairs[:n_valid]
        if not self.files:
            raise RuntimeError('No audio files found.')
        print(f"Found {len(self.files)} files for {split} split.")

        self.data_buffer = []
        if self.preload_to_ram:
            print(f"Pre-loading {len(self.files)} files into RAM for '{split}' split...")
            for idx in track(range(len(self.files)), description=f"Loading {split} data..."):
                self.data_buffer.append(self._load_item(idx))
            print("Pre-loading complete.")

    def __len__(self):
        return len(self.files)

    def _load_item(self, idx: int):
        filepath = self.files[idx]
        
        try:
            high_quality_wav = load_audio_mono(filepath, self.sample_rate)
            
            if len(high_quality_wav) < self.segment_len:
                pad_amount = self.segment_len - len(high_quality_wav)
                high_quality_wav = np.pad(high_quality_wav, (0, pad_amount), 'constant')
            elif len(high_quality_wav) > self.segment_len:
                start = random.randint(0, len(high_quality_wav) - self.segment_len) if self.is_train else 0
                high_quality_wav = high_quality_wav[start : start + self.segment_len]

            high_quality_tensor = torch.from_numpy(high_quality_wav).to(torch.float32)
            low_quality_tensor = create_lowpass_version(high_quality_tensor, self.sample_rate)
            
            with torch.no_grad():
                high_mel = self.mel_transform(high_quality_tensor)
                low_mel = self.mel_transform(low_quality_tensor)
            
            high_mel = torch.log(torch.clamp(high_mel, min=1e-5))
            low_mel = torch.log(torch.clamp(low_mel, min=1e-5))

            return {
                "fbank": high_mel.unsqueeze(0),
                "lowpass_mel": low_mel.unsqueeze(0)
            }
        except Exception as e:
            print(f"Error loading file {filepath}: {e}, skipping.")
            return None

    def __getitem__(self, idx: int):
        if self.preload_to_ram:
            item = self.data_buffer[idx]
            if item is None:
                # Handle cases where a file failed to load during preloading
                return self.__getitem__((idx + 1) % len(self.files))
            return item
        
        item = self._load_item(idx)
        if item is None:
            return self.__getitem__((idx + 1) % len(self.files))
        return item

```

## 文件: `model.py`

```python
# This file is adapted from the official AudioSR repository to be compatible with the project structure.
# It now integrates the LatentDiffusion model, VAE, and UNet in one place with the correct naming hierarchy.
#
# --- MODIFICATIONS ---
# 1. Replaced placeholder CLAP and Vocoder classes with detailed architectural skeletons
#    that precisely match the keys in the pre-trained checkpoint file.
# 2. Registered 'scale_factor' as a buffer in the main LatentDiffusion class.
# 3. Kept the custom EMA handler and the corrected UNet architecture from previous versions.
# 4. Corrected the shape of 'logit_scale_a' and 'logit_scale_t' to be scalars to match the checkpoint.
# 5. Fully built out the ClapWrapper's audio_branch with a SwinTransformer skeleton to match all checkpoint keys.
# 6. Set bias=False for STFT conv layers as per the checkpoint's structure.
# 7. Corrected a SyntaxError in the UNetModel's __init__ method.
# 8. Corrected an UnboundLocalError in the LatentDiffusion's register_schedule method.
# 9. Corrected the SwinTransformer's window_size and attn_mask shapes to match the checkpoint.
# 10. Re-instated the 'text_branch' and added the correct 'text_transform' and 'audio_transform' modules.
# 11. Fixed transform modules to match checkpoint structure (without .sequential nesting).
# 12. Implemented the forward pass for the Vocoder class with proper HiFi-GAN architecture.

import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange
from contextlib import contextmanager
from functools import partial
import numpy as np
import warnings
import sys
import os

# --- Official AudioSR Imports (or their equivalents) ---
from audiosr.latent_diffusion.util import (
    exists,
    default,
    instantiate_from_config,
)
from audiosr.latent_diffusion.modules.distributions.distributions import (
    DiagonalGaussianDistribution,
)
from audiosr.latent_diffusion.modules.diffusionmodules.util import (
    make_beta_schedule,
    extract_into_tensor,
    noise_like,
    checkpoint,
    conv_nd,
    linear,
    zero_module,
    normalization,
    timestep_embedding,
)
from audiosr.latent_diffusion.modules.attention import SpatialTransformer
from audiosr.latent_diffusion.modules.diffusionmodules.model import Encoder, Decoder # VAE components

# === GLOBAL ENCODER FOR WHOLE AUDIO PROCESSING ===
class GlobalAudioEncoder(nn.Module):
    """
    Lightweight CNN+Transformer Global Encoder that processes whole audio
    and extracts critical global information to condition the diffusion process.
    """
    def __init__(self, 
                 input_channels=256,      # Mel spectrogram channels
                 hidden_dim=128,          # Hidden dimension for efficiency
                 num_heads=4,             # Multi-head attention
                 num_layers=2,            # Transformer layers
                 context_dim=64,          # Output context dimension
                 max_seq_len=1024):       # Maximum sequence length
        super().__init__()
        
        self.hidden_dim = hidden_dim
        self.context_dim = context_dim
        
        # === CNN Feature Extractor ===
        # Efficiently downsample and extract features from mel spectrogram
        self.cnn_encoder = nn.Sequential(
            # First conv block - reduce frequency dimension
            nn.Conv2d(1, 32, kernel_size=(8, 3), stride=(4, 1), padding=(2, 1)),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            
            # Second conv block - further downsample
            nn.Conv2d(32, 64, kernel_size=(4, 3), stride=(2, 1), padding=(1, 1)),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            
            # Third conv block - final feature extraction
            nn.Conv2d(64, hidden_dim, kernel_size=(4, 3), stride=(2, 1), padding=(1, 1)),
            nn.BatchNorm2d(hidden_dim),
            nn.ReLU(inplace=True),
        )
        
        # === Positional Encoding ===
        self.pos_encoding = nn.Parameter(torch.randn(1, max_seq_len, hidden_dim) * 0.02)
        
        # === Transformer Encoder ===
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_dim,
            nhead=num_heads,
            dim_feedforward=hidden_dim * 2,
            dropout=0.1,
            activation='gelu',
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # === Output Projection ===
        self.output_proj = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, context_dim)
        )
        
        # === Global Context Pooling ===
        self.global_pool = nn.AdaptiveAvgPool1d(1)
        self.global_proj = nn.Linear(context_dim, context_dim)
        
    def forward(self, x):
        """
        Forward pass of Global Encoder
        Args:
            x: Input mel spectrogram [batch_size, 1, mel_bins, time_steps]
        Returns:
            global_context: Global context features [batch_size, context_dim]
            sequence_context: Sequence context features [batch_size, seq_len, context_dim]
        """
        batch_size = x.shape[0]
        
        # === CNN Feature Extraction ===
        # x: [B, 1, mel_bins, time] -> [B, hidden_dim, reduced_mel, time]
        features = self.cnn_encoder(x)  # [B, hidden_dim, ~16, time]
        
        # Reshape for transformer: [B, hidden_dim, reduced_mel, time] -> [B, seq_len, hidden_dim]
        B, C, H, W = features.shape
        features = features.permute(0, 3, 1, 2).contiguous()  # [B, time, hidden_dim, reduced_mel]
        features = features.view(B, W, C * H)  # [B, time, hidden_dim * reduced_mel]
        
        # Project to hidden_dim
        if C * H != self.hidden_dim:
            if not hasattr(self, '_adaptive_proj'):
                self._adaptive_proj = nn.Linear(C * H, self.hidden_dim).to(features.device)
            features = self._adaptive_proj(features)
        
        # === Add Positional Encoding ===
        seq_len = features.shape[1]
        if seq_len <= self.pos_encoding.shape[1]:
            pos_enc = self.pos_encoding[:, :seq_len, :]
        else:
            # If sequence is longer, interpolate positional encoding
            pos_enc = F.interpolate(
                self.pos_encoding.transpose(1, 2), 
                size=seq_len, 
                mode='linear', 
                align_corners=False
            ).transpose(1, 2)
        
        features = features + pos_enc
        
        # === Transformer Processing ===
        # Self-attention to capture global dependencies
        transformed_features = self.transformer(features)  # [B, seq_len, hidden_dim]
        
        # === Output Projections ===
        sequence_context = self.output_proj(transformed_features)  # [B, seq_len, context_dim]
        
        # Global context via pooling
        global_features = self.global_pool(sequence_context.transpose(1, 2)).squeeze(-1)  # [B, context_dim]
        global_context = self.global_proj(global_features)  # [B, context_dim]
        
        return global_context, sequence_context

class GlobalConditionedSpatialTransformer(SpatialTransformer):
    """
    Enhanced SpatialTransformer that incorporates global audio context
    """
    def __init__(self, in_channels, n_heads, d_head, depth=1, dropout=0., context_dim=None, global_context_dim=64):
        super().__init__(in_channels, n_heads, d_head, depth, dropout, context_dim)
        self.global_context_dim = global_context_dim
        
        if global_context_dim is not None:
            # Project global context to match transformer dimensions
            self.global_proj = nn.Linear(global_context_dim, n_heads * d_head)
            
            # Global context attention
            self.global_attn = nn.MultiheadAttention(
                embed_dim=n_heads * d_head,
                num_heads=n_heads,
                dropout=dropout,
                batch_first=True
            )
    
    def forward(self, x, context=None, global_context=None):
        """
        Forward pass with global context integration
        Args:
            x: Input features [B, C, H, W]
            context: Local context from other conditions
            global_context: Global audio context [B, global_context_dim] or [B, seq_len, global_context_dim]
        """
        if global_context is not None and hasattr(self, 'global_proj'):
            # Process global context
            if global_context.dim() == 2:
                # Single global vector: [B, global_context_dim] -> [B, 1, n_heads * d_head]
                global_features = self.global_proj(global_context).unsqueeze(1)
            else:
                # Sequence of global vectors: [B, seq_len, global_context_dim] -> [B, seq_len, n_heads * d_head]
                global_features = self.global_proj(global_context)
            
            # Reshape input for attention
            B, C, H, W = x.shape
            x_flat = x.view(B, C, H * W).transpose(1, 2)  # [B, H*W, C]
            
            # Apply global context attention
            attended_x, _ = self.global_attn(x_flat, global_features, global_features)
            
            # Reshape back
            x = attended_x.transpose(1, 2).view(B, C, H, W)
        
        # Call parent forward with potentially modified x
        return super().forward(x, context)

# === EXACT AUDIOSR VOCODER ===
class AudioSRVocoder(nn.Module):
    """
    Exact AudioSR HiFi-GAN vocoder implementation matching output size perfectly
    """
    def __init__(self):
        super().__init__()
        
        # Pre-conv
        self.conv_pre = nn.Conv1d(256, 1536, 7, 1, padding=3)
        
        # Upsampling - EXACT AudioSR configuration
        self.ups = nn.ModuleList([
            nn.ConvTranspose1d(1536, 768, 12, 6, padding=3),
            nn.ConvTranspose1d(768, 384, 10, 5, padding=2, output_padding=1),
            nn.ConvTranspose1d(384, 192, 8, 4, padding=2),
            nn.ConvTranspose1d(192, 96, 4, 2, padding=1),
            nn.ConvTranspose1d(96, 48, 4, 2, padding=1),
        ])
        
        # ResBlocks - EXACT AudioSR structure (20 total)
        self.resblocks = nn.ModuleList()
        
        channels_list = [768, 768, 768, 768, 384, 384, 384, 384, 192, 192, 192, 192, 96, 96, 96, 96, 48, 48, 48, 48]
        kernels_list = [3, 7, 11, 15, 3, 7, 11, 15, 3, 7, 11, 15, 3, 7, 11, 15, 3, 7, 11, 15]
        
        for channels, kernel_size in zip(channels_list, kernels_list):
            self.resblocks.append(AudioSRVocoderResBlock(channels, kernel_size))
        
        # Post conv
        self.conv_post = nn.Conv1d(48, 1, 7, 1, padding=3)
        
        # Don't apply weight norm during initialization - will be applied after weight loading
        self._weight_norm_applied = False
    
    def apply_weight_norm(self):
        """Apply weight normalization to all conv layers"""
        from torch.nn.utils import weight_norm
        
        self.conv_pre = weight_norm(self.conv_pre)
        
        for up in self.ups:
            up = weight_norm(up)
        
        for resblock in self.resblocks:
            resblock.apply_weight_norm()
        
        self.conv_post = weight_norm(self.conv_post)
    
    def forward(self, x):
        x = self.conv_pre(x)
        
        for i in range(len(self.ups)):
            x = F.leaky_relu(x, 0.1)
            x = self.ups[i](x)
            
            # Apply corresponding resblocks
            xs = None
            for j in range(4):  # 4 resblocks per upsampling stage
                resblock_idx = i * 4 + j
                if xs is None:
                    xs = self.resblocks[resblock_idx](x)
                else:
                    xs += self.resblocks[resblock_idx](x)
            x = xs / 4
        
        x = F.leaky_relu(x, 0.1)
        x = self.conv_post(x)
        x = torch.tanh(x)
        
        return x
    
    def remove_weight_norm(self):
        """Remove weight normalization for compatibility"""
        from torch.nn.utils import remove_weight_norm
        
        try:
            remove_weight_norm(self.conv_pre)
        except ValueError:
            pass
            
        for up in self.ups:
            try:
                remove_weight_norm(up)
            except ValueError:
                pass
        
        for resblock in self.resblocks:
            resblock.remove_weight_norm()
        
        try:
            remove_weight_norm(self.conv_post)
        except ValueError:
            pass

class AudioSRVocoderResBlock(nn.Module):
    """HiFi-GAN ResBlock for vocoder"""
    def __init__(self, channels, kernel_size, dilation=(1, 3, 5)):
        super().__init__()
        
        def get_padding(kernel_size, dilation=1):
            return int((kernel_size * dilation - dilation) / 2)
        
        self.convs1 = nn.ModuleList([
            nn.Conv1d(channels, channels, kernel_size, 1,
                     padding=get_padding(kernel_size, d), dilation=d)
            for d in dilation
        ])
        
        self.convs2 = nn.ModuleList([
            nn.Conv1d(channels, channels, kernel_size, 1,
                     padding=get_padding(kernel_size, 1), dilation=1)
            for _ in dilation
        ])
    
    def apply_weight_norm(self):
        """Apply weight normalization to all conv layers"""
        from torch.nn.utils import weight_norm
        
        for i in range(len(self.convs1)):
            self.convs1[i] = weight_norm(self.convs1[i])
            self.convs2[i] = weight_norm(self.convs2[i])
    
    def remove_weight_norm(self):
        """Remove weight normalization"""
        from torch.nn.utils import remove_weight_norm
        
        for i in range(len(self.convs1)):
            try:
                remove_weight_norm(self.convs1[i])
            except ValueError:
                pass
            try:
                remove_weight_norm(self.convs2[i])
            except ValueError:
                pass
    
    def forward(self, x):
        for c1, c2 in zip(self.convs1, self.convs2):
            xt = F.leaky_relu(x, 0.1)
            xt = c1(xt)
            xt = F.leaky_relu(xt, 0.1)
            xt = c2(xt)
            x = xt + x
        return x

# --- Import AudioSR's HiFi-GAN Vocoder ---
# We'll use the original AudioSR HiFi-GAN implementation
try:
    import sys
    import os
    sys.path.append('/home/husrcf/Code/Python/projectlily Z')
    from audiosr.hifigan.models import Generator as HiFiGANGenerator
    from audiosr.utilities.model import get_vocoder_config_48k, vocoder_infer
    from audiosr.hifigan import AttrDict
    AUDIOSR_AVAILABLE = True
except ImportError:
    print("Warning: AudioSR not available, using fallback vocoder")
    AUDIOSR_AVAILABLE = False
    
    # Fallback simplified vocoder for when AudioSR is not available
    class HiFiGANGenerator(nn.Module):
        def __init__(self, config):
            super().__init__()
            self.conv_pre = nn.Conv1d(256, 512, 7, 1, padding=3)
            self.conv_post = nn.Conv1d(32, 1, 7, 1, padding=3)
        def forward(self, x):
            return torch.tanh(self.conv_post(F.leaky_relu(self.conv_pre(x))))
        def remove_weight_norm(self): pass

# === EXACT AUDIOSR CLAP WITH ALL MISSING MODULES ===
class DropPath(nn.Module):
    """Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks)."""
    def __init__(self, drop_prob=None):
        super(DropPath, self).__init__()
        self.drop_prob = drop_prob

    def forward(self, x):
        if self.drop_prob == 0. or not self.training:
            return x
        keep_prob = 1 - self.drop_prob
        shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets
        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)
        random_tensor.floor_()  # binarize
        output = x.div(keep_prob) * random_tensor
        return output

class Mlp(nn.Module):
    def __init__(self, in_features, hidden_features, out_features, act_layer=nn.GELU, drop=0.):
        super().__init__()
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

class WindowAttention(nn.Module):
    def __init__(self, dim, num_heads):
        super().__init__()
        self.qkv = nn.Linear(dim, dim * 3, bias=True)
        self.proj = nn.Linear(dim, dim)
        
        # MISSING MODULES ADDED:
        self.attn_drop = nn.Dropout(0.0)  # Attention dropout
        self.proj_drop = nn.Dropout(0.0)  # Projection dropout
        self.softmax = nn.Softmax(dim=-1)  # Softmax activation
        
        window_size = 8
        self.relative_position_bias_table = nn.Parameter(
            torch.zeros((2 * window_size - 1) * (2 * window_size - 1), num_heads))
        self.relative_position_index = nn.Parameter(torch.zeros(64, 64), requires_grad=False)
    
    def forward(self, x):
        # Simplified forward pass - full implementation would be more complex
        qkv = self.qkv(x)
        q, k, v = qkv.chunk(3, dim=-1)
        
        # Attention calculation
        attn = (q @ k.transpose(-2, -1))
        attn = self.softmax(attn)
        attn = self.attn_drop(attn)
        
        x = (attn @ v)
        x = self.proj(x)
        x = self.proj_drop(x)
        
        return x

class SwinTransformerBlock(nn.Module):
    def __init__(self, dim, num_heads):
        super().__init__()
        self.norm1 = nn.LayerNorm(dim)
        self.attn = WindowAttention(dim, num_heads)
        self.norm2 = nn.LayerNorm(dim)
        
        # MISSING MODULE ADDED:
        self.drop_path = DropPath(0.1)  # DropPath for stochastic depth
        
        self.mlp = Mlp(dim, dim * 4, dim)
        self.register_parameter("attn_mask", None)
    
    def forward(self, x):
        x = x + self.drop_path(self.attn(self.norm1(x)))
        x = x + self.drop_path(self.mlp(self.norm2(x)))
        return x

class PatchMerging(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.reduction = nn.Linear(4 * in_channels, out_channels, bias=False)
        self.norm = nn.LayerNorm(4 * in_channels)

class BasicLayer(nn.Module):
    def __init__(self, dim, out_dim, num_blocks, num_heads, mask_shape=None):
        super().__init__()
        self.blocks = nn.ModuleList([SwinTransformerBlock(dim, num_heads) for _ in range(num_blocks)])
        if mask_shape is not None:
            for i in range(num_blocks):
                if i % 2 != 0:
                    self.blocks[i].attn_mask = nn.Parameter(torch.zeros(mask_shape), requires_grad=False)
        if out_dim is not None:
            self.downsample = PatchMerging(dim, out_dim)
        else:
            self.downsample = None

class PatchEmbed(nn.Module):
    def __init__(self):
        super().__init__()
        self.proj = nn.Conv2d(1, 128, kernel_size=4, stride=4)
        self.norm = nn.LayerNorm(128)

class ClapAudioBranch(nn.Module):
    """CLAP Audio Branch with ALL missing modules"""
    def __init__(self):
        super().__init__()
        # Existing modules...
        self.spectrogram_extractor = nn.Module()
        self.spectrogram_extractor.stft = nn.Module()
        self.spectrogram_extractor.stft.conv_real = nn.Conv1d(1, 513, 1024, bias=False)
        self.spectrogram_extractor.stft.conv_imag = nn.Conv1d(1, 513, 1024, bias=False)
        
        self.logmel_extractor = nn.Module()
        self.logmel_extractor.melW = nn.Parameter(torch.zeros(513, 64))
        
        self.bn0 = nn.BatchNorm1d(64)
        
        # Patch embedding
        self.patch_embed = PatchEmbed()
        
        # Layers with proper blocks
        self.layers = nn.ModuleList([
            BasicLayer(128, 256, 2, 4, mask_shape=(64, 64, 64)),
            BasicLayer(256, 512, 2, 8, mask_shape=(16, 64, 64)),
            BasicLayer(512, 1024, 12, 16, mask_shape=(4, 64, 64)),
            BasicLayer(1024, None, 2, 32) 
        ])
        
        self.norm = nn.LayerNorm(1024)
        self.tscam_conv = nn.Conv2d(1024, 527, kernel_size=(2,3), padding=(0,1))
        self.head = nn.Linear(527, 527)
        
        # CRITICAL MISSING MODULE:
        self.avgpool = nn.AdaptiveAvgPool1d(1)  # Global average pooling

class BertEmbeddings(nn.Module):
    def __init__(self):
        super().__init__()
        self.word_embeddings = nn.Embedding(50265, 768)
        self.position_embeddings = nn.Embedding(514, 768)
        self.token_type_embeddings = nn.Embedding(1, 768)
        self.LayerNorm = nn.LayerNorm(768, eps=1e-12)
        self.register_buffer("position_ids", torch.arange(514).expand((1, -1)))

class BertSelfAttention(nn.Module):
    def __init__(self):
        super().__init__()
        self.query = nn.Linear(768, 768)
        self.key = nn.Linear(768, 768)
        self.value = nn.Linear(768, 768)

class BertAttention(nn.Module):
    def __init__(self):
        super().__init__()
        self.self = BertSelfAttention()
        self.output = nn.Module()
        self.output.dense = nn.Linear(768, 768)
        self.output.LayerNorm = nn.LayerNorm(768, eps=1e-12)

class BertIntermediate(nn.Module):
    def __init__(self):
        super().__init__()
        self.dense = nn.Linear(768, 3072)

class BertOutput(nn.Module):
    def __init__(self):
        super().__init__()
        self.dense = nn.Linear(3072, 768)
        self.LayerNorm = nn.LayerNorm(768, eps=1e-12)

class BertLayer(nn.Module):
    def __init__(self):
        super().__init__()
        self.attention = BertAttention()
        self.intermediate = BertIntermediate()
        self.output = BertOutput()

class BertEncoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer = nn.ModuleList([BertLayer() for _ in range(12)])

class BertPooler(nn.Module):
    def __init__(self):
        super().__init__()
        self.dense = nn.Linear(768, 768)

class ClapTextBranch(nn.Module):
    def __init__(self):
        super().__init__()
        self.embeddings = BertEmbeddings()
        self.encoder = BertEncoder()
        self.pooler = BertPooler()

class ClapWrapper(nn.Module):
    def __init__(self):
        super().__init__()
        self.model = nn.Module()
        self.model.logit_scale_a = nn.Parameter(torch.tensor(0.0))
        self.model.logit_scale_t = nn.Parameter(torch.tensor(0.0))
        self.model.audio_branch = ClapAudioBranch()
        self.model.text_branch = ClapTextBranch()
        
        self.model.text_projection = nn.Sequential(nn.Linear(768, 512), nn.ReLU(), nn.Linear(512, 512))
        self.model.audio_projection = nn.Sequential(nn.Linear(1024, 512), nn.ReLU(), nn.Linear(512, 512))
        
        # Transform modules without .sequential nesting to match checkpoint
        # Create transform modules with .sequential structure to match AudioSR
        self.model.text_transform = nn.Module()
        self.model.text_transform.sequential = nn.Sequential(
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.GELU(),
            nn.Linear(512, 512)
        )
        self.model.audio_transform = nn.Module()
        self.model.audio_transform.sequential = nn.Sequential(
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.GELU(),
            nn.Linear(512, 512)
        )
        
        self.mel_transform = nn.Module()
        self.mel_transform.spectrogram = nn.Module()
        self.mel_transform.spectrogram.window = nn.Parameter(torch.zeros(1024))
        self.mel_transform.mel_scale = nn.Module()
        self.mel_transform.mel_scale.fb = nn.Parameter(torch.zeros(513, 64))

# --- Custom EMA Handler ---
class CustomLitEma(nn.Module):
    def __init__(self, model, decay=0.9999, use_num_updates=True):
        super().__init__()
        if decay < 0.0 or decay > 1.0:
            raise ValueError('Decay must be between 0 and 1')
        self.m_name2s_name = {name: name.replace('.', '') for name, p in model.named_parameters()}
        self.register_buffer('decay', torch.tensor(decay, dtype=torch.float32))
        self.register_buffer('num_updates', torch.tensor(0, dtype=torch.int64) if use_num_updates else torch.tensor(-1, dtype=torch.int64))
        for name, p in model.named_parameters():
            if p.requires_grad:
                s_name = self.m_name2s_name[name]
                self.register_buffer(s_name, p.clone().detach().data)
        self.collected_params = []
    @torch.no_grad()
    def forward(self, model):
        self.num_updates += 1
        for name, p in model.named_parameters():
            if p.requires_grad:
                s_name = self.m_name2s_name[name]
                s_param = self.get_buffer(s_name)
                s_param.sub_((1 - self.decay) * (s_param - p.data))
    def store(self, params): self.collected_params = [p.clone() for p in params]
    def restore(self, params):
        for p_old, p_new in zip(params, self.collected_params): p_old.data.copy_(p_new.data)
        self.collected_params = []
    def copy_to(self, model):
        for name, p in model.named_parameters():
            if p.requires_grad: p.data.copy_(self.get_buffer(self.m_name2s_name[name]))

# --- Building Blocks for UNet (ResBlock, Upsample, etc.) ---
class TimestepBlock(nn.Module):
    @staticmethod
    def forward(x, emb): raise NotImplementedError
class TimestepEmbedSequential(nn.Sequential, TimestepBlock):
    def forward(self, x, emb, context=None, global_context=None):
        for layer in self:
            if isinstance(layer, TimestepBlock): 
                x = layer(x, emb)
            elif isinstance(layer, GlobalConditionedSpatialTransformer): 
                x = layer(x, context, global_context)
            elif isinstance(layer, SpatialTransformer): 
                x = layer(x, context)
            else: 
                x = layer(x)
        return x
class Upsample(nn.Module):
    def __init__(self, channels, use_conv, dims=2, out_channels=None):
        super().__init__()
        self.channels, self.out_channels, self.use_conv, self.dims = channels, out_channels or channels, use_conv, dims
        if use_conv: self.conv = conv_nd(dims, self.channels, self.out_channels, 3, padding=1)
    def forward(self, x):
        assert x.shape[1] == self.channels
        x = F.interpolate(x, scale_factor=2, mode="nearest")
        if self.use_conv: x = self.conv(x)
        return x
class Downsample(nn.Module):
    def __init__(self, channels, use_conv, dims=2, out_channels=None):
        super().__init__()
        self.channels, self.out_channels, self.use_conv, self.dims = channels, out_channels or channels, use_conv, dims
        stride = 2
        if use_conv: self.op = conv_nd(dims, self.channels, self.out_channels, 3, stride=stride, padding=1)
        else: self.op = nn.AvgPool2d(kernel_size=stride, stride=stride)
    def forward(self, x):
        assert x.shape[1] == self.channels
        return self.op(x)
class AudioSRResBlock(TimestepBlock):
    """
    Exact ResBlock implementation matching AudioSR with h_upd modules
    """
    def __init__(self, channels, emb_channels, dropout, out_channels=None, dims=2, 
                 use_checkpoint=False, use_scale_shift_norm=False):
        super().__init__()
        self.channels = channels
        self.emb_channels = emb_channels
        self.dropout = dropout
        self.out_channels = out_channels or channels
        self.use_checkpoint = use_checkpoint
        self.use_scale_shift_norm = use_scale_shift_norm
        
        # Standard ResBlock components
        self.in_layers = nn.Sequential(
            normalization(channels),
            nn.SiLU(),
            conv_nd(dims, channels, self.out_channels, 3, padding=1)
        )
        
        self.emb_layers = nn.Sequential(
            nn.SiLU(),
            linear(emb_channels, 2 * self.out_channels if use_scale_shift_norm else self.out_channels)
        )
        
        self.out_layers = nn.Sequential(
            normalization(self.out_channels),
            nn.SiLU(),
            nn.Dropout(p=dropout),
            zero_module(conv_nd(dims, self.out_channels, self.out_channels, 3, padding=1))
        )
        
        if self.out_channels == channels:
            self.skip_connection = nn.Identity()
        else:
            self.skip_connection = conv_nd(dims, channels, self.out_channels, 1)
        
        # CRITICAL: Add the missing h_upd module
        self.h_upd = nn.Identity()  # This matches AudioSR's h_upd structure
    
    def forward(self, x, emb):
        """
        Apply the block to a Tensor, conditioned on a timestep embedding.
        """
        return checkpoint(self._forward, (x, emb), self.parameters(), self.use_checkpoint)
    
    def _forward(self, x, emb):
        h = self.in_layers(x)
        emb_out = self.emb_layers(emb).type(h.dtype)
        while len(emb_out.shape) < len(h.shape):
            emb_out = emb_out[..., None]
        
        if self.use_scale_shift_norm:
            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]
            scale, shift = torch.chunk(emb_out, 2, dim=1)
            h = out_norm(h) * (1 + scale) + shift
            h = out_rest(h)
        else:
            h = h + emb_out
            h = self.out_layers(h)
        
        return self.skip_connection(x) + h

# Keep original ResBlock for compatibility
ResBlock = AudioSRResBlock

# --- Main U-Net Model (Corrected Architecture) ---
class UNetModel(nn.Module):
    def __init__(self, image_size, in_channels, model_channels, out_channels, num_res_blocks, attention_resolutions, dropout=0, channel_mult=(1, 2, 4, 8), conv_resample=True, dims=2, num_classes=None, use_checkpoint=False, num_heads=-1, num_head_channels=-1, use_scale_shift_norm=False, resblock_updown=False, use_spatial_transformer=False, transformer_depth=1, context_dim=None, use_global_encoder=True, global_context_dim=64, **kwargs):
        super().__init__()
        self.image_size, self.in_channels, self.model_channels, self.out_channels, self.num_res_blocks, self.attention_resolutions, self.dropout, self.channel_mult, self.conv_resample, self.num_classes, self.use_checkpoint, self.num_heads, self.num_head_channels = image_size, in_channels, model_channels, out_channels, num_res_blocks, attention_resolutions, dropout, channel_mult, conv_resample, num_classes, use_checkpoint, num_heads, num_head_channels
        
        # Global Encoder Integration
        self.use_global_encoder = use_global_encoder
        self.global_context_dim = global_context_dim
        if use_global_encoder:
            self.global_encoder = GlobalAudioEncoder(
                input_channels=256,  # Mel spectrogram channels
                hidden_dim=128,      # Efficient hidden dimension
                num_heads=4,         # Multi-head attention
                num_layers=2,        # Lightweight transformer
                context_dim=global_context_dim,  # Output context dimension
                max_seq_len=1024     # Maximum sequence length
            )
        
        time_embed_dim = model_channels * 4
        self.time_embed = nn.Sequential(linear(model_channels, time_embed_dim), nn.SiLU(), linear(time_embed_dim, time_embed_dim))
        if self.num_classes is not None: self.label_emb = nn.Embedding(num_classes, time_embed_dim)
        self.input_blocks = nn.ModuleList([TimestepEmbedSequential(conv_nd(dims, in_channels, model_channels, 3, padding=1))])
        self._feature_size, input_block_chans, ch, ds = model_channels, [model_channels], model_channels, 1
        for level, mult in enumerate(channel_mult):
            for i in range(num_res_blocks):
                layers = [ResBlock(ch, time_embed_dim, dropout, out_channels=mult * model_channels, dims=dims, use_checkpoint=use_checkpoint, use_scale_shift_norm=use_scale_shift_norm)]
                ch = mult * model_channels
                if ds in attention_resolutions:
                    num_heads = ch // num_head_channels
                    dim_head = num_head_channels
                    if self.use_global_encoder:
                        layers.append(GlobalConditionedSpatialTransformer(ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim, global_context_dim=self.global_context_dim))
                        layers.append(GlobalConditionedSpatialTransformer(ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim, global_context_dim=self.global_context_dim))
                    else:
                        layers.append(SpatialTransformer(ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim))
                        layers.append(SpatialTransformer(ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim))
                self.input_blocks.append(TimestepEmbedSequential(*layers))
                self._feature_size += ch
                input_block_chans.append(ch)
            if level != len(channel_mult) - 1:
                out_ch = ch
                self.input_blocks.append(
                    TimestepEmbedSequential(
                        Downsample(ch, conv_resample, dims=dims, out_channels=out_ch)
                    )
                )
                ch = out_ch
                input_block_chans.append(ch)
                ds *= 2
                self._feature_size += ch
        
        num_heads = ch // num_head_channels
        dim_head = num_head_channels
        if self.use_global_encoder:
            self.middle_block = TimestepEmbedSequential(
                ResBlock(ch, time_embed_dim, dropout, dims=dims, use_checkpoint=use_checkpoint, use_scale_shift_norm=use_scale_shift_norm), 
                GlobalConditionedSpatialTransformer(ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim, global_context_dim=self.global_context_dim), 
                GlobalConditionedSpatialTransformer(ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim, global_context_dim=self.global_context_dim), 
                ResBlock(ch, time_embed_dim, dropout, dims=dims, use_checkpoint=use_checkpoint, use_scale_shift_norm=use_scale_shift_norm)
            )
        else:
            self.middle_block = TimestepEmbedSequential(
                ResBlock(ch, time_embed_dim, dropout, dims=dims, use_checkpoint=use_checkpoint, use_scale_shift_norm=use_scale_shift_norm), 
                SpatialTransformer(ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim), 
                SpatialTransformer(ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim), 
                ResBlock(ch, time_embed_dim, dropout, dims=dims, use_checkpoint=use_checkpoint, use_scale_shift_norm=use_scale_shift_norm)
            )
        self._feature_size += ch
        self.output_blocks = nn.ModuleList([])
        for level, mult in list(enumerate(channel_mult))[::-1]:
            for i in range(num_res_blocks + 1):
                ich = input_block_chans.pop()
                layers = [ResBlock(ch + ich, time_embed_dim, dropout, out_channels=model_channels * mult, dims=dims, use_checkpoint=use_checkpoint, use_scale_shift_norm=use_scale_shift_norm)]
                ch = model_channels * mult
                if ds in attention_resolutions:
                    num_heads = ch // num_head_channels
                    dim_head = num_head_channels
                    if self.use_global_encoder:
                        layers.append(GlobalConditionedSpatialTransformer(ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim, global_context_dim=self.global_context_dim))
                        layers.append(GlobalConditionedSpatialTransformer(ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim, global_context_dim=self.global_context_dim))
                    else:
                        layers.append(SpatialTransformer(ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim))
                        layers.append(SpatialTransformer(ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim))
                if level and i == num_res_blocks:
                    out_ch = ch
                    layers.append(Upsample(ch, conv_resample, dims=dims, out_channels=out_ch))
                    ds //= 2
                self.output_blocks.append(TimestepEmbedSequential(*layers))
                self._feature_size += ch
        self.out = nn.Sequential(normalization(ch), nn.SiLU(), zero_module(conv_nd(dims, model_channels, out_channels, 3, padding=1)))
    def forward(self, x, timesteps, context=None, y=None, full_audio=None, **kwargs):
        hs, t_emb = [], timestep_embedding(timesteps, self.model_channels, repeat_only=False)
        emb = self.time_embed(t_emb)
        if self.num_classes is not None: emb = emb + self.label_emb(y)
        
        # Extract global context if Global Encoder is enabled
        global_context = None
        if self.use_global_encoder and full_audio is not None:
            global_context_vec, sequence_context = self.global_encoder(full_audio)
            global_context = global_context_vec  # Use global context for simplicity
        
        h = x
        for module in self.input_blocks:
            if self.use_global_encoder:
                h = module(h, emb, context, global_context)
            else:
                h = module(h, emb, context)
            hs.append(h)
        
        if self.use_global_encoder:
            h = self.middle_block(h, emb, context, global_context)
        else:
            h = self.middle_block(h, emb, context)
        
        for module in self.output_blocks:
            if self.use_global_encoder:
                h = module(torch.cat([h, hs.pop()], dim=1), emb, context, global_context)
            else:
                h = module(torch.cat([h, hs.pop()], dim=1), emb, context)
        
        return self.out(h)

# === COMPLETE AUDIOSR-EXACT AUTOENCODER ===
class AudioSRAutoEncoderKL(nn.Module):
    def __init__(self, ddconfig, embed_dim, ckpt_path=None, ignore_keys=[]):
        super().__init__()
        self.encoder = Encoder(**ddconfig)
        self.decoder = Decoder(**ddconfig)
        
        # Use EXACT AudioSR vocoder
        self.vocoder = AudioSRVocoder()
        
        assert ddconfig["double_z"]
        self.quant_conv = torch.nn.Conv2d(2 * ddconfig["z_channels"], 2 * embed_dim, 1)
        self.post_quant_conv = torch.nn.Conv2d(embed_dim, ddconfig["z_channels"], 1)
        self.embed_dim = embed_dim
    
    def encode(self, x):
        return DiagonalGaussianDistribution(self.quant_conv(self.encoder(x)))
    
    def decode(self, z):
        return self.decoder(self.post_quant_conv(z))
    
    def decode_to_waveform(self, z):
        """Decode to waveform using exact AudioSR vocoder"""
        mel_spec = self.decode(z)
        
        batch_size = mel_spec.shape[0]
        
        if mel_spec.dim() == 4:
            mel_spec = mel_spec.squeeze(1)
            
            # Correct reshaping for AudioSR
            if mel_spec.shape[1] == 512 and mel_spec.shape[2] == 512:
                total_elements = mel_spec.shape[1] * mel_spec.shape[2]
                time_frames = total_elements // 256  # n_mels = 256
                mel_spec = mel_spec.reshape(batch_size, 256, time_frames)
        
        # CRITICAL: Apply AudioSR's spectral denormalization 
        from audiosr.utils import spectral_de_normalize_torch
        mel_spec_denormalized = spectral_de_normalize_torch(mel_spec)
        
        waveform = self.vocoder(mel_spec_denormalized)
        
        if waveform.dim() == 2:
            waveform = waveform.unsqueeze(1)
        
        return waveform

# Keep original AutoencoderKL for compatibility
class AutoencoderKL(nn.Module):
    def __init__(self, ddconfig, embed_dim, ckpt_path=None, ignore_keys=[]):
        super().__init__()
        self.encoder = Encoder(**ddconfig)
        self.decoder = Decoder(**ddconfig) 
        
        # Initialize AudioSR's HiFi-GAN vocoder
        n_mels = ddconfig.get("mel_bins", 256)
        self.n_mels = n_mels
        
        # Use EXACT AudioSR vocoder
        self.vocoder = AudioSRVocoder()
            
        assert ddconfig["double_z"]
        self.quant_conv = torch.nn.Conv2d(2 * ddconfig["z_channels"], 2 * embed_dim, 1)
        self.post_quant_conv = torch.nn.Conv2d(embed_dim, ddconfig["z_channels"], 1)
        self.embed_dim = embed_dim
    
    def encode(self, x): 
        return DiagonalGaussianDistribution(self.quant_conv(self.encoder(x)))
    
    def decode(self, z): 
        return self.decoder(self.post_quant_conv(z))
    
    def decode_to_waveform(self, z):
        """
        Decode latent to mel spectrogram and then to waveform using AudioSR's HiFi-GAN
        Args:
            z: Latent tensor of shape (batch, embed_dim, h, w)
        Returns:
            waveform: Audio waveform tensor of shape (batch, 1, time_samples)
        """
        # First decode to mel spectrogram
        mel_spec = self.decode(z)  # Shape: (batch, 1, h, w)
        
        batch_size = mel_spec.shape[0]
        
        if mel_spec.dim() == 4:  # (batch, channels, height, width)
            # Remove channel dimension: (batch, height, width)  
            mel_spec = mel_spec.squeeze(1)
            
            # Reshape to correct mel spectrogram format
            if mel_spec.shape[1] == 512 and mel_spec.shape[2] == 512:
                # Reshape 512x512 to get 256 mel channels
                total_elements = mel_spec.shape[1] * mel_spec.shape[2]  # 512*512 = 262144
                time_frames = total_elements // self.n_mels  # 262144/256 = 1024
                mel_spec = mel_spec.reshape(batch_size, self.n_mels, time_frames)
            elif mel_spec.shape[1] != self.n_mels:
                if mel_spec.shape[2] == self.n_mels:
                    # Transpose to get correct shape
                    mel_spec = mel_spec.transpose(1, 2)  # (batch, n_mels, time_frames)
                else:
                    # General reshape case
                    total_elements = mel_spec.shape[1] * mel_spec.shape[2]
                    time_frames = total_elements // self.n_mels
                    mel_spec = mel_spec.reshape(batch_size, self.n_mels, time_frames)
        
        # Ensure we have the right shape
        assert mel_spec.shape[1] == self.n_mels, f"Expected {self.n_mels} mel channels, got {mel_spec.shape[1]}"
        
        if AUDIOSR_AVAILABLE:
            # Use AudioSR's vocoder directly (not vocoder_infer which has dimension issues)
            # The vocoder expects (batch, n_mels, time) format, which is what we have
            with torch.no_grad():
                # CRITICAL: VAE decoder outputs need to be de-normalized like in AudioSR
                from audiosr.utils import spectral_de_normalize_torch
                
                # Apply AudioSR's spectral denormalization (dynamic range decompression)
                mel_spec_denormalized = spectral_de_normalize_torch(mel_spec)
                
                # HiFi-GAN vocoder expects the denormalized mel spectrograms
                waveform = self.vocoder(mel_spec_denormalized)
                
                # Ensure correct output shape (batch, 1, time_samples)
                if waveform.dim() == 3 and waveform.shape[1] == 1:
                    # Shape is already (batch, 1, time_samples)
                    pass
                elif waveform.dim() == 2:
                    # Shape is (batch, time_samples), add channel dimension
                    waveform = waveform.unsqueeze(1)
                elif waveform.dim() == 1:
                    # Shape is (time_samples), add batch and channel dimensions
                    waveform = waveform.unsqueeze(0).unsqueeze(0)
        else:
            # Fallback: use the vocoder directly when AudioSR is not available
            # Apply dynamic range decompression for consistency
            try:
                from audiosr.utils import spectral_de_normalize_torch
                mel_spec_denormalized = spectral_de_normalize_torch(mel_spec)
            except:
                # Fallback if AudioSR utils not available
                mel_spec_denormalized = torch.exp(mel_spec)
            waveform = self.vocoder(mel_spec_denormalized)
            if waveform.dim() == 2:
                waveform = waveform.unsqueeze(1)
        
        return waveform
class VAEFeatureExtract(nn.Module):
    def __init__(self, first_stage_config):
        super().__init__()
        self.vae = instantiate_from_config(first_stage_config)
        self.vae.eval()
        for p in self.vae.parameters(): p.requires_grad = False
    def forward(self, batch):
        with torch.no_grad(): vae_embed = self.vae.encode(batch).sample()
        return vae_embed.detach()

# --- Main LDM Class ---
class LatentDiffusion(nn.Module):
    def __init__(self, first_stage_config, cond_stage_config, unet_config, beta_schedule="linear", timesteps=1000, loss_type="l2", parameterization="v", scale_factor=1.0, scale_by_std=False, use_ema=True, **kwargs):
        super().__init__()
        self.scale_by_std, self.parameterization, self.model = scale_by_std, parameterization, DiffusionWrapper(unet_config)
        if use_ema: self.model_ema = CustomLitEma(self.model)
        self.first_stage_model = instantiate_from_config(first_stage_config)
        self.cond_stage_models, self.cond_stage_model_metadata = nn.ModuleList(), {}
        self.instantiate_cond_stage(cond_stage_config)
        self.clap = ClapWrapper()
        self.register_schedule(beta_schedule=beta_schedule, timesteps=timesteps, scale_factor=scale_factor)
        self.loss_type = loss_type
    def register_schedule(self, beta_schedule, timesteps, linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3, scale_factor=1.0):
        self.register_buffer('scale_factor', torch.tensor(scale_factor))
        betas = make_beta_schedule(beta_schedule, timesteps, linear_start=linear_start, linear_end=linear_end, cosine_s=cosine_s)
        alphas = 1. - betas
        alphas_cumprod = np.cumprod(alphas, axis=0)
        
        # --- FIX: Clip alphas_cumprod to prevent log(0) and division by zero warnings ---
        alphas_cumprod = np.clip(alphas_cumprod, a_min=0.0, a_max=1.0 - 1e-8)

        alphas_cumprod_prev = np.append(1., alphas_cumprod[:-1])
        to_torch = partial(torch.tensor, dtype=torch.float32)
        self.num_timesteps = int(timesteps)
        self.register_buffer('betas', to_torch(betas)); self.register_buffer('alphas_cumprod', to_torch(alphas_cumprod)); self.register_buffer('alphas_cumprod_prev', to_torch(alphas_cumprod_prev))
        self.register_buffer('sqrt_alphas_cumprod', to_torch(np.sqrt(alphas_cumprod))); self.register_buffer('sqrt_one_minus_alphas_cumprod', to_torch(np.sqrt(1. - alphas_cumprod)))
        self.register_buffer('log_one_minus_alphas_cumprod', to_torch(np.log(1. - alphas_cumprod))); self.register_buffer('sqrt_recip_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod)))
        self.register_buffer('sqrt_recipm1_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod - 1)))
        posterior_variance = (1 - self.scale_by_std) * betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)
        self.register_buffer('posterior_variance', to_torch(posterior_variance)); self.register_buffer('posterior_log_variance_clipped', to_torch(np.log(np.maximum(posterior_variance, 1e-20))))
        self.register_buffer('posterior_mean_coef1', to_torch(betas * np.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod)))
        self.register_buffer('posterior_mean_coef2', to_torch((1. - alphas_cumprod_prev) * np.sqrt(alphas) / (1. - alphas_cumprod)))
        self.register_buffer('logvar', torch.zeros(timesteps))
    def instantiate_cond_stage(self, config):
        for key, cfg in config.items():
            model = instantiate_from_config(cfg)
            self.cond_stage_models.append(model)
            self.cond_stage_model_metadata[key] = {"model_idx": len(self.cond_stage_models) - 1, "cond_stage_key": cfg.get("cond_stage_key"), "conditioning_key": cfg.get("conditioning_key")}
    @contextmanager
    def ema_scope(self, context=None):
        if hasattr(self, 'model_ema'):
            self.model_ema.store([p.data for p in self.model.parameters()]); self.model_ema.copy_to(self.model)
        try: yield None
        finally:
            if hasattr(self, 'model_ema'): self.model_ema.restore([p.data for p in self.model.parameters()])
    @torch.no_grad()
    def get_input(self, batch):
        x, c_concat_data = batch["fbank"], batch["lowpass_mel"]
        z = self.get_first_stage_encoding(self.first_stage_model.encode(x))
        cond = {}
        for key, meta in self.cond_stage_model_metadata.items():
            if meta["conditioning_key"] == "concat": cond[key] = self.cond_stage_models[meta["model_idx"]](c_concat_data)
        return z, cond
    def get_first_stage_encoding(self, encoder_posterior):
        z = encoder_posterior.sample() if isinstance(encoder_posterior, DiagonalGaussianDistribution) else encoder_posterior
        return self.scale_factor * z
    def q_sample(self, x_start, t, noise=None):
        noise = default(noise, lambda: torch.randn_like(x_start))
        return (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start + extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise)
    def get_v(self, x, noise, t): return (extract_into_tensor(self.sqrt_alphas_cumprod, t, x.shape) * noise - extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x.shape) * x)
    def apply_model(self, x_noisy, t, cond, full_audio=None): return self.model(x_noisy, t, cond, full_audio=full_audio)
    def get_loss(self, pred, target):
        if self.loss_type == 'l1': return F.l1_loss(pred, target)
        elif self.loss_type == 'l2': return F.mse_loss(pred, target)
        else: raise NotImplementedError()
    def p_losses(self, x_start, cond, t, noise=None, full_audio=None):
        noise = default(noise, lambda: torch.randn_like(x_start))
        x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)
        model_output = self.apply_model(x_noisy, t, cond, full_audio=full_audio)
        target = self.get_v(x_start, noise, t) if self.parameterization == "v" else noise
        loss = self.get_loss(model_output, target)
        return loss, {"loss": loss}
    def forward(self, batch):
        z, c = self.get_input(batch)
        t = torch.randint(0, self.num_timesteps, (z.shape[0],), device=z.device).long()
        
        # Pass the full audio (original high-quality mel) for global context
        full_audio = batch.get("fbank", None)  # Full audio for global encoder
        
        return self.p_losses(z, c, t, full_audio=full_audio)
    
    def load_checkpoint(self, checkpoint_path, strict=False):
        """
        Custom checkpoint loading function that handles missing keys gracefully
        """
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        if 'state_dict' in checkpoint:
            state_dict = checkpoint['state_dict']
        else:
            state_dict = checkpoint
        
        # Load with strict=False to handle missing keys
        missing_keys, unexpected_keys = self.load_state_dict(state_dict, strict=strict)
        
        if missing_keys:
            warnings.warn(f"Missing keys in checkpoint: {missing_keys[:10]}... (showing first 10)")
        if unexpected_keys:
            warnings.warn(f"Unexpected keys in checkpoint: {unexpected_keys[:10]}... (showing first 10)")
        
        return missing_keys, unexpected_keys

class DiffusionWrapper(nn.Module):
    def __init__(self, unet_config):
        super().__init__()
        self.diffusion_model = instantiate_from_config(unet_config)
    def forward(self, x, t, cond_dict={}, full_audio=None):
        xc, context = x, None
        for key, value in cond_dict.items():
            if "concat" in key: 
                # Ensure spatial dimensions match before concatenation
                if value.shape[-2:] != x.shape[-2:]:
                    # Interpolate conditioning tensor to match input spatial dimensions
                    value = F.interpolate(value, size=x.shape[-2:], mode='bilinear', align_corners=False)
                xc = torch.cat([x, value], dim=1)
        return self.diffusion_model(xc, t, context=context, full_audio=full_audio)
```

## 文件: `preprocess_audio.py`

```python

import os
import argparse
from typing import Optional, List
import numpy as np
import soundfile as sf
import torch
import torch.nn.functional as F

def load_audio_mono(path: str):
    audio, sr = sf.read(path, always_2d=False)
    if audio.ndim == 2:
        audio = audio.mean(axis=1)
    return audio.astype(np.float32), sr

def resample(audio: np.ndarray, sr_in: int, sr_out: int) -> np.ndarray:
    if sr_in == sr_out:
        return audio
    x = torch.tensor(audio, dtype=torch.float32)[None, None, :]
    ratio = sr_out / sr_in
    new_len = int(round(x.shape[-1] * ratio))
    y = F.interpolate(x, size=new_len, mode='linear', align_corners=False)
    return y[0,0].numpy()

def windowed_sinc_lowpass(cutoff_hz: float, sr: int, kernel_size: int = 127, eps: float = 1e-8):
    """
    Simple windowed-sinc low-pass filter kernel for 1D conv.
    cutoff_hz: pass-band edge
    """
    # normalized cutoff in [0,1], 1 -> Nyquist (sr/2)
    wc = cutoff_hz / (sr / 2 + eps)
    n = torch.arange(kernel_size) - (kernel_size - 1) / 2
    # sinc
    h = torch.where(n == 0, torch.tensor(1.0), torch.sin(torch.tensor(np.pi) * wc * n) / (torch.tensor(np.pi) * wc * n))
    # Hann window
    w = 0.5 * (1 - torch.cos(2 * torch.tensor(np.pi) * (torch.arange(kernel_size) / (kernel_size - 1))))
    h = h * w
    # normalize
    h = h / h.sum()
    return h.float()

def apply_lowpass(audio: np.ndarray, sr: int, cutoff_hz: float) -> np.ndarray:
    kernel = windowed_sinc_lowpass(cutoff_hz, sr, kernel_size=127)
    x = torch.tensor(audio, dtype=torch.float32)[None, None, :]
    k = kernel[None, None, :]
    y = F.conv1d(x, k, padding=kernel.shape[0]//2)
    return y[0,0].numpy()

def make_low_res(audio: np.ndarray, sr: int, target_sr_low: int = 12000, cutoff_hz: Optional[float] = None) -> np.ndarray:
    """
    Create a low-resolution version by (optional) LPF -> downsample -> upsample back to sr.
    """
    if cutoff_hz is None:
        cutoff_hz = min(target_sr_low // 2 - 1000, 6000)  # leave guard band
        cutoff_hz = max(2000, cutoff_hz)
    y = apply_lowpass(audio, sr=sr, cutoff_hz=cutoff_hz)
    y = resample(y, sr_in=sr, sr_out=target_sr_low)
    y = resample(y, sr_in=target_sr_low, sr_out=sr)  # bring back to original sr
    return y

def process_file(path_in: str, out_high: str, out_low: str, target_sr: int = 48000, target_sr_low: int = 12000, mp3_lpf_hz: Optional[float] = 16000):
    audio, sr = load_audio_mono(path_in)
    # resample to target_sr
    audio = resample(audio, sr_in=sr, sr_out=target_sr)

    # optional low-pass to attenuate MP3 artifacts (typically above ~16k for many bitrates)
    if mp3_lpf_hz is not None:
        audio = apply_lowpass(audio, sr=target_sr, cutoff_hz=mp3_lpf_hz)

    # synthesize low-res degraded input
    low = make_low_res(audio, sr=target_sr, target_sr_low=target_sr_low)

    os.makedirs(os.path.dirname(out_high), exist_ok=True)
    os.makedirs(os.path.dirname(out_low), exist_ok=True)
    sf.write(out_high, audio, target_sr)
    sf.write(out_low,  low,  target_sr)
    return out_high, out_low

def preprocess_dir(in_dir: str, out_high: str, out_low: str, sr: int = 48000, low_sr: int = 12000, mp3_lpf_hz: Optional[float] = 16000) -> List[str]:
    """Batch preprocess a directory. Returns list of processed basenames."""
    exts = ('.wav', '.flac', '.ogg', '.mp3')
    files = [f for f in os.listdir(in_dir) if f.lower().endswith(exts)]
    processed = []
    for f in files:
        in_path = os.path.join(in_dir, f)
        high_path = os.path.join(out_high, os.path.splitext(f)[0] + '.wav')
        low_path  = os.path.join(out_low,  os.path.splitext(f)[0] + '.wav')
        process_file(in_path, high_path, low_path, target_sr=sr, target_sr_low=low_sr, mp3_lpf_hz=mp3_lpf_hz)
        processed.append(f)
    return processed

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--in_dir', type=str, required=True, help='Input directory containing wav/flac/ogg/mp3')
    parser.add_argument('--out_high', type=str, required=True, help='Output directory for 48k high-quality wav')
    parser.add_argument('--out_low',  type=str, required=True, help='Output directory for degraded low-res wav')
    parser.add_argument('--sr', type=int, default=48000)
    parser.add_argument('--low_sr', type=int, default=12000, help='Intermediate bottleneck sample rate to simulate bandwidth loss')
    parser.add_argument('--mp3_lpf_hz', type=float, default=16000.0, help='Apply pre LPF at this cutoff to reduce MP3 artifacts; set -1 to disable')
    args = parser.parse_args()

    mp3_lpf = None if args.mp3_lpf_hz < 0 else float(args.mp3_lpf_hz)
    processed = preprocess_dir(args.in_dir, args.out_high, args.out_low, sr=args.sr, low_sr=args.low_sr, mp3_lpf_hz=mp3_lpf)
    for f in processed:
        print('Processed:', f)

if __name__ == '__main__':
    main()

```

## 文件: `run_gui.py`

```python
import os
import sys
import yaml
import threading
import tkinter as tk
from tkinter import ttk, filedialog, scrolledtext
import random
import shutil
import subprocess

# --- This script is the unified GUI for the 2D Spectrogram pipeline ---

BASE = os.path.dirname(os.path.abspath(__file__))
if BASE not in sys.path:
    sys.path.append(BASE)

# Import the backend modules for preprocessing
import preprocess_audio as prep

CFG_PATH = os.path.join(BASE, 'config.yaml')

def ensure_dirs(cfg):
    """Ensures that all necessary directories from the config file exist."""
    # This function now uses paths from the new config structure
    dataset_root = os.path.join(BASE, cfg['data']['dataset_root'])
    # Simplified to ensure train/valid parent folders exist.
    os.makedirs(os.path.join(dataset_root, 'train'), exist_ok=True)
    os.makedirs(os.path.join(dataset_root, 'valid'), exist_ok=True)
    os.makedirs(os.path.join(BASE, cfg['experiment']['out_dir']), exist_ok=True)

class AudioSR_GUI(tk.Tk):
    def __init__(self, cfg):
        super().__init__()
        self.cfg = cfg
        self.title("AudioSR 2D Spectrogram Pipeline (Desktop GUI)")
        self.geometry("800x700") # Adjusted height for new options

        style = ttk.Style(self)
        style.theme_use('clam')

        self.notebook = ttk.Notebook(self)
        self.notebook.pack(expand=True, fill='both', padx=10, pady=10)

        self.create_preprocess_tab()
        self.create_train_tab()
        self.create_inference_tab()

    def create_preprocess_tab(self):
        tab = ttk.Frame(self.notebook)
        self.notebook.add(tab, text='数据准备 / Data Prep')

        prep_frame = ttk.LabelFrame(tab, text="第一步: 生成训练数据", padding=(10, 5))
        prep_frame.pack(fill='x', padx=10, pady=5, anchor='n')
        
        self.in_dir_var = tk.StringVar(value='raw_audio')
        self.out_train_high_var = tk.StringVar(value=os.path.join(self.cfg['data']['dataset_root'], 'train', self.cfg['data']['high_dir_name']))
        self.out_train_low_var = tk.StringVar(value=os.path.join(self.cfg['data']['dataset_root'], 'train', self.cfg['data']['low_dir_name']))

        ttk.Label(prep_frame, text="原始音频目录:").grid(row=0, column=0, sticky='w', padx=5, pady=2)
        ttk.Entry(prep_frame, textvariable=self.in_dir_var, width=80).grid(row=0, column=1, sticky='ew', padx=5, pady=2)
        ttk.Label(prep_frame, text="训练集输出 (高):").grid(row=1, column=0, sticky='w', padx=5, pady=2)
        ttk.Entry(prep_frame, textvariable=self.out_train_high_var, state='readonly').grid(row=1, column=1, sticky='ew', padx=5, pady=2)
        ttk.Label(prep_frame, text="训练集输出 (低):").grid(row=2, column=0, sticky='w', padx=5, pady=2)
        ttk.Entry(prep_frame, textvariable=self.out_train_low_var, state='readonly').grid(row=2, column=1, sticky='ew', padx=5, pady=2)
        prep_frame.columnconfigure(1, weight=1)

        self.btn_p = ttk.Button(prep_frame, text="开始预处理", command=self.start_preprocess)
        self.btn_p.grid(row=4, column=0, columnspan=2, pady=10)
        
        split_frame = ttk.LabelFrame(tab, text="第二步: 划分验证集", padding=(10, 5))
        split_frame.pack(fill='x', padx=10, pady=10, anchor='n')
        
        self.valid_ratio_var = tk.DoubleVar(value=5.0)
        
        ttk.Label(split_frame, text="验证集比例 (%):").grid(row=0, column=0, sticky='w', padx=5, pady=5)
        self.ratio_slider = ttk.Scale(split_frame, from_=0, to=30, orient='horizontal', variable=self.valid_ratio_var, length=200)
        self.ratio_slider.grid(row=0, column=1, sticky='ew', padx=5, pady=5)
        self.ratio_label = ttk.Label(split_frame, text=f"{self.valid_ratio_var.get():.1f}%")
        self.ratio_label.grid(row=0, column=2, padx=10, pady=5)
        self.valid_ratio_var.trace_add('write', self.update_ratio_label)
        
        self.btn_s = ttk.Button(split_frame, text="开始划分", command=self.start_split_valid_set)
        self.btn_s.grid(row=1, column=0, columnspan=3, pady=10)
        split_frame.columnconfigure(1, weight=1)

        log_frame = ttk.LabelFrame(tab, text="日志", padding=(10, 5))
        log_frame.pack(fill='both', expand=True, padx=10, pady=5)
        self.log_p = scrolledtext.ScrolledText(log_frame, wrap=tk.WORD, height=10)
        self.log_p.pack(fill='both', expand=True)

    def update_ratio_label(self, *args):
        self.ratio_label.config(text=f"{self.valid_ratio_var.get():.1f}%")

    def create_train_tab(self):
        tab = ttk.Frame(self.notebook)
        self.notebook.add(tab, text='训练 / Train')

        ft_frame = ttk.LabelFrame(tab, text="微调 / 预训练设置", padding=(10, 5))
        ft_frame.pack(fill='x', padx=10, pady=10)

        self.pretrained_path_var = tk.StringVar(value=self.cfg['train'].get('pretrained_path', ''))
        self.key_filter_var = tk.StringVar(value=self.cfg['train'].get('key_filter_contains', 'model.diffusion_model.'))
        self.freeze_strategy_var = tk.StringVar(value="不冻结 (No Freeze)")
        self.freeze_custom_var = tk.StringVar(value=", ".join(self.cfg['train'].get('freeze_substrings', [])))

        ttk.Label(ft_frame, text="预训练权重路径:").grid(row=0, column=0, sticky='w', padx=5, pady=3)
        ttk.Entry(ft_frame, textvariable=self.pretrained_path_var, width=70).grid(row=0, column=1, sticky='ew', padx=5, pady=3)
        ttk.Button(ft_frame, text="浏览...", command=lambda: self.select_file(self.pretrained_path_var, "选择预训练权重")).grid(row=0, column=2, padx=5, pady=3)

        ttk.Label(ft_frame, text="权重键过滤器:").grid(row=1, column=0, sticky='w', padx=5, pady=3)
        ttk.Entry(ft_frame, textvariable=self.key_filter_var, width=70).grid(row=1, column=1, sticky='ew', padx=5, pady=3)
        
        ttk.Label(ft_frame, text="冻结策略:").grid(row=2, column=0, sticky='w', padx=5, pady=3)
        freeze_options = ["不冻结 (No Freeze)", "冻结编码器 (Freeze Encoder)", "仅微调注意力层 (Finetune Attention)", "自定义 (Custom)"]
        self.freeze_menu = ttk.OptionMenu(ft_frame, self.freeze_strategy_var, freeze_options[0], *freeze_options, command=self.on_freeze_strategy_change)
        self.freeze_menu.grid(row=2, column=1, sticky='w', padx=5, pady=3)

        self.custom_freeze_label = ttk.Label(ft_frame, text="自定义冻结层:")
        self.custom_freeze_entry = ttk.Entry(ft_frame, textvariable=self.freeze_custom_var, width=70)
        
        ft_frame.columnconfigure(1, weight=1)

        info_text = ("提示: 其他训练参数 (如模型架构、学习率等) 请直接修改 `config.yaml` 文件。")
        ttk.Label(tab, text=info_text, wraplength=750, justify='center').pack(pady=5)

        self.btn_t = ttk.Button(tab, text="开始训练", command=self.start_train, style='Accent.TButton')
        self.btn_t.pack(pady=20)

        log_frame = ttk.LabelFrame(tab, text="训练日志", padding=(10, 5))
        log_frame.pack(fill='both', expand=True, padx=10, pady=10)
        self.log_t = scrolledtext.ScrolledText(log_frame, wrap=tk.WORD, height=15)
        self.log_t.pack(fill='both', expand=True)

    def on_freeze_strategy_change(self, strategy):
        if strategy == "自定义 (Custom)":
            self.custom_freeze_label.grid(row=3, column=0, sticky='w', padx=5, pady=3)
            self.custom_freeze_entry.grid(row=3, column=1, sticky='ew', padx=5, pady=3)
        else:
            self.custom_freeze_label.grid_forget()
            self.custom_freeze_entry.grid_forget()

    def create_inference_tab(self):
        tab = ttk.Frame(self.notebook)
        self.notebook.add(tab, text='推理 / Inference')
        
        info_frame = ttk.LabelFrame(tab, text="功能说明", padding=(10, 5))
        info_frame.pack(fill='both', expand=True, padx=10, pady=10)
        
        info_text = ("推理功能正在开发中。\n\n"
                     "2D频谱工作流的模型输出的是频谱图，需要一个声码器 (Vocoder) "
                     "或使用 Griffin-Lim 算法才能将其转换回音频波形。\n\n"
                     "我们将在后续步骤中构建此功能。")
        ttk.Label(info_frame, text=info_text, wraplength=750, justify='center', font=("", 12)).pack(pady=20, padx=10, expand=True)

    def run_in_thread(self, target, *args):
        threading.Thread(target=target, args=args, daemon=True).start()

    def select_file(self, string_var, title):
        filepath = filedialog.askopenfilename(title=title, filetypes=(("PyTorch Checkpoints", "*.pt *.ckpt"), ("All files", "*.*")))
        if filepath:
            string_var.set(filepath)

    def start_preprocess(self):
        self.btn_p.config(state=tk.DISABLED)
        self.log_p.delete('1.0', tk.END)
        self.log_p.insert(tk.END, "开始预处理...\n")
        self.run_in_thread(self._preprocess_task)

    def _preprocess_task(self):
        try:
            in_dir_abs = os.path.join(BASE, self.in_dir_var.get())
            out_high_abs = os.path.join(BASE, self.out_train_high_var.get())
            out_low_abs = os.path.join(BASE, self.out_train_low_var.get())
            processed = prep.preprocess_dir(in_dir_abs, out_high_abs, out_low_abs, sr=self.cfg['data']['sample_rate'], low_sr=12000)
            log_msg = '\n'.join([f'Processed: {x}' for x in processed]) or 'No files found.'
            self.log_p.insert(tk.END, log_msg + "\n预处理完成。\n")
        except Exception as e:
            self.log_p.insert(tk.END, f"[ERROR] {repr(e)}\n")
        finally:
            self.btn_p.config(state=tk.NORMAL)

    def start_split_valid_set(self):
        self.btn_s.config(state=tk.DISABLED)
        self.log_p.insert(tk.END, "\n开始划分验证集...\n")
        self.run_in_thread(self._split_valid_set_task)

    def _split_valid_set_task(self):
        try:
            ratio = self.valid_ratio_var.get()
            if ratio <= 0:
                self.log_p.insert(tk.END, "划分比例为0，无需操作。\n")
                return

            train_high_path = os.path.join(BASE, self.out_train_high_var.get())
            train_low_path = os.path.join(BASE, self.out_train_low_var.get())
            valid_high_path = os.path.join(BASE, self.cfg['data']['dataset_root'], 'valid', self.cfg['data']['high_dir_name'])
            valid_low_path = os.path.join(BASE, self.cfg['data']['dataset_root'], 'valid', self.cfg['data']['low_dir_name'])
            
            os.makedirs(valid_high_path, exist_ok=True)
            os.makedirs(valid_low_path, exist_ok=True)

            if not os.path.exists(train_high_path):
                raise FileNotFoundError(f"训练集目录不存在: {train_high_path}")

            files = [f for f in os.listdir(train_high_path) if f.lower().endswith(('.wav', '.flac', '.ogg', '.mp3'))]
            if not files:
                self.log_p.insert(tk.END, "训练集中没有找到音频文件，无法划分。\n")
                return
            
            num_to_move = int(len(files) * (ratio / 100.0))
            if num_to_move == 0 and len(files) > 0:
                 self.log_p.insert(tk.END, f"文件总数太少 ({len(files)})，按比例计算需要移动0个文件。\n")
                 return

            files_to_move = random.sample(files, num_to_move)
            moved_count = 0
            for f in files_to_move:
                src_high, dst_high = os.path.join(train_high_path, f), os.path.join(valid_high_path, f)
                src_low, dst_low = os.path.join(train_low_path, f), os.path.join(valid_low_path, f)
                if os.path.exists(src_high) and os.path.exists(src_low):
                    shutil.move(src_high, dst_high)
                    shutil.move(src_low, dst_low)
                    self.log_p.insert(tk.END, f"  移动: {f}\n")
                    moved_count += 1
            
            self.log_p.insert(tk.END, f"\n划分完成！共移动 {moved_count} 对文件到验证集。\n")
        except Exception as e:
            self.log_p.insert(tk.END, f"[ERROR] {repr(e)}\n")
        finally:
            self.btn_s.config(state=tk.NORMAL)

    def start_train(self):
        self.btn_t.config(state=tk.DISABLED)
        self.log_t.delete('1.0', tk.END)
        self.log_t.insert(tk.END, "准备启动训练...\n")
        self.run_in_thread(self._train_task)

    def _train_task(self):
        try:
            with open(CFG_PATH, 'r', encoding='utf-8') as f:
                current_cfg = yaml.safe_load(f)
            
            current_cfg['train']['pretrained_path'] = self.pretrained_path_var.get() or None
            current_cfg['train']['key_filter_contains'] = self.key_filter_var.get() or None

            strategy = self.freeze_strategy_var.get()
            freeze_list = []
            if strategy == "冻结编码器 (Freeze Encoder)":
                freeze_list = ["input_blocks."]
            elif strategy == "仅微调注意力层 (Finetune Attention)":
                self.log_t.insert(tk.END, "注意: '仅微调注意力层'策略会冻结所有非注意力块。\n")
                # This freezes all ResBlocks and convs, leaving AttnBlocks trainable
                freeze_list = ["input_blocks.0", "input_blocks.1.0", "input_blocks.2.0", "input_blocks.3.0", "input_blocks.4.0", "input_blocks.5.0", "input_blocks.6.0", "input_blocks.7.0", "input_blocks.8.0", "input_blocks.9.0", "input_blocks.10.0", "input_blocks.11.0", "middle_block.0", "middle_block.2", "output_blocks.", "out."]
            elif strategy == "自定义 (Custom)":
                freeze_str = self.freeze_custom_var.get()
                if freeze_str:
                    freeze_list = [s.strip() for s in freeze_str.split(',') if s.strip()]
            current_cfg['train']['freeze_substrings'] = freeze_list

            with open(CFG_PATH, 'w', encoding='utf-8') as f:
                yaml.dump(current_cfg, f, sort_keys=False, allow_unicode=True)

            self.log_t.insert(tk.END, f"配置文件已更新。\n使用预训练权重: {current_cfg['train']['pretrained_path']}\n冻结策略: '{strategy}' -> {freeze_list}\n")
            self.log_t.insert(tk.END, f"启动 2D 频谱管线...\n")

            cmd = [sys.executable, os.path.join(BASE, 'train.py'), '--config', CFG_PATH]
            process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, encoding='utf-8', errors='replace', bufsize=1)
            for line in iter(process.stdout.readline, ''):
                self.log_t.insert(tk.END, line)
                self.log_t.see(tk.END)
            process.stdout.close()
            return_code = process.wait()
            if return_code:
                self.log_t.insert(tk.END, f"\n--- 训练进程异常结束，返回码: {return_code} ---\n")
            else:
                self.log_t.insert(tk.END, f"\n--- 训练进程正常结束 ---\n")
        except Exception as e:
            self.log_t.insert(tk.END, f"\n[GUI ERROR] 启动训练失败: {repr(e)}\n")
        finally:
            self.btn_t.config(state=tk.NORMAL)

if __name__ == '__main__':
    try:
        with open(CFG_PATH, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        ensure_dirs(config)
        app = AudioSR_GUI(config)
        app.mainloop()
    except FileNotFoundError:
        print(f"错误: 配置文件 'config.yaml' 未找到。请确保它在脚本所在的目录中。")
    except Exception as e:
        print(f"启动时发生错误: {e}")

```

## 文件: `train_gui.py`

```python
import os
import sys
import yaml
import math
import threading
import argparse
from typing import Optional

# --- GUI Imports ---
import tkinter as tk
from tkinter import ttk, filedialog, messagebox

# --- Plotting Imports ---
import matplotlib.pyplot as plt
from matplotlib.figure import Figure
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg

# --- Training Imports ---
import torch
from torch.utils.data import DataLoader, Subset
from rich.console import Console
from rich.progress import Progress, BarColumn, TextColumn, TimeElapsedColumn, MofNCompleteColumn, SpinnerColumn

# --- Local Project Imports ---
# 确保这些文件与 train_gui.py 在同一目录下
from model import LatentDiffusion 
from dataset_loader import PairedMelDataset
from audiosr.latent_diffusion.util import instantiate_from_config

# --- Base Configuration ---
BASE = os.path.dirname(os.path.abspath(__file__))
CFG_PATH = os.path.join(BASE, 'config.yaml')

# =====================================================================================
# SECTION: UTILITY & CORE TRAINING LOGIC
# =====================================================================================

def ensure_dir(p: str):
    """Ensures a single directory exists."""
    os.makedirs(p, exist_ok=True)

def ensure_project_dirs(cfg: dict):
    """Ensures that all necessary directories from the config file exist."""
    try:
        # Create main output directory and its checkpoints subdir
        outp = os.path.join(BASE, cfg['experiment']['out_dir'], 'checkpoints')
        ensure_dir(outp)
        
        # Create data directories
        dataset_root = os.path.join(BASE, cfg['data']['dataset_root'])
        ensure_dir(os.path.join(dataset_root, 'train', cfg['data']['high_dir_name']))
        ensure_dir(os.path.join(dataset_root, 'train', cfg['data']['low_dir_name']))
        ensure_dir(os.path.join(dataset_root, 'valid', cfg['data']['high_dir_name']))
        ensure_dir(os.path.join(dataset_root, 'valid', cfg['data']['low_dir_name']))
    except KeyError as e:
        raise KeyError(f"Config file is missing a required key: {e}. Please check your 'config.yaml'.")


def set_seed(seed: int):
    import random, numpy as np
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

class EMA:
    """
    Exponential Moving Average of model parameters, maintained in FP32 for stability.
    """
    def __init__(self, model, decay: float):
        self.decay = decay
        # Create a shadow copy of parameters in full FP32 precision
        self.shadow = {
            k: p.data.clone().to(dtype=torch.float32)
            for k, p in model.named_parameters() if p.requires_grad
        }
    
    def update(self, model):
        with torch.no_grad():
            for k, p in model.named_parameters():
                if p.requires_grad and k in self.shadow:
                    p_fp32 = p.data.to(dtype=torch.float32)
                    self.shadow[k] = (1.0 - self.decay) * p_fp32 + self.decay * self.shadow[k]

class AdvancedLossMonitor:
    """
    Advanced loss monitoring with explosion detection, adaptive LR, and early stopping.
    """
    def __init__(self, explosion_threshold=10.0, lr_reduction_factor=0.5, patience=1000, min_lr=1e-7):
        self.explosion_threshold = explosion_threshold
        self.lr_reduction_factor = lr_reduction_factor
        self.patience = patience
        self.min_lr = min_lr
        
        # Loss tracking
        self.loss_history = []
        self.best_loss = float('inf')
        self.steps_without_improvement = 0
        self.lr_reductions = 0
        self.exploded_count = 0
        
        # Moving averages for stability
        self.short_ma = 0.0  # Short-term moving average (last 10 steps)
        self.long_ma = 0.0   # Long-term moving average (last 100 steps)
        self.ma_alpha_short = 0.1  # Smoothing for short MA
        self.ma_alpha_long = 0.01  # Smoothing for long MA
        
    def update(self, loss_value, step):
        """Update loss monitoring and return actions to take."""
        actions = {
            'reduce_lr': False,
            'stop_training': False,
            'skip_step': False,
            'message': ''
        }
        
        # Check for NaN or infinite loss
        if not torch.isfinite(torch.tensor(loss_value)):
            actions['skip_step'] = True
            actions['message'] = f"⚠️ NaN/Inf loss detected at step {step}, skipping step"
            return actions
        
        # Update moving averages
        if len(self.loss_history) == 0:
            self.short_ma = loss_value
            self.long_ma = loss_value
        else:
            self.short_ma = (1 - self.ma_alpha_short) * self.short_ma + self.ma_alpha_short * loss_value
            self.long_ma = (1 - self.ma_alpha_long) * self.long_ma + self.ma_alpha_long * loss_value
        
        self.loss_history.append(loss_value)
        
        # 1. Loss Explosion Detection
        if loss_value > self.explosion_threshold:
            self.exploded_count += 1
            actions['reduce_lr'] = True
            actions['message'] = f"🔥 Loss explosion detected ({loss_value:.4f} > {self.explosion_threshold}), reducing LR"
            
            # If multiple explosions, be more aggressive
            if self.exploded_count > 3:
                actions['reduce_lr'] = True
                actions['message'] += f" (explosion #{self.exploded_count})"
        
        # 2. Adaptive Learning Rate (Plateau Detection)
        elif len(self.loss_history) > 100:
            # Check if loss is plateauing (short MA not improving over long MA)
            if self.short_ma > self.long_ma * 1.01 and step % 500 == 0:  # Check every 500 steps
                actions['reduce_lr'] = True
                actions['message'] = f"📉 Loss plateau detected (short: {self.short_ma:.4f}, long: {self.long_ma:.4f}), reducing LR"
        
        # 3. Track best loss for early stopping
        if loss_value < self.best_loss:
            self.best_loss = loss_value
            self.steps_without_improvement = 0
        else:
            self.steps_without_improvement += 1
        
        # 4. Early Stopping Check
        if self.steps_without_improvement >= self.patience:
            actions['stop_training'] = True
            actions['message'] = f"🛑 Early stopping: no improvement for {self.patience} steps"
        
        return actions
    
    def reduce_learning_rate(self, optimizer, console):
        """Reduce learning rate for all parameter groups."""
        old_lr = optimizer.param_groups[0]['lr']
        new_lr = max(old_lr * self.lr_reduction_factor, self.min_lr)
        
        for param_group in optimizer.param_groups:
            param_group['lr'] = new_lr
            
        self.lr_reductions += 1
        self.exploded_count = max(0, self.exploded_count - 1)  # Reset explosion counter after LR reduction
        
        console.print(f"[yellow]📉 Learning rate reduced: {old_lr:.2e} → {new_lr:.2e} (reduction #{self.lr_reductions})[/yellow]")
        
        if new_lr <= self.min_lr:
            console.print(f"[red]⚠️ Learning rate hit minimum ({self.min_lr:.2e})[/red]")
            
        return new_lr
    
    def get_stats(self):
        """Get monitoring statistics."""
        return {
            'best_loss': self.best_loss,
            'steps_without_improvement': self.steps_without_improvement,
            'lr_reductions': self.lr_reductions,
            'exploded_count': self.exploded_count,
            'short_ma': self.short_ma,
            'long_ma': self.long_ma
        }

class WarmupCosine:
    """Learning rate scheduler with warmup and cosine decay."""
    def __init__(self, opt, base_lr, warmup, max_steps, min_ratio=0.1):
        self.opt, self.base, self.w, self.m, self.r = opt, base_lr, warmup, max_steps, min_ratio
        self.step_id = 0
    
    def step(self):
        if self.step_id < self.w:
            f = (self.step_id + 1) / max(1, self.w)
        else:
            t = (self.step_id - self.w) / max(1, self.m - self.w)
            f = self.r + 0.5 * (1 - self.r) * (1 + math.cos(math.pi * t))
        for g in self.opt.param_groups:
            g['lr'] = self.base * f
        self.step_id += 1

def load_pretrained(model: torch.nn.Module, path: str, device: str, console: Console):
    if not path or not os.path.exists(path):
        console.print("[yellow]No pretrained path found. Starting from scratch.[/yellow]")
        return
    
    console.print(f"[cyan]Loading pretrained weights from:[/] [default]{path}[/]")
    ckpt = torch.load(path, map_location=device)
    
    sd = ckpt.get("model", ckpt.get("state_dict", ckpt))

    missing, unexpected = model.load_state_dict(sd, strict=False)
    
    console.print(f"[green]Weights loaded.[/] Missing keys: {len(missing)}, Unexpected keys: {len(unexpected)}")
    if missing:
        console.print(f"  [dim]Missing (first 5): {missing[:5]}[/dim]")
    if unexpected:
        console.print(f"  [dim]Unexpected (first 5): {unexpected[:5]}[/dim]")

# =====================================================================================
# SECTION: MAIN GUI APPLICATION
# =====================================================================================

class TrainingApp(tk.Tk):
    def __init__(self, cfg):
        super().__init__()
        self.cfg = cfg
        self.title("AudioSR Integrated Trainer - Performance Edition")
        self.geometry("1000x850")

        # --- Style Configuration ---
        style = ttk.Style(self)
        style.theme_use('clam')
        style.configure('TNotebook.Tab', font=('Helvetica', 12, 'bold'))
        style.configure('Accent.TButton', font=('Helvetica', 12, 'bold'), foreground='white', background='#0078D7')
        style.configure('TCheckbutton', font=('Helvetica', 10))

        # --- Rich Console for terminal output ---
        self.console = Console()

        # --- Main Layout ---
        self.create_widgets()
    
    def _update_layer_selection_state(self, *args):
        """Enable/disable custom layer selection based on training mode."""
        is_custom = self.training_mode.get() == "custom"
        state = 'normal' if is_custom else 'disabled'
        for child in self.custom_layers_frame.winfo_children():
            if isinstance(child, ttk.Checkbutton):
                child.configure(state=state)

    def create_widgets(self):
        # --- Top control frame ---
        control_frame = ttk.LabelFrame(self, text="Training Control & Optimizations", padding=(15, 10))
        control_frame.pack(fill='x', padx=15, pady=10)

        # Row 0: Pretrained model path
        self.pretrained_path_var = tk.StringVar(value=self.cfg['train'].get('pretrained_path', ''))
        ttk.Label(control_frame, text="Pretrained Weights:").grid(row=0, column=0, sticky='w', padx=5, pady=5)
        ttk.Entry(control_frame, textvariable=self.pretrained_path_var, width=70).grid(row=0, column=1, columnspan=2, sticky='ew', padx=5, pady=5)
        ttk.Button(control_frame, text="Browse...", command=self.select_file).grid(row=0, column=3, padx=5, pady=5)
        
        # Row 1: Gradient accumulation steps
        self.grad_accum_var = tk.StringVar(value=str(self.cfg['train'].get('gradient_accumulation_steps', 1)))
        ttk.Label(control_frame, text="Gradient Accumulation Steps:").grid(row=1, column=0, sticky='w', padx=5, pady=5)
        ttk.Spinbox(control_frame, from_=1, to=128, textvariable=self.grad_accum_var, width=8).grid(row=1, column=1, sticky='w', padx=5, pady=5)
        
        # Row 2: Performance Toggles
        perf_frame = ttk.Frame(control_frame)
        perf_frame.grid(row=2, column=0, columnspan=4, sticky='w', pady=10)
        
        self.fastboot_var = tk.BooleanVar(value=self.cfg['train'].get('fastboot', True))
        ttk.Checkbutton(perf_frame, text="Fastboot (Async Loading)", variable=self.fastboot_var).pack(side='left', padx=10)

        self.jit_var = tk.BooleanVar(value=self.cfg['train'].get('use_jit_compile', True))
        ttk.Checkbutton(perf_frame, text="Use JIT Compilation", variable=self.jit_var).pack(side='left', padx=10)

        self.grad_checkpoint_var = tk.BooleanVar(value=self.cfg['train'].get('use_gradient_checkpointing', True))
        ttk.Checkbutton(perf_frame, text="Use Gradient Checkpointing", variable=self.grad_checkpoint_var).pack(side='left', padx=10)
        
        # Advanced Loss Monitoring Frame
        loss_frame = ttk.LabelFrame(control_frame, text="Advanced Loss Monitoring", padding=(10, 5))
        loss_frame.grid(row=2, column=0, columnspan=4, sticky='ew', pady=5)
        
        # Loss explosion detection
        self.loss_explosion_var = tk.BooleanVar(value=True)
        ttk.Checkbutton(loss_frame, text="Loss Explosion Protection", variable=self.loss_explosion_var).grid(row=0, column=0, sticky='w', padx=5)
        
        ttk.Label(loss_frame, text="Explosion Threshold:").grid(row=0, column=1, sticky='w', padx=5)
        self.explosion_threshold_var = tk.StringVar(value="10.0")
        ttk.Entry(loss_frame, textvariable=self.explosion_threshold_var, width=8).grid(row=0, column=2, padx=5)
        
        # Adaptive learning rate
        self.adaptive_lr_var = tk.BooleanVar(value=True)
        ttk.Checkbutton(loss_frame, text="Adaptive Learning Rate", variable=self.adaptive_lr_var).grid(row=1, column=0, sticky='w', padx=5)
        
        ttk.Label(loss_frame, text="Reduction Factor:").grid(row=1, column=1, sticky='w', padx=5)
        self.lr_reduction_var = tk.StringVar(value="0.5")
        ttk.Entry(loss_frame, textvariable=self.lr_reduction_var, width=8).grid(row=1, column=2, padx=5)
        
        # Early stopping
        self.early_stopping_var = tk.BooleanVar(value=False)
        ttk.Checkbutton(loss_frame, text="Early Stopping", variable=self.early_stopping_var).grid(row=2, column=0, sticky='w', padx=5)
        
        ttk.Label(loss_frame, text="Patience (steps):").grid(row=2, column=1, sticky='w', padx=5)
        self.patience_var = tk.StringVar(value="1000")
        ttk.Entry(loss_frame, textvariable=self.patience_var, width=8).grid(row=2, column=2, padx=5)

        control_frame.columnconfigure(1, weight=1)
        
        # Row 4: Layer Selection Frame (moved down)
        layer_frame = ttk.LabelFrame(control_frame, text="Selective Layer Training", padding=(10, 5))
        layer_frame.grid(row=4, column=0, columnspan=4, sticky='ew', pady=10)
        
        # Training mode selection
        self.training_mode = tk.StringVar(value="full")
        ttk.Radiobutton(layer_frame, text="Full Model Training", variable=self.training_mode, value="full").grid(row=0, column=0, sticky='w', padx=5)
        ttk.Radiobutton(layer_frame, text="Global Encoder Only", variable=self.training_mode, value="global_encoder").grid(row=0, column=1, sticky='w', padx=5)
        ttk.Radiobutton(layer_frame, text="U-Net Only (Freeze Global Encoder)", variable=self.training_mode, value="unet_only").grid(row=0, column=2, sticky='w', padx=5)
        ttk.Radiobutton(layer_frame, text="Custom Layer Selection", variable=self.training_mode, value="custom").grid(row=1, column=0, sticky='w', padx=5)
        
        # Custom layer selection (initially disabled)
        self.custom_layers_frame = ttk.Frame(layer_frame)
        self.custom_layers_frame.grid(row=2, column=0, columnspan=4, sticky='ew', pady=5)
        
        # Layer checkboxes
        self.layer_vars = {}
        layer_options = [
            ("diffusion_model.global_encoder", "Global Audio Encoder"),
            ("diffusion_model.input_blocks", "U-Net Input Blocks"),
            ("diffusion_model.middle_block", "U-Net Middle Block"),
            ("diffusion_model.output_blocks", "U-Net Output Blocks"),
            ("first_stage_model", "VAE Encoder/Decoder"),
            ("cond_stage_model", "Conditioning Model")
        ]
        
        for i, (layer_key, layer_name) in enumerate(layer_options):
            var = tk.BooleanVar(value=True)
            self.layer_vars[layer_key] = var
            cb = ttk.Checkbutton(self.custom_layers_frame, text=layer_name, variable=var, state='disabled')
            cb.grid(row=i//2, column=i%2, sticky='w', padx=10, pady=2)
        
        # Update custom layer frame state based on training mode
        self.training_mode.trace('w', self._update_layer_selection_state)
        
        # Row 5: Start Button (moved down)
        self.btn_t = ttk.Button(control_frame, text="Start Training", command=self.start_training_thread, style='Accent.TButton', padding=10)
        self.btn_t.grid(row=5, column=0, columnspan=4, pady=15)

        # --- Plotting Frame ---
        plot_frame = ttk.LabelFrame(self, text="Loss Curve", padding=(15, 10))
        plot_frame.pack(fill='both', expand=True, padx=15, pady=10)

        plt.style.use('seaborn-v0_8-darkgrid')
        self.fig = Figure(figsize=(8, 5), dpi=100)
        self.ax = self.fig.add_subplot(111)
        
        self.canvas = FigureCanvasTkAgg(self.fig, master=plot_frame)
        self.canvas.get_tk_widget().pack(side=tk.TOP, fill=tk.BOTH, expand=True)
        
        self.train_loss_data = {'steps': [], 'losses': []}
        self.valid_loss_data = {'steps': [], 'losses': []}
        self.init_plot()

    def init_plot(self):
        self.ax.clear()
        self.ax.set_title("Training & Validation Loss", fontsize=16)
        self.ax.set_xlabel("Steps (Optimizer Updates)", fontsize=12)
        self.ax.set_ylabel("Loss", fontsize=12)
        self.train_line, = self.ax.plot([], [], 'o-', label='Train Loss', alpha=0.7, markersize=4)
        self.valid_line, = self.ax.plot([], [], 's-', label='Validation Loss', markersize=7, linewidth=2.5)
        self.ax.legend()
        self.ax.grid(True, which='both', linestyle='--', linewidth=0.5)
        self.fig.tight_layout()
        self.canvas.draw()
    
    def _setup_selective_training(self, model, config):
        """Setup selective training - prepares parameters for selective optimization."""
        training_mode = self.training_mode.get()
        
        # Keep all parameters with requires_grad=True for forward pass
        for param in model.parameters():
            param.requires_grad = True
        
        # Store selected parameters for later optimizer creation
        if training_mode == "full":
            self.selected_params = list(model.parameters())
            self.console.print("[green]Full model training enabled[/green]")
                
        elif training_mode == "global_encoder":
            # Only train Global Encoder parameters
            self.selected_params = []
            if hasattr(model.model, 'diffusion_model') and hasattr(model.model.diffusion_model, 'global_encoder'):
                self.selected_params = list(model.model.diffusion_model.global_encoder.parameters())
                self.console.print("[green]Global Encoder only training enabled[/green]")
            else:
                self.console.print("[yellow]Global Encoder not found, falling back to full training[/yellow]")
                self.selected_params = list(model.parameters())
                    
        elif training_mode == "unet_only":
            # Train U-Net but not Global Encoder
            self.selected_params = []
            for name, param in model.named_parameters():
                if 'global_encoder' not in name:
                    self.selected_params.append(param)
            self.console.print("[green]U-Net only training enabled (Global Encoder excluded)[/green]")
            
        elif training_mode == "custom":
            # Custom layer selection
            selected_layers = [key for key, var in self.layer_vars.items() if var.get()]
            self.selected_params = []
            for name, param in model.named_parameters():
                for layer_key in selected_layers:
                    if layer_key in name:
                        self.selected_params.append(param)
                        break
            self.console.print(f"[green]Custom training enabled for: {selected_layers}[/green]")
    
    def _get_trainable_parameters(self, model):
        """Get list of selected parameters for training."""
        if not hasattr(self, 'selected_params'):
            # Fallback to all parameters if setup wasn't called
            self.selected_params = list(model.parameters())
        
        trainable_params = self.selected_params
        
        # Log parameter breakdown for debugging
        total_params = sum(p.numel() for p in model.parameters())
        trainable_count = sum(p.numel() for p in trainable_params)
        excluded_count = total_params - trainable_count
        
        self.console.print(f"[dim]Parameter breakdown:[/dim]")
        self.console.print(f"[dim]  Total: {total_params:,}[/dim]")
        self.console.print(f"[dim]  Training: {trainable_count:,} ({100*trainable_count/total_params:.1f}%)[/dim]")
        self.console.print(f"[dim]  Excluded: {excluded_count:,} ({100*excluded_count/total_params:.1f}%)[/dim]")
        
        return trainable_params

    def update_plot(self, msg_type, step, loss_value):
        """Thread-safe method to update plot data."""
        if msg_type == 'train':
            self.train_loss_data['steps'].append(step)
            self.train_loss_data['losses'].append(float(loss_value))
            self.train_line.set_data(self.train_loss_data['steps'], self.train_loss_data['losses'])
        elif msg_type == 'valid':
            self.valid_loss_data['steps'].append(step)
            self.valid_loss_data['losses'].append(float(loss_value))
            self.valid_line.set_data(self.valid_loss_data['steps'], self.valid_loss_data['losses'])
        
        self.ax.relim()
        self.ax.autoscale_view()
        self.canvas.draw()

    def select_file(self):
        filepath = filedialog.askopenfilename(title="Select Pretrained Weights", filetypes=(("PyTorch Checkpoints", "*.pt *.ckpt"), ("All files", "*.*")))
        if filepath:
            self.pretrained_path_var.set(filepath)

    def start_training_thread(self):
        self.btn_t.config(state=tk.DISABLED, text="Training...")
        
        # Clear previous plot data
        self.train_loss_data = {'steps': [], 'losses': []}
        self.valid_loss_data = {'steps': [], 'losses': []}
        self.init_plot()
        
        # Start training in a new thread to keep the GUI responsive
        threading.Thread(target=self.run_training_logic, daemon=True).start()

    def on_training_finish(self, final_path):
        """Callback to run on the main thread after training finishes."""
        self.btn_t.config(state=tk.NORMAL, text="Start Training")
        self.console.print(f"[bold green]✅ Training task finished. Final model saved to: {final_path}[/bold green]")
        messagebox.showinfo("Training Complete", f"Training has finished!\nFinal model saved to:\n{final_path}")
    
    def _background_loader(self, dataset, buffer, initial_chunk_event):
        """Helper function to load data in a background thread."""
        num_files = len(dataset.files)
        initial_chunk_size = math.ceil(num_files * 0.1)
        
        for i in range(num_files):
            item = dataset._load_item(i)
            if item:
                buffer.append(item)
            if i == initial_chunk_size - 1:
                initial_chunk_event.set() # Signal that the first 10% is ready
        
        if not initial_chunk_event.is_set():
            initial_chunk_event.set()

    def run_training_logic(self):
        """The main training loop, executed in a separate thread."""
        try:
            # --- 1. Update and save config ---
            with open(CFG_PATH, 'r', encoding='utf-8') as f:
                current_cfg = yaml.safe_load(f)
            
            current_cfg['train']['pretrained_path'] = self.pretrained_path_var.get() or None
            current_cfg['train']['gradient_accumulation_steps'] = int(self.grad_accum_var.get())
            current_cfg['train']['fastboot'] = self.fastboot_var.get()
            current_cfg['train']['use_jit_compile'] = self.jit_var.get()
            current_cfg['train']['use_gradient_checkpointing'] = self.grad_checkpoint_var.get()
            
            # Advanced loss monitoring settings
            current_cfg['train']['loss_explosion_protection'] = self.loss_explosion_var.get()
            current_cfg['train']['explosion_threshold'] = float(self.explosion_threshold_var.get())
            current_cfg['train']['adaptive_lr'] = self.adaptive_lr_var.get()
            current_cfg['train']['lr_reduction_factor'] = float(self.lr_reduction_var.get())
            current_cfg['train']['early_stopping'] = self.early_stopping_var.get()
            current_cfg['train']['patience'] = int(self.patience_var.get())
            
            # Remove fp16 from config if it exists
            current_cfg['train'].pop('use_fp16', None)

            with open(CFG_PATH, 'w', encoding='utf-8') as f:
                yaml.dump(current_cfg, f, sort_keys=False, allow_unicode=True)
            
            self.console.rule("[bold cyan]🚀 Starting new training task[/bold cyan]")
            
            # --- 2. Setup ---
            set_seed(current_cfg['experiment']['seed'])
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            accumulation_steps = current_cfg['train']['gradient_accumulation_steps']
            use_grad_checkpoint = current_cfg['train']['use_gradient_checkpointing']
            
            self.console.print(f"Using device: [bold cyan]{device}[/]")
            self.console.print(f"Gradient Accumulation: [bold cyan]{accumulation_steps} steps[/]")
            self.console.print(f"Fastboot: [bold {'green' if self.fastboot_var.get() else 'red'}]{self.fastboot_var.get()}[/]")
            self.console.print(f"JIT Compilation: [bold {'green' if self.jit_var.get() else 'red'}]{self.jit_var.get()}[/]")
            self.console.print(f"Gradient Checkpointing: [bold {'green' if use_grad_checkpoint else 'red'}]{use_grad_checkpoint}[/]")

            # --- 3. DataLoaders ---
            train_set_full = PairedMelDataset(split='train', **current_cfg['data'], preload_to_ram=False)
            valid_set = PairedMelDataset(split='valid', **current_cfg['data'], preload_to_ram=True)
            
            train_buffer = []
            loader_thread = None
            
            if self.fastboot_var.get():
                initial_chunk_ready = threading.Event()
                loader_thread = threading.Thread(target=self._background_loader, args=(train_set_full, train_buffer, initial_chunk_ready), daemon=True)
                loader_thread.start()
                self.console.print("Fastboot enabled: Waiting for initial 10% of data...")
                
                # Wait with timeout to prevent freezing
                if initial_chunk_ready.wait(timeout=60.0):
                    self.console.print(f"[green]Initial chunk loaded ({len(train_buffer)} items). Starting training...[/green]")
                else:
                    self.console.print("[yellow]Initial loading timeout, loading synchronously instead...[/yellow]")
                    # Fallback to synchronous loading
                    for i in range(min(len(train_set_full), 100)):  # Load first 100 items as fallback
                        item = train_set_full._load_item(i)
                        if item: train_buffer.append(item)
                    loader_thread = None  # Don't try to join later
                    
                train_loader = DataLoader(train_buffer, batch_size=current_cfg['train']['batch_size'], shuffle=True)
            else:
                self.console.print("Loading full training data synchronously...")
                for i in range(len(train_set_full)):
                    item = train_set_full._load_item(i)
                    if item: train_buffer.append(item)
                self.console.print("[green]Full dataset loaded.[/green]")
                train_loader = DataLoader(train_buffer, batch_size=current_cfg['train']['batch_size'], shuffle=True)

            valid_loader = DataLoader(valid_set, batch_size=current_cfg['train']['batch_size'], shuffle=False)

            # --- 4. Model, Optimizer, Scheduler, EMA ---
            model_config = current_cfg['model'].copy()
            
            # Disable gradient checkpointing for selective training to avoid grad issues
            if self.training_mode.get() != "full" and use_grad_checkpoint:
                if 'unet_config' in model_config['params'] and 'params' in model_config['params']['unet_config']:
                    model_config['params']['unet_config']['params']['use_checkpoint'] = False
                self.console.print("[yellow]Disabled gradient checkpointing for selective training[/yellow]")
            elif use_grad_checkpoint:
                if 'unet_config' in model_config['params'] and 'params' in model_config['params']['unet_config']:
                    model_config['params']['unet_config']['params']['use_checkpoint'] = True
            
            model = instantiate_from_config(model_config).to(device)
            
            # --- Apply selective training based on user selection ---
            self._setup_selective_training(model, current_cfg)
            
            # Get trainable parameters BEFORE creating optimizer
            trainable_params = self._get_trainable_parameters(model)
            if not trainable_params:
                raise RuntimeError("No trainable parameters found! Check your layer selection.")
            
            epsilon = 1e-8
            model.log_one_minus_alphas_cumprod.clamp_max_(math.log(1.0 - epsilon))
            model.sqrt_recipm1_alphas_cumprod.clamp_max_(math.sqrt(1.0 / epsilon - 1.0))

            if current_cfg['train'].get('pretrained_path'):
                load_pretrained(model, current_cfg['train']['pretrained_path'], device, self.console)

            if self.jit_var.get():
                if not use_grad_checkpoint:
                    self.console.print("[cyan]Compiling model with Torch JIT...[/]")
                    model = torch.jit.script(model)
                    self.console.print("[green]Model compiled.[/green]")
                else:
                    self.console.print("[yellow]Skipping JIT: Incompatible with Gradient Checkpointing.[/yellow]")

            self.console.print(f"[cyan]Training mode: {self.training_mode.get()}[/]")
            self.console.print(f"[cyan]Trainable parameters: {sum(p.numel() for p in trainable_params):,}[/]")
            
            opt = torch.optim.AdamW(trainable_params, lr=current_cfg['model']['params']['base_learning_rate'], betas=tuple(current_cfg['train']['betas']), weight_decay=current_cfg['train']['weight_decay'])
            
            num_batches = len(train_set_full)
            max_steps = (current_cfg['train']['epochs'] * num_batches) // accumulation_steps
            
            sched = WarmupCosine(opt, current_cfg['model']['params']['base_learning_rate'], warmup=current_cfg['train']['warmup_steps'], max_steps=max_steps)
            ema = EMA(model, decay=current_cfg['train']['ema_decay'])
            
            # Initialize advanced loss monitor
            loss_monitor = None
            if current_cfg['train'].get('loss_explosion_protection', False) or current_cfg['train'].get('adaptive_lr', False):
                loss_monitor = AdvancedLossMonitor(
                    explosion_threshold=current_cfg['train'].get('explosion_threshold', 10.0),
                    lr_reduction_factor=current_cfg['train'].get('lr_reduction_factor', 0.5),
                    patience=current_cfg['train'].get('patience', 1000) if current_cfg['train'].get('early_stopping', False) else float('inf')
                )
                self.console.print(f"[cyan]Advanced Loss Monitor enabled:[/cyan]")
                self.console.print(f"[dim]  Explosion threshold: {loss_monitor.explosion_threshold}[/dim]")
                self.console.print(f"[dim]  LR reduction factor: {loss_monitor.lr_reduction_factor}[/dim]")
                if current_cfg['train'].get('early_stopping', False):
                    self.console.print(f"[dim]  Early stopping patience: {loss_monitor.patience} steps[/dim]")

            # --- 5. Training Loop ---
            best_val_loss = float('inf')
            gstep = 0 
            outp = os.path.join(current_cfg['experiment']['out_dir'], 'checkpoints')
            ensure_dir(outp)

            # Early stopping flag
            early_stop_triggered = False

            model.train()
            
            # --- FIX: Initialize display variables properly ---
            current_loss_value = 0.0
            current_lr_value = current_cfg['model']['params']['base_learning_rate']
            
            with Progress(
                SpinnerColumn(), TextColumn("[progress.description]{task.description}"), BarColumn(),
                MofNCompleteColumn(), TextColumn("•"), TimeElapsedColumn(),
                TextColumn("• {task.fields[loss]}"),
                TextColumn("• {task.fields[lr]}"),
                console=self.console,
            ) as progress:
                
                main_task = progress.add_task("Overall Progress", total=current_cfg['train']['epochs'], loss="", lr="")
                current_epoch_task = None  # Track current epoch task
                epoch_tasks_to_cleanup = []  # Track old tasks for cleanup

                for ep in range(current_cfg['train']['epochs']):
                    if gstep >= max_steps or early_stop_triggered: 
                        break
                    
                    # Check for early stopping at epoch level
                    if loss_monitor is not None:
                        stats = loss_monitor.get_stats()
                        if stats['steps_without_improvement'] >= loss_monitor.patience and current_cfg['train'].get('early_stopping', False):
                            self.console.print(f"[red]Early stopping: training terminated at epoch {ep+1}[/red]")
                            early_stop_triggered = True
                            break
                    
                    # Handle fastboot transition - load full dataset after first epoch completes
                    if self.fastboot_var.get() and ep == 1 and loader_thread is not None:
                        self.console.print("[cyan]Waiting for full dataset to load...[/cyan]")
                        try:
                            # Wait for background loading to complete
                            if loader_thread.is_alive():
                                loader_thread.join(timeout=60.0)  # Increased timeout
                                if loader_thread.is_alive():
                                    self.console.print("[yellow]Dataset loading timeout, continuing with current data[/yellow]")
                                    loader_thread = None  # Stop trying to use it
                                else:
                                    self.console.print(f"[green]Full dataset loaded ({len(train_buffer)} items). Re-initializing DataLoader.[/green]")
                                    train_loader = DataLoader(train_buffer, batch_size=current_cfg['train']['batch_size'], shuffle=True)
                            else:
                                self.console.print("[green]Background loading already completed.[/green]")
                                train_loader = DataLoader(train_buffer, batch_size=current_cfg['train']['batch_size'], shuffle=True)
                        except Exception as e:
                            self.console.print(f"[yellow]Dataset loading error: {e}, continuing with current data[/yellow]")
                            loader_thread = None

                    # Mark previous epoch task for cleanup (don't remove during training)
                    if current_epoch_task is not None:
                        epoch_tasks_to_cleanup.append(current_epoch_task)
                        # Only keep last 2 epoch tasks visible to prevent clutter
                        if len(epoch_tasks_to_cleanup) > 2:
                            old_task = epoch_tasks_to_cleanup.pop(0)
                            try:
                                if (hasattr(progress, 'tasks') and old_task in progress.tasks):
                                    progress.remove_task(old_task)
                            except (KeyError, IndexError, ValueError):
                                pass
                    
                    # Create display strings for this epoch
                    loss_display = f"Loss: {current_loss_value:.4f}" if current_loss_value > 0 else "Loss: Starting..."
                    lr_display = f"LR: {current_lr_value:.2e}"
                    
                    # Use a stable batch count to avoid confusion
                    stable_batch_count = len(train_loader)
                    current_epoch_task = progress.add_task(f"[cyan]Epoch {ep+1}", total=stable_batch_count, loss=loss_display, lr=lr_display)
                    opt.zero_grad()
                    
                    self.console.print(f"[green]📈 Starting epoch {ep+1}/{current_cfg['train']['epochs']} with {stable_batch_count} batches[/green]")
                    
                    for batch_idx, batch in enumerate(train_loader):
                        # Debug info every 50 batches to track progress
                        if batch_idx % 50 == 0:
                            self.console.print(f"[dim]Epoch {ep+1}, Batch {batch_idx+1}/{len(train_loader)}[/dim]")
                        
                        batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}

                        loss, _ = model(batch)
                        
                        # Store raw loss for accumulation
                        raw_loss = loss.item()
                        
                        # Advanced loss monitoring
                        skip_this_step = False
                        if loss_monitor is not None:
                            actions = loss_monitor.update(raw_loss, gstep)
                            
                            if actions['message']:
                                self.console.print(f"[yellow]{actions['message']}[/yellow]")
                            
                            if actions['skip_step']:
                                skip_this_step = True
                                self.console.print(f"[red]Skipping step {gstep} due to NaN/Inf loss[/red]")
                            
                            if actions['reduce_lr'] and current_cfg['train'].get('adaptive_lr', False):
                                new_lr = loss_monitor.reduce_learning_rate(opt, self.console)
                                current_lr_value = new_lr
                            
                            if actions['stop_training'] and current_cfg['train'].get('early_stopping', False):
                                self.console.print(f"[red]Early stopping triggered at step {gstep}[/red]")
                                early_stop_triggered = True
                                break  # Break out of batch loop
                        
                        if skip_this_step:
                            continue  # Skip this batch
                        
                        loss = loss / accumulation_steps
                        
                        # Only backward if loss requires grad
                        if loss.requires_grad:
                            loss.backward()
                        else:
                            raise RuntimeError(f"Loss tensor does not require grad. Check model parameters.")

                        if (batch_idx + 1) % accumulation_steps == 0:
                            # Only clip gradients for parameters that require grad
                            trainable_params_for_clipping = [p for p in model.parameters() if p.requires_grad and p.grad is not None]
                            if trainable_params_for_clipping:
                                torch.nn.utils.clip_grad_norm_(trainable_params_for_clipping, 1.0)
                            
                            opt.step()
                            
                            sched.step()
                            ema.update(model)
                            opt.zero_grad()
                            gstep += 1
                            
                            # Update current values for display
                            current_loss_value = raw_loss
                            current_lr_value = opt.param_groups[0]['lr']
                            
                            # Create display strings for progress
                            loss_display = f"Loss: {current_loss_value:.4f}"
                            lr_display = f"LR: {current_lr_value:.2e}"
                            
                            # Always update plot and print loss for immediate feedback
                            if gstep % current_cfg['train']['log_interval'] == 0:
                                self.console.print(f"[green]Step {gstep}: Loss = {current_loss_value:.4f}, LR = {current_lr_value:.2e}[/green]")
                                self.after(0, self.update_plot, 'train', gstep, current_loss_value)
                                
                                # Show loss monitoring stats periodically
                                if loss_monitor is not None and gstep % (current_cfg['train']['log_interval'] * 4) == 0:
                                    stats = loss_monitor.get_stats()
                                    self.console.print(f"[dim]📊 Monitor Stats - Best: {stats['best_loss']:.4f}, "
                                                     f"No improvement: {stats['steps_without_improvement']}, "
                                                     f"LR reductions: {stats['lr_reductions']}, "
                                                     f"Short MA: {stats['short_ma']:.4f}, Long MA: {stats['long_ma']:.4f}[/dim]")

                            if gstep % current_cfg['train']['valid_interval_steps'] == 0:
                                model.eval()
                                val_losses = []
                                for val_batch in valid_loader:
                                    val_batch = {k: v.to(device) for k, v in val_batch.items() if isinstance(v, torch.Tensor)}
                                    with torch.no_grad():
                                        val_loss, _ = model(val_batch)
                                    val_losses.append(val_loss.item())
                                
                                avg_val_loss = sum(val_losses) / len(val_losses)
                                progress.console.print(f"📊 Validation @ step {gstep}: loss = {avg_val_loss:.4f}")
                                self.after(0, self.update_plot, 'valid', gstep, avg_val_loss)
                                model.train()

                                if avg_val_loss < best_val_loss:
                                    best_val_loss = avg_val_loss
                                    path = os.path.join(outp, f'best_step_{gstep}.pt')
                                    torch.save({'state_dict': model.state_dict(), 'ema': ema.shadow}, path)
                                    progress.console.print(f"💾 [bold magenta]Saved best model to {path}[/bold magenta]")

                            if gstep % current_cfg['train']['save_interval_steps'] == 0:
                                path = os.path.join(outp, f'step_{gstep}.pt')
                                torch.save({'state_dict': model.state_dict(), 'ema': ema.shadow}, path)
                                progress.console.print(f"💾 [cyan]Saved checkpoint to {path}[/cyan]")
                        
                        # Update progress display with current values
                        current_batch_loss = f"Loss: {raw_loss:.4f}" if 'raw_loss' in locals() else loss_display
                        current_lr_str = f"LR: {opt.param_groups[0]['lr']:.2e}" if len(opt.param_groups) > 0 else lr_display
                        
                        # Update progress with robust error handling
                        try:
                            # Check if task exists before accessing
                            if (current_epoch_task is not None and 
                                hasattr(progress, 'tasks') and 
                                current_epoch_task in progress.tasks):
                                
                                task = progress.tasks[current_epoch_task]
                                current_progress = task.completed
                                total_progress = task.total or len(train_loader)
                                
                                if current_progress < total_progress:
                                    progress.update(current_epoch_task, advance=1, fields={"loss": current_batch_loss, "lr": current_lr_str})
                                else:
                                    # Don't advance, just update fields if possible
                                    progress.update(current_epoch_task, fields={"loss": current_batch_loss, "lr": current_lr_str})
                        except (KeyError, IndexError, AttributeError):
                            # Task was removed or doesn't exist, skip silently
                            pass
                        except Exception as e:
                            # Log other errors for debugging, but don't spam
                            if batch_idx % 100 == 0:  # Only log every 100 batches
                                self.console.print(f"[dim]Progress update error (batch {batch_idx}): {e}[/dim]")
                    
                    # Check if early stopping was triggered during batch processing
                    if early_stop_triggered:
                        break
                    
                    # Epoch completed, will be cleaned up at start of next epoch
                    progress.update(main_task, advance=1)

                # Clean up all epoch tasks when training completes
                for task_id in epoch_tasks_to_cleanup:
                    try:
                        if (hasattr(progress, 'tasks') and task_id in progress.tasks):
                            progress.remove_task(task_id)
                    except (KeyError, IndexError, ValueError):
                        pass
                        
                if (current_epoch_task is not None and 
                    hasattr(progress, 'tasks') and 
                    current_epoch_task in progress.tasks):
                    try:
                        progress.remove_task(current_epoch_task)
                    except (KeyError, IndexError, ValueError):
                        pass

            final_p = os.path.join(outp, 'final_ema.pt')
            torch.save({'state_dict': model.state_dict(), 'ema': ema.shadow}, final_p)
            self.after(0, self.on_training_finish, final_p)

        except Exception as e:
            error_msg = str(e)
            self.console.print_exception()
            self.after(0, lambda: self.btn_t.config(state=tk.NORMAL, text="Start Training"))
            self.after(0, lambda msg=error_msg: messagebox.showerror("Training Error", f"Training failed: {msg}"))

if __name__ == '__main__':
    try:
        with open(CFG_PATH, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        ensure_project_dirs(config)
        app = TrainingApp(config)
        app.mainloop()
    except FileNotFoundError:
        print(f"错误: 配置文件 'config.yaml' 未找到。请确保它在脚本所在的目录中。")
        sys.exit(1)
    except Exception as e:
        print(f"启动 GUI 时发生严重错误: {e}")
        sys.exit(1)

```

## 文件: `inspector.py`

```python
import torch
import argparse
from typing import Dict, Any
import tkinter as tk
from tkinter import ttk, filedialog, scrolledtext
import threading

def inspect_checkpoint(ckpt_path: str, log_widget: scrolledtext.ScrolledText):
    """
    加载一个 PyTorch 检查点文件 (.pt, .ckpt) 并将其内部结构
    打印到指定的 Tkinter ScrolledText 组件中。
    """
    def log(message):
        log_widget.insert(tk.END, message + "\n")
        log_widget.see(tk.END)

    log(f"--- 正在检查权重文件: {ckpt_path} ---")

    try:
        # 加载到 CPU 以避免占用显存
        ckpt = torch.load(ckpt_path, map_location="cpu")
    except Exception as e:
        log(f"\n[错误] 无法加载文件: {e}")
        return

    if not isinstance(ckpt, dict):
        log("\n[错误] 权重文件不是一个字典 (dictionary)。")
        return

    log("\n--- 文件顶层键 (Top-Level Keys) ---")
    for key in ckpt.keys():
        if isinstance(ckpt[key], dict):
            log(f"- {key} (包含 {len(ckpt[key])} 个子键)")
        else:
            log(f"- {key} (类型: {type(ckpt[key])})")

    def print_state_dict(prefix: str, sd: Dict[str, Any]):
        log(f"\n--- 权重字典详情: '{prefix}' ---")
        if not sd:
            log("  (此字典为空)")
            return
        
        max_key_len = max(len(k) for k in sd.keys()) if sd else 0
        
        for name, param in sd.items():
            if isinstance(param, torch.Tensor):
                shape_str = str(list(param.shape))
                log(f"  {name:<{max_key_len}} | 形状: {shape_str}")
            else:
                log(f"  {name:<{max_key_len}} | (非张量, 类型: {type(param)})")

    # 递归地查找并打印所有 state_dict
    def find_and_print_sds(data, prefix=""):
        if isinstance(data, dict):
            is_state_dict = all(isinstance(v, torch.Tensor) for v in data.values()) and data
            
            if is_state_dict:
                print_state_dict(prefix if prefix else "root", data)
            else:
                for key, value in data.items():
                    new_prefix = f"{prefix}.{key}" if prefix else key
                    find_and_print_sds(value, new_prefix)

    find_and_print_sds(ckpt)
    log("\n--- 检查完毕 ---")


class CheckpointInspectorGUI(tk.Tk):
    def __init__(self):
        super().__init__()
        self.title("PyTorch Checkpoint Inspector")
        self.geometry("900x700")

        # --- UI Elements ---
        main_frame = ttk.Frame(self, padding="10")
        main_frame.pack(fill="both", expand=True)

        # File selection
        file_frame = ttk.LabelFrame(main_frame, text="选择权重文件", padding="10")
        file_frame.pack(fill="x", pady=5)

        self.path_var = tk.StringVar()
        self.path_entry = ttk.Entry(file_frame, textvariable=self.path_var, width=80)
        self.path_entry.pack(side="left", fill="x", expand=True, padx=(0, 5))

        self.browse_button = ttk.Button(file_frame, text="浏览...", command=self.browse_file)
        self.browse_button.pack(side="left")

        # Inspect button
        self.inspect_button = ttk.Button(main_frame, text="开始检查", command=self.start_inspection)
        self.inspect_button.pack(pady=10)

        # Log display
        log_frame = ttk.LabelFrame(main_frame, text="检查结果", padding="10")
        log_frame.pack(fill="both", expand=True)

        self.log_widget = scrolledtext.ScrolledText(log_frame, wrap=tk.WORD, height=20)
        self.log_widget.pack(fill="both", expand=True)

    def browse_file(self):
        filepath = filedialog.askopenfilename(
            title="选择权重文件",
            filetypes=(("PyTorch Checkpoints", "*.pt *.ckpt"), ("All files", "*.*"))
        )
        if filepath:
            self.path_var.set(filepath)

    def start_inspection(self):
        ckpt_path = self.path_var.get()
        if not ckpt_path:
            self.log_widget.delete('1.0', tk.END)
            self.log_widget.insert(tk.END, "[错误] 请先选择一个文件路径。")
            return

        self.log_widget.delete('1.0', tk.END)
        self.inspect_button.config(state="disabled")
        
        # Run inspection in a separate thread to avoid freezing the GUI
        thread = threading.Thread(target=self._run_inspection_task, args=(ckpt_path,), daemon=True)
        thread.start()

    def _run_inspection_task(self, ckpt_path):
        try:
            inspect_checkpoint(ckpt_path, self.log_widget)
        except Exception as e:
            self.log_widget.insert(tk.END, f"\n[致命错误] 发生意外: {repr(e)}")
        finally:
            self.inspect_button.config(state="normal")


if __name__ == "__main__":
    app = CheckpointInspectorGUI()
    app.mainloop()

```

## 文件: `convert_trained_model.py`

```python
#!/usr/bin/env python3
"""
Convert your trained model to match the new CLAP structure
"""

import torch
import os

def convert_clap_structure(ckpt_path, output_path):
    """Convert direct CLAP structure to sequential structure"""
    print(f"🔄 Converting {ckpt_path}")
    
    # Load checkpoint
    ckpt = torch.load(ckpt_path, map_location='cpu')
    
    # Convert both state_dict and ema if they exist
    for key_name in ['state_dict', 'ema']:
        if key_name in ckpt:
            weights = ckpt[key_name]
            converted_weights = {}
            
            for weight_key, weight_value in weights.items():
                new_key = weight_key
                
                # Convert CLAP transform keys
                if 'clap.model.text_transform.' in weight_key and '.sequential.' not in weight_key:
                    # clap.model.text_transform.0.weight -> clap.model.text_transform.sequential.0.weight
                    new_key = weight_key.replace('clap.model.text_transform.', 'clap.model.text_transform.sequential.')
                    print(f"  Converting: {weight_key} -> {new_key}")
                    
                elif 'clap.model.audio_transform.' in weight_key and '.sequential.' not in weight_key:
                    # clap.model.audio_transform.0.weight -> clap.model.audio_transform.sequential.0.weight  
                    new_key = weight_key.replace('clap.model.audio_transform.', 'clap.model.audio_transform.sequential.')
                    print(f"  Converting: {weight_key} -> {new_key}")
                
                converted_weights[new_key] = weight_value
            
            ckpt[key_name] = converted_weights
            print(f"✅ Converted {len(weights)} keys in {key_name}")
    
    # Save converted checkpoint
    torch.save(ckpt, output_path)
    print(f"💾 Saved converted model to: {output_path}")

def main():
    # Convert your trained model
    original_path = "/home/husrcf/Code/Python/ProjectLily_Z_III/outputs/audiosr_ldm_train/checkpoints/step_30000.pt"
    converted_path = "/home/husrcf/Code/Python/ProjectLily_Z_III/outputs/audiosr_ldm_train/checkpoints/step_30000_converted.pt"
    
    if os.path.exists(original_path):
        convert_clap_structure(original_path, converted_path)
        
        # Verify the conversion
        print("\n🔍 Verifying conversion...")
        ckpt = torch.load(converted_path, map_location='cpu')
        
        if 'state_dict' in ckpt:
            clap_keys = [k for k in ckpt['state_dict'].keys() if 'clap.model' in k and 'transform' in k]
        else:
            clap_keys = [k for k in ckpt['ema'].keys() if 'clap.model' in k and 'transform' in k]
            
        print("✅ CLAP keys after conversion:")
        for key in clap_keys:
            print(f"  {key}")
            
        print(f"\n🎉 Conversion complete! Use {converted_path} with your updated model.")
    else:
        print(f"❌ Original model not found: {original_path}")

if __name__ == "__main__":
    main()
```

## 文件: `config.yaml`

```yaml
data:
  blank_hop_seconds: 1.0
  blank_ratio_max: 0.3
  blank_thr: 0.0001
  categories:
  - train
  - valid
  dataset_root: data
  fmax: 24000.0
  fmin: 20.0
  high_dir_name: high
  hop_length: 480
  low_dir_name: low
  n_fft: 2048
  n_mels: 256
  sample_rate: 48000
  segment_seconds: 10.24
  split_seed: 1337
  valid_ratio: 0.05
  win_length: 2048
experiment:
  out_dir: outputs/audiosr_ldm_train
  seed: 1337
inference:
  chunk_hop_seconds: 3.0
  chunk_seconds: 4.0
  dynamic_clip_percentile: 0.99
  guidance_scale: 1.5
  sample_steps: 50
  sampler: ddim
model:
  params:
    base_learning_rate: 0.0001
    beta_schedule: cosine
    cond_stage_config:
      concat_lowpass_cond:
        cond_stage_key: lowpass_mel
        conditioning_key: concat
        params:
          first_stage_config:
            params:
              ddconfig:
                attn_resolutions: []
                ch: 128
                ch_mult:
                - 1
                - 2
                - 4
                - 8
                double_z: true
                dropout: 0.1
                in_channels: 1
                mel_bins: 256
                num_res_blocks: 2
                out_ch: 1
                resolution: 256
                z_channels: 16
              embed_dim: 16
            target: model.AudioSRAutoEncoderKL
        target: model.VAEFeatureExtract
    first_stage_config:
      params:
        ddconfig:
          attn_resolutions: []
          ch: 128
          ch_mult:
          - 1
          - 2
          - 4
          - 8
          double_z: true
          dropout: 0.1
          in_channels: 1
          mel_bins: 256
          num_res_blocks: 2
          out_ch: 1
          resolution: 256
          z_channels: 16
        embed_dim: 16
      target: model.AudioSRAutoEncoderKL
    loss_type: l2
    parameterization: v
    scale_by_std: true
    scale_factor: 1.0
    timesteps: 1000
    unet_config:
      params:
        attention_resolutions:
        - 8
        - 4
        - 2
        channel_mult:
        - 1
        - 2
        - 3
        - 5
        context_dim: null
        image_size: 64
        in_channels: 32
        model_channels: 128
        num_head_channels: 32
        num_res_blocks: 2
        out_channels: 16
        transformer_depth: 1
        use_spatial_transformer: true
        use_global_encoder: true
        global_context_dim: 64
      target: model.UNetModel
  target: model.LatentDiffusion
train:
  batch_size: 2
  betas:
  - 0.9
  - 0.99
  ema_decay: 0.999
  epochs: 500
  fastboot: true
  freeze_substrings: []
  gradient_accumulation_steps: 8
  key_filter_contains: model.diffusion_model.
  log_interval: 50
  lr: 0.0001
  min_lr_ratio: 0.1
  num_workers: 12
  preload_data_to_ram: true
  pretrained_path: /home/husrcf/Code/Python/ProjectLily_Z_III/outputs/audiosr_ldm_train/checkpoints/step_14000.pt
  save_interval_steps: 2000
  unfreeze_if_no_pretrained: true
  use_gradient_checkpointing: true
  use_jit_compile: true
  valid_interval_steps: 1000
  warmup_steps: 2000
  weight_decay: 0.0001
  loss_explosion_protection: true
  explosion_threshold: 10.0
  adaptive_lr: true
  lr_reduction_factor: 0.5
  early_stopping: false
  patience: 1000

```

## 文件: `package.py`

```python
#!/usr/bin/env python3
import os
import re
import sys
import fnmatch

# 移除 from pathlib import Path，后续用 os 替代
# from pathlib import Path

# --- 配置区 ---
# 目标目录，使用 os 获取当前脚本所在目录
SOURCE_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "")

# 输出的Markdown文件
OUTPUT_FILE = "codebase_prompt.md"

# ==============================================================================
# 新增：仅包含（白名单）模式 - 用于筛选【文件内容】
# ==============================================================================
INCLUDE_ONLY_PATTERNS = [
    "*.py", "*.yaml"
]

# ==============================================================================
# 排除（黑名单）模式
# ==============================================================================
# 要排除的目录。此规则对【项目结构图】和【文件内容】两部分都生效。
EXCLUDE_DIRS = [
    "*/node_modules/*", "*/.git/*", "*/dist/*", "*/build/*", "*/.vscode/*", "*/.idea/*",
    "*/__pycache__/*", "*/venv/*", "*/.nuxt/*", "*/diaries/*", "*/runs/*", "*/output*/*"
]

# 要排除的文件类型或文件。
EXCLUDE_FILES = [
    "*.log", "*.tmp", "*.lock", "*.map", "package-lock.json", "yarn.lock", "pnpm-lock.yaml",
    "*.DS_Store", "*.sqlite3", "*.db", "*.png", "*.ico", "*.jpg", "*.jpeg", "*.gif", "*.svg",
    "*.woff", "*.woff2", "*.ttf", "*.eot", "*.pth", "*.npy", "tokenizer.json", "alphagenome_pytorch"
]

# --- END 配置区 ---

def main():
    # 检查 SOURCE_DIR 是否存在
    if not os.path.isdir(SOURCE_DIR):
        print(f"错误：源目录 '{SOURCE_DIR}' 不存在。")
        sys.exit(1)

    # 清空或创建输出文件
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        f.write('')

    # --- 1. 生成项目结构图 ---
    generate_project_structure()

    # --- 2. 拼接代码内容 ---
    generate_code_content()

    print(f"Done! 代码已拼接至 '{OUTPUT_FILE}'")

def generate_project_structure():
    """生成项目结构的Markdown文档"""
    # 预处理要排除的目录名
    exclude_names_pattern = []
    for dir_pattern in EXCLUDE_DIRS:
        dir_name_part = re.sub(r'/\*$', '', dir_pattern)
        dir_name_part = os.path.basename(dir_name_part)
        if dir_name_part and dir_name_part != '*' and dir_name_part != '.':
            exclude_names_pattern.append(dir_name_part)
    
    exclude_names_regex = '|'.join(exclude_names_pattern) if exclude_names_pattern else ''

    with open(OUTPUT_FILE, 'a', encoding='utf-8') as f:
        f.write("# 项目结构\n\n")
        f.write(f"项目`{SOURCE_DIR}`的目录结构（已排除如`node_modules`等目录）：\n")
        f.write("```\n")
        f.write(f"{SOURCE_DIR}/\n")
        
        # 生成目录树
        generate_tree(SOURCE_DIR, '', exclude_names_regex, f)
        
        f.write("```\n\n")
        f.write("---\n\n")

def generate_tree(dir_path, prefix, exclude_regex, file_obj):
    """递归生成目录树"""
    # 获取当前目录下一级的所有文件和目录，并排序
    items = sorted(os.listdir(dir_path))
    
    for i, item in enumerate(items):
        # 检查是否需要排除这个文件或目录
        if exclude_regex and re.search(exclude_regex, item):
            continue
            
        item_path = os.path.join(dir_path, item)
        
        # 判断连接符
        connector = "├── "
        new_prefix = "│   "
        if i == len(items) - 1:
            connector = "└── "
            new_prefix = "    "
        
        # 判断是目录还是文件，并输出
        if os.path.isdir(item_path):
            file_obj.write(f"{prefix}{connector}{item}/\n")
            # 递归调用
            generate_tree(item_path, prefix + new_prefix, exclude_regex, file_obj)
        else:
            file_obj.write(f"{prefix}{connector}{item}\n")

def generate_code_content():
    """生成代码内容的Markdown文档"""
    with open(OUTPUT_FILE, 'a', encoding='utf-8') as f:
        f.write("# 代码内容\n\n")
        
        # 构建文件搜索条件
        if INCLUDE_ONLY_PATTERNS:
            print(f"模式：仅拼接匹配 {' '.join(INCLUDE_ONLY_PATTERNS)} 的文件内容")
            include_patterns = INCLUDE_ONLY_PATTERNS
        else:
            print("模式：拼接所有文件，除了黑名单中的文件和目录")
            include_patterns = None
        
        # 遍历所有文件
        for root, dirs, files in os.walk(SOURCE_DIR):
            # 排除目录
            dirs[:] = [d for d in dirs if not should_exclude(os.path.join(root, d), EXCLUDE_DIRS)]
            
            for file in files:
                file_path = os.path.join(root, file)
                
                # 排除文件
                if should_exclude(file_path, EXCLUDE_DIRS + [f"*/{pattern}" for pattern in EXCLUDE_FILES]):
                    continue
                
                # 检查是否在包含列表中
                if include_patterns and not any(fnmatch.fnmatch(file_path, pattern) for pattern in include_patterns):
                    continue
                
                # 写入文件内容
                relative_path = os.path.relpath(file_path, SOURCE_DIR)
                f.write(f"## 文件: `{relative_path}`\n\n")
                
                # 获取文件扩展名作为代码块语言
                file_ext = os.path.splitext(file_path)[1].lstrip('.')
                f.write(f"```{'python' if file_ext == 'py' else file_ext}\n")
                
                try:
                    with open(file_path, 'r', encoding='utf-8') as code_file:
                        f.write(code_file.read())
                except Exception as e:
                    f.write(f"无法读取文件: {file_path} ({str(e)})\n")
                
                f.write("\n```\n\n")

def should_exclude(path, patterns):
    """检查路径是否应被排除"""
    path_str = str(path)
    return any(re.search(pattern.replace('*', '.*'), path_str) for pattern in patterns)

if __name__ == "__main__":
    main()
```

## 文件: `inference.py`

```python
import os
import sys
import yaml
import threading
import numpy as np
import torch
import torchaudio
import soundfile as sf
from scipy.signal import butter, lfilter
import traceback # Import the standard traceback module

# Set offline mode to avoid network issues with transformers
os.environ['TRANSFORMERS_OFFLINE'] = '1'
os.environ['HF_HUB_OFFLINE'] = '1' 
os.environ['HF_DATASETS_OFFLINE'] = '1'

# --- GUI Imports ---
import tkinter as tk
from tkinter import ttk, filedialog, messagebox

# --- Plotting Imports ---
import matplotlib.pyplot as plt
from matplotlib.figure import Figure
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg

# --- Local Project Imports ---
# Make sure model.py and audiosr folder are in the same directory
from model import LatentDiffusion
from audiosr.latent_diffusion.util import instantiate_from_config
from audiosr.utils import _locate_cutoff_freq
from audiosr.lowpass import lowpass
# --- HiFi-GAN Imports ---
# Note: The Vocoder class is already defined in your model.py inside AutoencoderKL,
# so we don't need to import it separately if the weights are in the main checkpoint.

# =====================================================================================
# SECTION: CORE INFERENCE AND AUDIO PROCESSING LOGIC
# =====================================================================================

class InferenceEngine:
    """
    Handles the core logic for audio processing, model inference, and file I/O.
    """
    def __init__(self, cfg, device='cuda'):
        self.cfg = cfg
        self.device = device
        self.model = self.load_model()
        # The vocoder is part of the first_stage_model and is loaded with it.
        # No separate setup_transforms needed for vocoder if it's integrated.
        self.setup_mel_transform()

    def load_model(self):
        """Instantiates the model from the config."""
        model = instantiate_from_config(self.cfg['model']).to(self.device)
        print(f"模型已在设备上实例化: {self.device}")
        return model

    def load_weights(self, weights_path):
        """Loads weights into the model and handles EMA."""
        print(f"正在从以下路径加载权重: {weights_path}")
        ckpt = torch.load(weights_path, map_location=self.device)
        
        # Try to load AudioSR checkpoint with vocoder weights if available
        audiosr_checkpoint_path = "/home/husrcf/.cache/huggingface/hub/models--haoheliu--audiosr_basic/snapshots/74a47f49061a1e788e968cc43ad45c0b6243f37d/pytorch_model.bin"
        audiosr_ckpt = None
        if os.path.exists(audiosr_checkpoint_path):
            print("正在加载 AudioSR 检查点以获取 vocoder 权重...")
            # Load on CPU first to avoid memory issues, then move only vocoder weights to device
            audiosr_ckpt = torch.load(audiosr_checkpoint_path, map_location='cpu')
        
        if "ema" in ckpt and ckpt["ema"]:
            print("正在加载 EMA 权重...")
            ema_shadow_params = ckpt["ema"]
            
            new_ema_state_dict = {}
            for name, p_ema in ema_shadow_params.items():
                if name.startswith('model.'):
                    internal_name = name[len('model.'):]
                    buffer_name = internal_name.replace('.', '')
                    new_ema_state_dict[buffer_name] = p_ema

            if not new_ema_state_dict:
                print("警告: 在检查点中找到EMA权重，但模型的EMA模块中没有匹配的参数。将回退到标准权重。")
                self.model.load_state_dict(ckpt["state_dict"], strict=False)
            else:
                missing, unexpected = self.model.model_ema.load_state_dict(new_ema_state_dict, strict=False)
                print(f"EMA 权重已加载。缺失键: {len(missing)}, 意外键: {len(unexpected)}")
                self.model.model_ema.copy_to(self.model.model)

        elif "state_dict" in ckpt:
            print("正在加载标准 state_dict...")
            self.model.load_state_dict(ckpt["state_dict"], strict=False)
        else:
            print("正在加载原始权重...")
            self.model.load_state_dict(ckpt, strict=False)
        
        # Load AudioSR vocoder weights if available
        if audiosr_ckpt is not None:
            print("正在加载 AudioSR vocoder 权重...")
            audiosr_state_dict = audiosr_ckpt["state_dict"] if "state_dict" in audiosr_ckpt else audiosr_ckpt
            
            # Extract vocoder weights and move to device
            vocoder_weights = {}
            for key, value in audiosr_state_dict.items():
                if key.startswith('first_stage_model.vocoder.'):
                    new_key = key.replace('first_stage_model.vocoder.', 'first_stage_model.vocoder.')
                    vocoder_weights[new_key] = value.to(self.device)
            
            if vocoder_weights:
                missing_vocoder, unexpected_vocoder = self.model.load_state_dict(vocoder_weights, strict=False)
                print(f"AudioSR vocoder 权重已加载。缺失键: {len(missing_vocoder)}, 意外键: {len(unexpected_vocoder)}")
                # Clear the large checkpoint from memory
                del audiosr_ckpt, audiosr_state_dict, vocoder_weights
                torch.cuda.empty_cache() if torch.cuda.is_available() else None
                
                # Remove weight norm from vocoder (required for HiFi-GAN)
                if hasattr(self.model.first_stage_model, 'vocoder') and hasattr(self.model.first_stage_model.vocoder, 'remove_weight_norm'):
                    try:
                        self.model.first_stage_model.vocoder.remove_weight_norm()
                        print("已移除 vocoder 的 weight normalization")
                    except ValueError as e:
                        if "weight_norm" in str(e):
                            print(f"警告: 部分层没有 weight normalization，跳过移除: {e}")
                        else:
                            raise e
            else:
                print("警告: 未在 AudioSR 检查点中找到 vocoder 权重")
        
        self.model.eval()
        print("权重加载成功，模型已进入评估模式。")
    
    def _safe_remove_weight_norm(self, module):
        """
        Safely remove weight normalization from modules where it exists.
        """
        from torch.nn.utils import remove_weight_norm
        
        for name, child in module.named_modules():
            # Check each parameter to see if weight_norm is applied
            for param_name, _ in child.named_parameters(recurse=False):
                if param_name == 'weight_g' or param_name == 'weight_v':
                    # This indicates weight normalization is applied
                    try:
                        remove_weight_norm(child, 'weight')
                        print(f"已移除 {name} 的 weight normalization")
                        break  # Only one weight norm per module
                    except ValueError:
                        continue  # Skip if weight norm not found

    def setup_mel_transform(self):
        """Sets up AudioSR-exact mel spectrogram transformation."""
        # Use AudioSR's exact mel processing
        from audiosr.utils import mel_spectrogram_train, spectral_normalize_torch
        
        # Store the functions for use in preprocessing
        self.mel_spectrogram_train = mel_spectrogram_train
        self.spectral_normalize_torch = spectral_normalize_torch
        
        print("🎵 Using AudioSR-exact mel processing pipeline")

    def _preprocess_audio(self, audio_path):
        """Loads, resamples, and creates a low-pass version of the audio."""
        sr = self.cfg['data']['sample_rate']
        audio, orig_sr = torchaudio.load(audio_path)
        if audio.shape[0] > 1:
            audio = torch.mean(audio, dim=0, keepdim=True)
        if orig_sr != sr:
            audio = torchaudio.functional.resample(audio, orig_freq=orig_sr, new_freq=sr)
        
        # CRITICAL FIX: Normalize audio to handle silent/very quiet inputs
        current_rms = torch.sqrt(torch.mean(audio**2))
        if current_rms < 0.001:  # Very quiet audio
            print(f"⚠️  检测到安静音频 (RMS: {current_rms:.6f})，正在标准化...")
            target_rms = 0.05
            scale_factor = target_rms / (current_rms + 1e-8)
            scale_factor = min(scale_factor, 1000.0)  # Cap scaling
            audio = audio * scale_factor
            # Prevent clipping
            if audio.abs().max() > 0.95:
                audio = audio * (0.95 / audio.abs().max())
            new_rms = torch.sqrt(torch.mean(audio**2))
            print(f"   标准化完成: RMS {current_rms:.6f} -> {new_rms:.6f}")
        
        window = torch.hann_window(2048, device=self.device).to(torch.float32)
        
        stft_for_cutoff = torch.stft(audio.to(self.device).squeeze(0), n_fft=2048, hop_length=480, win_length=2048, window=window, return_complex=True, center=True)
        
        cutoff_freq = (_locate_cutoff_freq(stft_for_cutoff.abs(), percentile=0.985) / 1024) * (sr / 2)
        
        nyquist = sr / 2
        cutoff_freq = np.clip(cutoff_freq, 10.0, nyquist - 1.0)
        
        low_quality_waveform = lowpass(audio.cpu().numpy().squeeze(), highcut=cutoff_freq, fs=sr, order=8, _type="butter")
        
        return torch.from_numpy(low_quality_waveform.copy()).to(self.device).unsqueeze(0), audio.squeeze()

    @torch.no_grad()
    def _ddim_sample(self, start_noise, cond, steps=100, eta=1.0, full_audio=None):
        """Performs DDIM sampling to generate audio from noise."""
        shape = start_noise.shape
        b = shape[0]
        t_total = self.model.num_timesteps
        
        timesteps = torch.linspace(t_total - 1, 0, steps, device=self.device).long()
        x_t = start_noise
        
        for i, t_curr in enumerate(timesteps):
            t_prev = timesteps[i + 1] if i < steps - 1 else torch.tensor(-1, device=self.device)
            model_output = self.model.apply_model(x_t, t_curr.expand(b), cond, full_audio=full_audio)
            
            if self.model.parameterization == "v":
                sqrt_alpha_prod = self.model.sqrt_alphas_cumprod[t_curr]
                sqrt_one_minus_alpha_prod = self.model.sqrt_one_minus_alphas_cumprod[t_curr]
                x0_pred = sqrt_alpha_prod * x_t - sqrt_one_minus_alpha_prod * model_output
            else:
                x0_pred = (x_t - self.model.sqrt_one_minus_alphas_cumprod[t_curr] * model_output) / self.model.sqrt_alphas_cumprod[t_curr]
            
            x0_pred.clamp_(-1., 1.)
            
            if t_prev < 0:
                x_t = x0_pred
                continue
            
            alpha_prod_t_prev = self.model.alphas_cumprod_prev[t_curr]
            dir_xt = (1. - alpha_prod_t_prev - (eta * self.model.posterior_variance[t_curr])).sqrt() * model_output
            x_prev = alpha_prod_t_prev.sqrt() * x0_pred + dir_xt
            x_t = x_prev
            
        return x_t

    def run_inference(self, audio_path, output_dir, progress_callback, **kwargs):
        """Main inference function with chunking, stitching, and HiFi-GAN vocoder."""
        try:
            # --- 1. Preprocessing ---
            print("\n--- 步骤 1: 预处理音频 ---")
            sr = self.cfg['data']['sample_rate']
            low_quality_waveform, original_waveform = self._preprocess_audio(audio_path)
            print("音频已加载并预处理。")
            
            # --- 2. Chunking Setup ---
            print("\n--- 步骤 2: 音频分块 ---")
            chunk_seconds = self.cfg['data']['segment_seconds']
            chunk_samples = int(chunk_seconds * sr)
            overlap_samples = int(0.2 * sr)
            fade_samples = overlap_samples
            
            chunks = []
            current_pos = 0
            total_len = low_quality_waveform.shape[1]
            step = chunk_samples - overlap_samples

            while current_pos + chunk_samples <= total_len:
                chunk = low_quality_waveform[:, current_pos : current_pos + chunk_samples]
                chunks.append(chunk)
                current_pos += step
            
            if current_pos < total_len:
                last_chunk = low_quality_waveform[:, -chunk_samples:]
                chunks.append(last_chunk)

            print(f"音频被分割成 {len(chunks)} 个块。")

            processed_chunks = []
            
            # --- 3. Global Audio Processing (for Global Encoder) ---
            print("\n--- 步骤 3: 处理全局音频上下文 ---")
            full_audio_mel = None
            if hasattr(self.model.model.diffusion_model, 'use_global_encoder') and self.model.model.diffusion_model.use_global_encoder:
                with torch.no_grad():
                    # Process the full audio for global context
                    # Use the original high-quality audio instead of low-quality for better global context
                    original_tensor = original_waveform.unsqueeze(0).to(self.device)
                    
                    # Create mel spectrogram of full audio
                    full_log_mel_spec, _ = self.mel_spectrogram_train(original_tensor.to(torch.float32))
                    full_audio_mel = full_log_mel_spec.unsqueeze(0).unsqueeze(0)  # Shape: [1, 1, n_mels, time]
                    print(f"   Full audio mel for global context: {full_audio_mel.shape}")
            
            # --- 4. Inference over Chunks ---
            print("\n--- 步骤 4: 运行模型推理 ---")
            
            for i, chunk in enumerate(chunks):
                progress_callback(i + 1, len(chunks)) # Update GUI progress
                print(f"\n{'='*20} 正在处理块 {i+1}/{len(chunks)} {'='*20}")
                
                with torch.no_grad():
                    # Use AudioSR's exact mel processing pipeline
                    # mel_spectrogram_train expects (batch, samples) format
                    log_mel_spec, stft = self.mel_spectrogram_train(chunk.to(torch.float32))
                    
                    # AudioSR applies spectral normalization (dynamic range compression)
                    # This creates the proper log mel format that the VAE was trained on
                    log_mel = log_mel_spec.unsqueeze(0).unsqueeze(0)  # Shape: [1, 1, n_mels, time]
                    
                    print(f"   AudioSR mel: {log_mel.shape}, range: [{log_mel.min():.3f}, {log_mel.max():.3f}]")
                    
                    cond_tensor = self.model.cond_stage_models[0](log_mel)
                    cond = {"concat": cond_tensor}
                    
                    start_noise = torch.randn_like(cond_tensor)
                    latent = self._ddim_sample(start_noise, cond, steps=kwargs.get('ddim_steps', 100), full_audio=full_audio_mel)
                    
                    # Use the integrated HiFi-GAN vocoder via decode_to_waveform
                    waveform_chunk = self.model.first_stage_model.decode_to_waveform(latent)
                    
                    # Move to CPU immediately to free GPU memory and convert to numpy
                    processed_chunks.append(waveform_chunk.squeeze().detach().cpu().numpy())
                    
                    # --- Clean up GPU memory ---
                    del log_mel_spec, stft, log_mel, cond_tensor, cond, start_noise, latent, waveform_chunk
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()

            # --- 4. Stitching with Crossfade ---
            print("\n--- 步骤 4: 拼接音频块 ---")
            fade_in = np.linspace(0, 1, fade_samples)
            fade_out = np.linspace(1, 0, fade_samples)
            final_audio = np.zeros(len(original_waveform) + chunk_samples)
            current_pos = 0
            
            for i, chunk in enumerate(processed_chunks):
                chunk_len = chunk.shape[0]
                if i == 0:
                    final_audio[current_pos : current_pos + chunk_len] += chunk
                else:
                    chunk[:fade_samples] *= fade_in
                    final_audio[current_pos : current_pos + fade_samples] *= fade_out
                    final_audio[current_pos : current_pos + chunk_len] += chunk
                current_pos += chunk_len - overlap_samples

            final_audio = final_audio[:len(original_waveform)]
            print("音频块拼接成功。")
            
            # --- 5. Advanced Post-processing ---
            print("\n--- 步骤 5: 后期处理 ---")
            if kwargs.get('use_clipping', False):
                final_audio = self._frequency_clipping(original_waveform.cpu().numpy(), final_audio, sr, 12000)
            if kwargs.get('use_filter', False):
                print("正在应用 12kHz 低通滤波器...")
                b, a = butter(8, 12000, btype='low', fs=sr)
                final_audio = lfilter(b, a, final_audio)
            
            max_abs_val = np.max(np.abs(final_audio))
            if max_abs_val > 0:
                print("正在标准化音频...")
                final_audio /= max_abs_val
            else:
                print("警告: 生成的音频为完全静音，跳过标准化。")

            
            # --- 6. Save Outputs ---
            print("\n--- 步骤 6: 保存输出 ---")
            basename = os.path.splitext(os.path.basename(audio_path))[0]
            output_path = os.path.join(output_dir, f"{basename}_enhanced.wav")
            sf.write(output_path, final_audio, sr)
            print(f"增强后的音频已保存至: {output_path}")
            spec_path = os.path.join(output_dir, f"{basename}_spectrogram.png")
            self._save_spectrogram(final_audio, sr, spec_path)
            print(f"频谱图已保存至: {spec_path}")

            return output_path, spec_path
        except Exception as e:
            raise e

    def _frequency_clipping(self, original_wav, generated_wav, sr, cutoff_freq):
        """Combines low frequencies from original audio with high frequencies from generated audio."""
        print(f"正在应用频率剪切，分界线为 {cutoff_freq} Hz...")
        n_fft = self.cfg['data']['n_fft']
        min_len = min(len(original_wav), len(generated_wav))
        original_wav, generated_wav = original_wav[:min_len], generated_wav[:min_len]

        stft_orig = torch.stft(torch.from_numpy(original_wav), n_fft=n_fft, return_complex=True)
        stft_gen = torch.stft(torch.from_numpy(generated_wav), n_fft=n_fft, return_complex=True)
        
        freq_bins = torch.fft.rfftfreq(n_fft, d=1.0/sr)
        cutoff_bin = torch.where(freq_bins >= cutoff_freq)[0][0]
        
        mask = torch.ones_like(stft_orig)
        mask[cutoff_bin:, :] = 0
        
        stft_combined = stft_orig * mask + stft_gen * (1 - mask)
        
        combined_wav = torch.istft(stft_combined, n_fft=n_fft)
        return combined_wav.cpu().numpy()

    def _save_spectrogram(self, waveform, sr, save_path):
        """Generates and saves a spectrogram plot."""
        fig = Figure(figsize=(12, 6), dpi=100)
        ax = fig.add_subplot(111)
        ax.specgram(waveform, Fs=sr, cmap='viridis', NFFT=1024, noverlap=512)
        ax.set_title("Generated Audio Spectrogram")
        ax.set_xlabel("Time (s)")
        ax.set_ylabel("Frequency (Hz)")
        fig.tight_layout()
        fig.savefig(save_path)
        plt.close(fig)

# =====================================================================================
# SECTION: TKINTER GUI APPLICATION
# =====================================================================================

class App(tk.Tk):
    def __init__(self, cfg_path):
        super().__init__()
        self.title("Audio Super-Resolution Inference GUI")
        self.geometry("800x700")
        
        self.cfg_path = cfg_path
        self.engine = None
        self.thread = None

        # --- Style ---
        style = ttk.Style(self)
        style.theme_use('clam')
        style.configure('TButton', font=('Helvetica', 10))
        style.configure('Accent.TButton', font=('Helvetica', 12, 'bold'), foreground='white', background='#0078D7')
        style.configure('TLabel', font=('Helvetica', 10))
        style.configure('TFrame', padding=10)

        # --- Variables ---
        self.input_audio_var = tk.StringVar(value="/home/husrcf/Code/Python/ProjectLily_Z_III/data/valid/low/02 - 共同渡过.wav")
        self.weights_path_var = tk.StringVar(value="/home/husrcf/Code/Python/ProjectLily_Z_III/outputs/audiosr_ldm_train/checkpoints/step_30000.pt")
        self.output_dir_var = tk.StringVar(value="/home/husrcf/Code/Python/ProjectLily_Z_III/output")
        self.use_filter_var = tk.BooleanVar(value=True)
        self.use_clipping_var = tk.BooleanVar(value=True)
        self.ddim_steps_var = tk.IntVar(value=100)

        # --- Main Frame ---
        main_frame = ttk.Frame(self)
        main_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)

        self._create_io_widgets(main_frame)
        self._create_options_widgets(main_frame)
        self._create_control_widgets(main_frame)
        self._create_output_widgets(main_frame)

    def _create_io_widgets(self, parent):
        io_frame = ttk.LabelFrame(parent, text="输入 / 输出", padding=15)
        io_frame.pack(fill=tk.X, pady=5)

        ttk.Label(io_frame, text="输入音频:").grid(row=0, column=0, sticky='w', padx=5, pady=5)
        ttk.Entry(io_frame, textvariable=self.input_audio_var, width=60).grid(row=0, column=1, sticky='ew', padx=5)
        ttk.Button(io_frame, text="浏览...", command=lambda: self._browse_file(self.input_audio_var, "选择音频文件", (("Audio Files", "*.wav *.flac *.mp3"), ("All files", "*.*")))).grid(row=0, column=2, padx=5)

        ttk.Label(io_frame, text="模型权重:").grid(row=1, column=0, sticky='w', padx=5, pady=5)
        ttk.Entry(io_frame, textvariable=self.weights_path_var, width=60).grid(row=1, column=1, sticky='ew', padx=5)
        ttk.Button(io_frame, text="浏览...", command=lambda: self._browse_file(self.weights_path_var, "选择模型权重", (("PyTorch Checkpoints", "*.pt *.ckpt"), ("All files", "*.*")))).grid(row=1, column=2, padx=5)

        ttk.Label(io_frame, text="输出目录:").grid(row=2, column=0, sticky='w', padx=5, pady=5)
        ttk.Entry(io_frame, textvariable=self.output_dir_var, width=60).grid(row=2, column=1, sticky='ew', padx=5)
        ttk.Button(io_frame, text="浏览...", command=lambda: self._browse_dir(self.output_dir_var, "选择输出目录")).grid(row=2, column=2, padx=5)

        io_frame.columnconfigure(1, weight=1)

    def _create_options_widgets(self, parent):
        options_frame = ttk.LabelFrame(parent, text="推理选项", padding=15)
        options_frame.pack(fill=tk.X, pady=10)
        ttk.Checkbutton(options_frame, text="对输出应用 12kHz 低通滤波器", variable=self.use_filter_var).pack(anchor='w', padx=5)
        ttk.Checkbutton(options_frame, text="使用频率剪切 (低频来自原始音频, 高频来自AI)", variable=self.use_clipping_var).pack(anchor='w', padx=5, pady=5)
        ddim_frame = ttk.Frame(options_frame)
        ddim_frame.pack(anchor='w', padx=5, pady=5)
        ttk.Label(ddim_frame, text="DDIM 步数:").pack(side=tk.LEFT)
        ttk.Spinbox(ddim_frame, from_=10, to=1000, textvariable=self.ddim_steps_var, width=8).pack(side=tk.LEFT, padx=5)

    def _create_control_widgets(self, parent):
        control_frame = ttk.Frame(parent)
        control_frame.pack(fill=tk.X, pady=10)
        self.start_button = ttk.Button(control_frame, text="开始推理", command=self.start_inference, style='Accent.TButton', padding=10)
        self.start_button.pack(pady=10)
        self.progress_bar = ttk.Progressbar(control_frame, orient='horizontal', mode='determinate')
        self.progress_bar.pack(fill=tk.X, expand=True, pady=5)
        self.status_label = ttk.Label(control_frame, text="状态: 空闲", anchor='center')
        self.status_label.pack(fill=tk.X, expand=True)

    def _create_output_widgets(self, parent):
        output_frame = ttk.LabelFrame(parent, text="输出频谱图", padding=15)
        output_frame.pack(fill=tk.BOTH, expand=True, pady=5)
        self.fig = Figure(figsize=(5, 3), dpi=100)
        self.ax = self.fig.add_subplot(111)
        self.ax.set_facecolor("#f0f0f0")
        self.ax.tick_params(axis='x', colors='gray'); self.ax.tick_params(axis='y', colors='gray')
        self.ax.spines['bottom'].set_color('gray'); self.ax.spines['top'].set_color('gray') 
        self.ax.spines['right'].set_color('gray'); self.ax.spines['left'].set_color('gray')
        self.fig.tight_layout()
        self.canvas = FigureCanvasTkAgg(self.fig, master=output_frame)
        self.canvas.get_tk_widget().pack(side=tk.TOP, fill=tk.BOTH, expand=True)

    def _browse_file(self, var, title, filetypes):
        filepath = filedialog.askopenfilename(title=title, filetypes=filetypes)
        if filepath: var.set(filepath)

    def _browse_dir(self, var, title):
        dirpath = filedialog.askdirectory(title=title)
        if dirpath: var.set(dirpath)

    def start_inference(self):
        if not self.input_audio_var.get() or not self.weights_path_var.get() or not self.output_dir_var.get():
            messagebox.showerror("错误", "请指定输入音频、模型权重和输出目录。")
            return
        os.makedirs(self.output_dir_var.get(), exist_ok=True)
        self.start_button.config(state=tk.DISABLED, text="处理中...")
        self.progress_bar['value'] = 0
        self.update_status("状态: 初始化...")
        self.thread = threading.Thread(target=self._inference_thread_func, daemon=True)
        self.thread.start()

    def _inference_thread_func(self):
        try:
            print("正在初始化引擎...")
            if self.engine is None:
                self.update_status("状态: 加载配置并实例化模型...")
                with open(self.cfg_path, 'r', encoding='utf-8') as f:
                    cfg = yaml.safe_load(f)
                self.engine = InferenceEngine(cfg)
            
            self.update_status("状态: 加载模型权重...")
            self.engine.load_weights(self.weights_path_var.get())
            
            self.update_status("状态: 开始推理流程...")
            output_path, spec_path = self.engine.run_inference(
                audio_path=self.input_audio_var.get(), output_dir=self.output_dir_var.get(),
                progress_callback=self.update_progress, use_filter=self.use_filter_var.get(),
                use_clipping=self.use_clipping_var.get(), ddim_steps=self.ddim_steps_var.get()
            )
            self.after(0, self.on_inference_complete, output_path, spec_path)
        except BaseException:
            print("在推理线程中捕获到错误。正在向主线程报告...")
            exc_info = sys.exc_info()
            self.after(0, self.on_inference_error, exc_info)

    def update_progress(self, current, total):
        self.after(0, self._update_progress_gui, current, total)

    def _update_progress_gui(self, current, total):
        self.progress_bar['maximum'] = total
        self.progress_bar['value'] = current
        self.update_status(f"状态: 正在处理块 {current} / {total}...")

    def update_status(self, text):
        self.status_label.config(text=text)

    def on_inference_complete(self, output_path, spec_path):
        self.start_button.config(state=tk.NORMAL, text="开始推理")
        self.update_status(f"状态: 完成！音频已保存至 {output_path}")
        print(f"\n✅ 推理完成！音频已保存至: {output_path}")
        messagebox.showinfo("成功", f"推理完成！\n输出已保存至:\n{output_path}")
        
        img = plt.imread(spec_path)
        self.ax.clear()
        self.ax.imshow(img, aspect='auto', origin='lower')
        self.ax.axis('off')
        self.fig.tight_layout()
        self.canvas.draw()

    def on_inference_error(self, exc_info):
        self.start_button.config(state=tk.NORMAL, text="开始推理")
        self.update_status(f"状态: 发生错误！详情请查看控制台。")
        print("\n" + "="*50)
        print("推理过程中发生错误:")
        # Use the standard traceback module to print the exception
        traceback.print_exception(exc_info[0], exc_info[1], exc_info[2])
        print("="*50 + "\n")

# =====================================================================================
# SECTION: MAIN EXECUTION
# =====================================================================================

if __name__ == '__main__':
    CFG_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'config.yaml')
    if not os.path.exists(CFG_PATH):
        tk.Tk().withdraw()
        messagebox.showerror("配置错误", f"错误: 在脚本目录中未找到 'config.yaml'。\n请确保该文件存在。")
        sys.exit(1)
    app = App(CFG_PATH)
    app.mainloop()

```

## 文件: `data/model.py`

```python
# This file is adapted from the official AudioSR repository to be compatible with the project structure.
# It now integrates the LatentDiffusion model, VAE, and UNet in one place with the correct naming hierarchy.
#
# --- MODIFICATIONS ---
# 1. Replaced placeholder CLAP and Vocoder classes with detailed architectural skeletons
#    that precisely match the keys in the pre-trained checkpoint file.
# 2. Registered 'scale_factor' as a buffer in the main LatentDiffusion class.
# 3. Kept the custom EMA handler and the corrected UNet architecture from previous versions.
# 4. Corrected the shape of 'logit_scale_a' and 'logit_scale_t' to be scalars to match the checkpoint.
# 5. Fully built out the ClapWrapper's audio_branch with a SwinTransformer skeleton to match all checkpoint keys.
# 6. Set bias=False for STFT conv layers as per the checkpoint's structure.
# 7. Corrected a SyntaxError in the UNetModel's __init__ method.
# 8. Corrected an UnboundLocalError in the LatentDiffusion's register_schedule method.
# 9. Corrected the SwinTransformer's window_size and attn_mask shapes to match the checkpoint.
# 10. Re-instated the 'text_branch' and added the correct 'text_transform' and 'audio_transform' modules.
# 11. FINAL FIX: Added the '.sequential' nesting to the transform modules to match the checkpoint keys.

import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange
from contextlib import contextmanager
from functools import partial
import numpy as np

# --- Official AudioSR Imports (or their equivalents) ---
from audiosr.latent_diffusion.util import (
    exists,
    default,
    instantiate_from_config,
)
from audiosr.latent_diffusion.modules.distributions.distributions import (
    DiagonalGaussianDistribution,
)
from audiosr.latent_diffusion.modules.diffusionmodules.util import (
    make_beta_schedule,
    extract_into_tensor,
    noise_like,
    checkpoint,
    conv_nd,
    linear,
    zero_module,
    normalization,
    timestep_embedding,
)
from audiosr.latent_diffusion.modules.attention import SpatialTransformer
from audiosr.latent_diffusion.modules.diffusionmodules.model import Encoder, Decoder # VAE components

# --- Detailed Placeholder for Vocoder ---
class ResBlockModule(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size):
        super().__init__()
        self.convs1 = nn.ModuleList([
            nn.Conv1d(in_channels, out_channels, kernel_size, padding=kernel_size//2, dilation=1),
            nn.Conv1d(in_channels, out_channels, kernel_size, padding=kernel_size//2, dilation=3),
            nn.Conv1d(in_channels, out_channels, kernel_size, padding=kernel_size//2, dilation=5),
        ])
        self.convs2 = nn.ModuleList([
            nn.Conv1d(out_channels, out_channels, kernel_size, padding=kernel_size//2),
            nn.Conv1d(out_channels, out_channels, kernel_size, padding=kernel_size//2),
            nn.Conv1d(out_channels, out_channels, kernel_size, padding=kernel_size//2),
        ])

class Vocoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv_pre = nn.Conv1d(256, 1536, 7, 1, padding=3)
        self.ups = nn.ModuleList([
            nn.ConvTranspose1d(1536, 768, 12, 6, padding=3),
            nn.ConvTranspose1d(768, 384, 10, 5, padding=2, output_padding=1),
            nn.ConvTranspose1d(384, 192, 8, 4, padding=2),
            nn.ConvTranspose1d(192, 96, 4, 2, padding=1),
            nn.ConvTranspose1d(96, 48, 4, 2, padding=1),
        ])
        self.resblocks = nn.ModuleList([
            ResBlockModule(768, 768, 3), ResBlockModule(768, 768, 7), ResBlockModule(768, 768, 11), ResBlockModule(768, 768, 15),
            ResBlockModule(384, 384, 3), ResBlockModule(384, 384, 7), ResBlockModule(384, 384, 11), ResBlockModule(384, 384, 15),
            ResBlockModule(192, 192, 3), ResBlockModule(192, 192, 7), ResBlockModule(192, 192, 11), ResBlockModule(192, 192, 15),
            ResBlockModule(96, 96, 3), ResBlockModule(96, 96, 7), ResBlockModule(96, 96, 11), ResBlockModule(96, 96, 15),
            ResBlockModule(48, 48, 3), ResBlockModule(48, 48, 7), ResBlockModule(48, 48, 11), ResBlockModule(48, 48, 15),
        ])
        self.conv_post = nn.Conv1d(48, 1, 7, 1, padding=3)

# --- Detailed Placeholders for CLAP ---
class Mlp(nn.Module):
    def __init__(self, in_features, hidden_features, out_features, act_layer=nn.GELU, drop=0.):
        super().__init__()
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

class WindowAttention(nn.Module):
    def __init__(self, dim, num_heads):
        super().__init__()
        self.qkv = nn.Linear(dim, dim * 3, bias=True)
        self.proj = nn.Linear(dim, dim)
        window_size = 8
        self.relative_position_bias_table = nn.Parameter(
            torch.zeros((2 * window_size - 1) * (2 * window_size - 1), num_heads))
        self.relative_position_index = nn.Parameter(torch.zeros(64, 64), requires_grad=False)

class SwinTransformerBlock(nn.Module):
    def __init__(self, dim, num_heads):
        super().__init__()
        self.norm1 = nn.LayerNorm(dim)
        self.attn = WindowAttention(dim, num_heads)
        self.norm2 = nn.LayerNorm(dim)
        self.mlp = Mlp(dim, dim * 4, dim)
        self.register_parameter("attn_mask", None)

class PatchMerging(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.reduction = nn.Linear(4 * in_channels, out_channels, bias=False)
        self.norm = nn.LayerNorm(4 * in_channels)

class BasicLayer(nn.Module):
    def __init__(self, dim, out_dim, num_blocks, num_heads, mask_shape=None):
        super().__init__()
        self.blocks = nn.ModuleList([SwinTransformerBlock(dim, num_heads) for _ in range(num_blocks)])
        if mask_shape is not None:
            for i in range(num_blocks):
                if i % 2 != 0:
                    self.blocks[i].attn_mask = nn.Parameter(torch.zeros(mask_shape), requires_grad=False)
        if out_dim is not None:
            self.downsample = PatchMerging(dim, out_dim)
        else:
            self.downsample = None

class PatchEmbed(nn.Module):
    def __init__(self):
        super().__init__()
        self.proj = nn.Conv2d(1, 128, kernel_size=4, stride=4)
        self.norm = nn.LayerNorm(128)

class ClapAudioBranch(nn.Module):
    def __init__(self):
        super().__init__()
        self.spectrogram_extractor = nn.Module()
        self.spectrogram_extractor.stft = nn.Module()
        self.spectrogram_extractor.stft.conv_real = nn.Conv1d(1, 513, 1024, bias=False)
        self.spectrogram_extractor.stft.conv_imag = nn.Conv1d(1, 513, 1024, bias=False)
        self.logmel_extractor = nn.Module()
        self.logmel_extractor.melW = nn.Parameter(torch.zeros(513, 64))
        self.bn0 = nn.BatchNorm1d(64)
        self.patch_embed = PatchEmbed()
        self.layers = nn.ModuleList([
            BasicLayer(128, 256, 2, 4, mask_shape=(64, 64, 64)),
            BasicLayer(256, 512, 2, 8, mask_shape=(16, 64, 64)),
            BasicLayer(512, 1024, 12, 16, mask_shape=(4, 64, 64)),
            BasicLayer(1024, None, 2, 32) 
        ])
        self.norm = nn.LayerNorm(1024)
        self.tscam_conv = nn.Conv2d(1024, 527, kernel_size=(2,3), padding=(0,1))
        self.head = nn.Linear(527, 527)

class BertEmbeddings(nn.Module):
    def __init__(self):
        super().__init__()
        self.word_embeddings = nn.Embedding(50265, 768)
        self.position_embeddings = nn.Embedding(514, 768)
        self.token_type_embeddings = nn.Embedding(1, 768)
        self.LayerNorm = nn.LayerNorm(768, eps=1e-12)
        self.register_buffer("position_ids", torch.arange(514).expand((1, -1)))

class BertSelfAttention(nn.Module):
    def __init__(self):
        super().__init__()
        self.query = nn.Linear(768, 768)
        self.key = nn.Linear(768, 768)
        self.value = nn.Linear(768, 768)

class BertAttention(nn.Module):
    def __init__(self):
        super().__init__()
        self.self = BertSelfAttention()
        self.output = nn.Module()
        self.output.dense = nn.Linear(768, 768)
        self.output.LayerNorm = nn.LayerNorm(768, eps=1e-12)

class BertIntermediate(nn.Module):
    def __init__(self):
        super().__init__()
        self.dense = nn.Linear(768, 3072)

class BertOutput(nn.Module):
    def __init__(self):
        super().__init__()
        self.dense = nn.Linear(3072, 768)
        self.LayerNorm = nn.LayerNorm(768, eps=1e-12)

class BertLayer(nn.Module):
    def __init__(self):
        super().__init__()
        self.attention = BertAttention()
        self.intermediate = BertIntermediate()
        self.output = BertOutput()

class BertEncoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer = nn.ModuleList([BertLayer() for _ in range(12)])

class BertPooler(nn.Module):
    def __init__(self):
        super().__init__()
        self.dense = nn.Linear(768, 768)

class ClapTextBranch(nn.Module):
    def __init__(self):
        super().__init__()
        self.embeddings = BertEmbeddings()
        self.encoder = BertEncoder()
        self.pooler = BertPooler()

class ClapWrapper(nn.Module):
    def __init__(self):
        super().__init__()
        self.model = nn.Module()
        self.model.logit_scale_a = nn.Parameter(torch.tensor(0.0))
        self.model.logit_scale_t = nn.Parameter(torch.tensor(0.0))
        self.model.audio_branch = ClapAudioBranch()
        self.model.text_branch = ClapTextBranch()
        
        self.model.text_projection = nn.Sequential(nn.Linear(768, 512), nn.ReLU(), nn.Linear(512, 512))
        self.model.audio_projection = nn.Sequential(nn.Linear(1024, 512), nn.ReLU(), nn.Linear(512, 512))
        
        # --- FIX: Added the '.sequential' nesting to match the checkpoint keys ---
        self.model.text_transform = nn.Module()
        self.model.text_transform.sequential = nn.Sequential(
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.GELU(),
            nn.Linear(512, 512)
        )
        self.model.audio_transform = nn.Module()
        self.model.audio_transform.sequential = nn.Sequential(
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.GELU(),
            nn.Linear(512, 512)
        )
        
        self.mel_transform = nn.Module()
        self.mel_transform.spectrogram = nn.Module()
        self.mel_transform.spectrogram.window = nn.Parameter(torch.zeros(1024))
        self.mel_transform.mel_scale = nn.Module()
        self.mel_transform.mel_scale.fb = nn.Parameter(torch.zeros(513, 64))

# --- Custom EMA Handler ---
class CustomLitEma(nn.Module):
    def __init__(self, model, decay=0.9999, use_num_updates=True):
        super().__init__()
        if decay < 0.0 or decay > 1.0:
            raise ValueError('Decay must be between 0 and 1')
        self.m_name2s_name = {name: name.replace('.', '') for name, p in model.named_parameters()}
        self.register_buffer('decay', torch.tensor(decay, dtype=torch.float32))
        self.register_buffer('num_updates', torch.tensor(0, dtype=torch.int64) if use_num_updates else torch.tensor(-1, dtype=torch.int64))
        for name, p in model.named_parameters():
            if p.requires_grad:
                s_name = self.m_name2s_name[name]
                self.register_buffer(s_name, p.clone().detach().data)
        self.collected_params = []
    @torch.no_grad()
    def forward(self, model):
        self.num_updates += 1
        for name, p in model.named_parameters():
            if p.requires_grad:
                s_name = self.m_name2s_name[name]
                s_param = self.get_buffer(s_name)
                s_param.sub_((1 - self.decay) * (s_param - p.data))
    def store(self, params): self.collected_params = [p.clone() for p in params]
    def restore(self, params):
        for p_old, p_new in zip(params, self.collected_params): p_old.data.copy_(p_new.data)
        self.collected_params = []
    def copy_to(self, model):
        for name, p in model.named_parameters():
            if p.requires_grad: p.data.copy_(self.get_buffer(self.m_name2s_name[name]))

# --- Building Blocks for UNet (ResBlock, Upsample, etc.) ---
class TimestepBlock(nn.Module):
    @staticmethod
    def forward(x, emb): raise NotImplementedError
class TimestepEmbedSequential(nn.Sequential, TimestepBlock):
    def forward(self, x, emb, context=None):
        for layer in self:
            if isinstance(layer, TimestepBlock): x = layer(x, emb)
            elif isinstance(layer, SpatialTransformer): x = layer(x, context)
            else: x = layer(x)
        return x
class Upsample(nn.Module):
    def __init__(self, channels, use_conv, dims=2, out_channels=None):
        super().__init__()
        self.channels, self.out_channels, self.use_conv, self.dims = channels, out_channels or channels, use_conv, dims
        if use_conv: self.conv = conv_nd(dims, self.channels, self.out_channels, 3, padding=1)
    def forward(self, x):
        assert x.shape[1] == self.channels
        x = F.interpolate(x, scale_factor=2, mode="nearest")
        if self.use_conv: x = self.conv(x)
        return x
class Downsample(nn.Module):
    def __init__(self, channels, use_conv, dims=2, out_channels=None):
        super().__init__()
        self.channels, self.out_channels, self.use_conv, self.dims = channels, out_channels or channels, use_conv, dims
        stride = 2
        if use_conv: self.op = conv_nd(dims, self.channels, self.out_channels, 3, stride=stride, padding=1)
        else: self.op = nn.AvgPool2d(kernel_size=stride, stride=stride)
    def forward(self, x):
        assert x.shape[1] == self.channels
        return self.op(x)
class ResBlock(TimestepBlock):
    def __init__(self, channels, emb_channels, dropout, out_channels=None, dims=2, use_checkpoint=False, use_scale_shift_norm=False):
        super().__init__()
        self.channels, self.emb_channels, self.dropout, self.out_channels, self.use_checkpoint, self.use_scale_shift_norm = channels, emb_channels, dropout, out_channels or channels, use_checkpoint, use_scale_shift_norm
        self.in_layers = nn.Sequential(normalization(channels), nn.SiLU(), conv_nd(dims, channels, self.out_channels, 3, padding=1))
        self.emb_layers = nn.Sequential(nn.SiLU(), linear(emb_channels, 2 * self.out_channels if use_scale_shift_norm else self.out_channels))
        self.out_layers = nn.Sequential(normalization(self.out_channels), nn.SiLU(), nn.Dropout(p=dropout), zero_module(conv_nd(dims, self.out_channels, self.out_channels, 3, padding=1)))
        self.skip_connection = nn.Identity() if self.out_channels == channels else conv_nd(dims, channels, self.out_channels, 1)
    def forward(self, x, emb): return checkpoint(self._forward, (x, emb), self.parameters(), self.use_checkpoint)
    def _forward(self, x, emb):
        h = self.in_layers(x)
        emb_out = self.emb_layers(emb).type(h.dtype)
        while len(emb_out.shape) < len(h.shape): emb_out = emb_out[..., None]
        if self.use_scale_shift_norm:
            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]
            scale, shift = torch.chunk(emb_out, 2, dim=1)
            h = out_norm(h) * (1 + scale) + shift
            h = out_rest(h)
        else:
            h = h + emb_out
            h = self.out_layers(h)
        return self.skip_connection(x) + h

# --- Main U-Net Model (Corrected Architecture) ---
class UNetModel(nn.Module):
    def __init__(self, image_size, in_channels, model_channels, out_channels, num_res_blocks, attention_resolutions, dropout=0, channel_mult=(1, 2, 4, 8), conv_resample=True, dims=2, num_classes=None, use_checkpoint=False, num_heads=-1, num_head_channels=-1, use_scale_shift_norm=False, resblock_updown=False, use_spatial_transformer=False, transformer_depth=1, context_dim=None, **kwargs):
        super().__init__()
        self.image_size, self.in_channels, self.model_channels, self.out_channels, self.num_res_blocks, self.attention_resolutions, self.dropout, self.channel_mult, self.conv_resample, self.num_classes, self.use_checkpoint, self.num_heads, self.num_head_channels = image_size, in_channels, model_channels, out_channels, num_res_blocks, attention_resolutions, dropout, channel_mult, conv_resample, num_classes, use_checkpoint, num_heads, num_head_channels
        time_embed_dim = model_channels * 4
        self.time_embed = nn.Sequential(linear(model_channels, time_embed_dim), nn.SiLU(), linear(time_embed_dim, time_embed_dim))
        if self.num_classes is not None: self.label_emb = nn.Embedding(num_classes, time_embed_dim)
        self.input_blocks = nn.ModuleList([TimestepEmbedSequential(conv_nd(dims, in_channels, model_channels, 3, padding=1))])
        self._feature_size, input_block_chans, ch, ds = model_channels, [model_channels], model_channels, 1
        for level, mult in enumerate(channel_mult):
            for i in range(num_res_blocks):
                layers = [ResBlock(ch, time_embed_dim, dropout, out_channels=mult * model_channels, dims=dims, use_checkpoint=use_checkpoint, use_scale_shift_norm=use_scale_shift_norm)]
                ch = mult * model_channels
                if ds in attention_resolutions:
                    num_heads = ch // num_head_channels
                    dim_head = num_head_channels
                    layers.append(SpatialTransformer(ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim))
                    layers.append(SpatialTransformer(ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim))
                self.input_blocks.append(TimestepEmbedSequential(*layers))
                self._feature_size += ch
                input_block_chans.append(ch)
            if level != len(channel_mult) - 1:
                out_ch = ch
                self.input_blocks.append(
                    TimestepEmbedSequential(
                        Downsample(ch, conv_resample, dims=dims, out_channels=out_ch)
                    )
                )
                ch = out_ch
                input_block_chans.append(ch)
                ds *= 2
                self._feature_size += ch
        
        num_heads = ch // num_head_channels
        dim_head = num_head_channels
        self.middle_block = TimestepEmbedSequential(ResBlock(ch, time_embed_dim, dropout, dims=dims, use_checkpoint=use_checkpoint, use_scale_shift_norm=use_scale_shift_norm), SpatialTransformer(ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim), SpatialTransformer(ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim), ResBlock(ch, time_embed_dim, dropout, dims=dims, use_checkpoint=use_checkpoint, use_scale_shift_norm=use_scale_shift_norm))
        self._feature_size += ch
        self.output_blocks = nn.ModuleList([])
        for level, mult in list(enumerate(channel_mult))[::-1]:
            for i in range(num_res_blocks + 1):
                ich = input_block_chans.pop()
                layers = [ResBlock(ch + ich, time_embed_dim, dropout, out_channels=model_channels * mult, dims=dims, use_checkpoint=use_checkpoint, use_scale_shift_norm=use_scale_shift_norm)]
                ch = model_channels * mult
                if ds in attention_resolutions:
                    num_heads = ch // num_head_channels
                    dim_head = num_head_channels
                    layers.append(SpatialTransformer(ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim))
                    layers.append(SpatialTransformer(ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim))
                if level and i == num_res_blocks:
                    out_ch = ch
                    layers.append(Upsample(ch, conv_resample, dims=dims, out_channels=out_ch))
                    ds //= 2
                self.output_blocks.append(TimestepEmbedSequential(*layers))
                self._feature_size += ch
        self.out = nn.Sequential(normalization(ch), nn.SiLU(), zero_module(conv_nd(dims, model_channels, out_channels, 3, padding=1)))
    def forward(self, x, timesteps, context=None, y=None, **kwargs):
        hs, t_emb = [], timestep_embedding(timesteps, self.model_channels, repeat_only=False)
        emb = self.time_embed(t_emb)
        if self.num_classes is not None: emb = emb + self.label_emb(y)
        h = x
        for module in self.input_blocks:
            h = module(h, emb, context)
            hs.append(h)
        h = self.middle_block(h, emb, context)
        for module in self.output_blocks:
            h = module(torch.cat([h, hs.pop()], dim=1), emb, context)
        return self.out(h)

# --- VAE and Conditioner definitions ---
class AutoencoderKL(nn.Module):
    def __init__(self, ddconfig, embed_dim, ckpt_path=None, ignore_keys=[]):
        super().__init__()
        self.encoder, self.decoder, self.vocoder = Encoder(**ddconfig), Decoder(**ddconfig), Vocoder()
        assert ddconfig["double_z"]
        self.quant_conv = torch.nn.Conv2d(2 * ddconfig["z_channels"], 2 * embed_dim, 1)
        self.post_quant_conv = torch.nn.Conv2d(embed_dim, ddconfig["z_channels"], 1)
        self.embed_dim = embed_dim
    def encode(self, x): return DiagonalGaussianDistribution(self.quant_conv(self.encoder(x)))
    def decode(self, z): return self.decoder(self.post_quant_conv(z))
class VAEFeatureExtract(nn.Module):
    def __init__(self, first_stage_config):
        super().__init__()
        self.vae = instantiate_from_config(first_stage_config)
        self.vae.eval()
        for p in self.vae.parameters(): p.requires_grad = False
    def forward(self, batch):
        with torch.no_grad(): vae_embed = self.vae.encode(batch).sample()
        return vae_embed.detach()

# --- Main LDM Class ---
class LatentDiffusion(nn.Module):
    def __init__(self, first_stage_config, cond_stage_config, unet_config, beta_schedule="linear", timesteps=1000, loss_type="l2", parameterization="v", scale_factor=1.0, scale_by_std=False, use_ema=True, **kwargs):
        super().__init__()
        self.scale_by_std, self.parameterization, self.model = scale_by_std, parameterization, DiffusionWrapper(unet_config)
        if use_ema: self.model_ema = CustomLitEma(self.model)
        self.first_stage_model = instantiate_from_config(first_stage_config)
        self.cond_stage_models, self.cond_stage_model_metadata = nn.ModuleList(), {}
        self.instantiate_cond_stage(cond_stage_config)
        self.clap = ClapWrapper()
        self.register_schedule(beta_schedule=beta_schedule, timesteps=timesteps, scale_factor=scale_factor)
        self.loss_type = loss_type
    def register_schedule(self, beta_schedule, timesteps, linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3, scale_factor=1.0):
        self.register_buffer('scale_factor', torch.tensor(scale_factor))
        betas = make_beta_schedule(beta_schedule, timesteps, linear_start=linear_start, linear_end=linear_end, cosine_s=cosine_s)
        alphas = 1. - betas
        alphas_cumprod = np.cumprod(alphas, axis=0)
        alphas_cumprod_prev = np.append(1., alphas_cumprod[:-1])
        to_torch = partial(torch.tensor, dtype=torch.float32)
        self.num_timesteps = int(timesteps)
        self.register_buffer('betas', to_torch(betas)); self.register_buffer('alphas_cumprod', to_torch(alphas_cumprod)); self.register_buffer('alphas_cumprod_prev', to_torch(alphas_cumprod_prev))
        self.register_buffer('sqrt_alphas_cumprod', to_torch(np.sqrt(alphas_cumprod))); self.register_buffer('sqrt_one_minus_alphas_cumprod', to_torch(np.sqrt(1. - alphas_cumprod)))
        self.register_buffer('log_one_minus_alphas_cumprod', to_torch(np.log(1. - alphas_cumprod))); self.register_buffer('sqrt_recip_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod)))
        self.register_buffer('sqrt_recipm1_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod - 1)))
        posterior_variance = (1 - self.scale_by_std) * betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)
        self.register_buffer('posterior_variance', to_torch(posterior_variance)); self.register_buffer('posterior_log_variance_clipped', to_torch(np.log(np.maximum(posterior_variance, 1e-20))))
        self.register_buffer('posterior_mean_coef1', to_torch(betas * np.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod)))
        self.register_buffer('posterior_mean_coef2', to_torch((1. - alphas_cumprod_prev) * np.sqrt(alphas) / (1. - alphas_cumprod)))
        self.register_buffer('logvar', torch.zeros(timesteps))
    def instantiate_cond_stage(self, config):
        for key, cfg in config.items():
            model = instantiate_from_config(cfg)
            self.cond_stage_models.append(model)
            self.cond_stage_model_metadata[key] = {"model_idx": len(self.cond_stage_models) - 1, "cond_stage_key": cfg.get("cond_stage_key"), "conditioning_key": cfg.get("conditioning_key")}
    @contextmanager
    def ema_scope(self, context=None):
        if hasattr(self, 'model_ema'):
            self.model_ema.store([p.data for p in self.model.parameters()]); self.model_ema.copy_to(self.model)
        try: yield None
        finally:
            if hasattr(self, 'model_ema'): self.model_ema.restore([p.data for p in self.model.parameters()])
    @torch.no_grad()
    def get_input(self, batch):
        x, c_concat_data = batch["fbank"], batch["lowpass_mel"]
        z = self.get_first_stage_encoding(self.first_stage_model.encode(x))
        cond = {}
        for key, meta in self.cond_stage_model_metadata.items():
            if meta["conditioning_key"] == "concat": cond[key] = self.cond_stage_models[meta["model_idx"]](c_concat_data)
        return z, cond
    def get_first_stage_encoding(self, encoder_posterior):
        z = encoder_posterior.sample() if isinstance(encoder_posterior, DiagonalGaussianDistribution) else encoder_posterior
        return self.scale_factor * z
    def q_sample(self, x_start, t, noise=None):
        noise = default(noise, lambda: torch.randn_like(x_start))
        return (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start + extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise)
    def get_v(self, x, noise, t): return (extract_into_tensor(self.sqrt_alphas_cumprod, t, x.shape) * noise - extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x.shape) * x)
    def apply_model(self, x_noisy, t, cond): return self.model(x_noisy, t, cond)
    def get_loss(self, pred, target):
        if self.loss_type == 'l1': return F.l1_loss(pred, target)
        elif self.loss_type == 'l2': return F.mse_loss(pred, target)
        else: raise NotImplementedError()
    def p_losses(self, x_start, cond, t, noise=None):
        noise = default(noise, lambda: torch.randn_like(x_start))
        x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)
        model_output = self.apply_model(x_noisy, t, cond)
        target = self.get_v(x_start, noise, t) if self.parameterization == "v" else noise
        loss = self.get_loss(model_output, target)
        return loss, {"loss": loss}
    def forward(self, batch):
        z, c = self.get_input(batch)
        t = torch.randint(0, self.num_timesteps, (z.shape[0],), device=z.device).long()
        return self.p_losses(z, c, t)
class DiffusionWrapper(nn.Module):
    def __init__(self, unet_config):
        super().__init__()
        self.diffusion_model = instantiate_from_config(unet_config)
    def forward(self, x, t, cond_dict={}):
        xc, context = x, None
        for key, value in cond_dict.items():
            if "concat" in key: xc = torch.cat([x, value], dim=1)
        return self.diffusion_model(xc, t, context=context)

```

## 文件: `backpack/inference_backup.py`

```python
import os
import sys
import yaml
import threading
import numpy as np
import torch
import torchaudio
import soundfile as sf
from scipy.signal import butter, lfilter
import traceback # Import the standard traceback module

# --- GUI Imports ---
import tkinter as tk
from tkinter import ttk, filedialog, messagebox

# --- Plotting Imports ---
import matplotlib.pyplot as plt
from matplotlib.figure import Figure
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg

# --- Local Project Imports ---
# Make sure model.py and audiosr folder are in the same directory
from model import LatentDiffusion
from audiosr.latent_diffusion.util import instantiate_from_config
from audiosr.utils import _locate_cutoff_freq
from audiosr.lowpass import lowpass
# --- HiFi-GAN Imports ---
# Note: The Vocoder class is already defined in your model.py inside AutoencoderKL,
# so we don't need to import it separately if the weights are in the main checkpoint.

# =====================================================================================
# SECTION: CORE INFERENCE AND AUDIO PROCESSING LOGIC
# =====================================================================================

class InferenceEngine:
    """
    Handles the core logic for audio processing, model inference, and file I/O.
    """
    def __init__(self, cfg, device='cuda'):
        self.cfg = cfg
        self.device = device
        self.model = self.load_model()
        # The vocoder is part of the first_stage_model and is loaded with it.
        # No separate setup_transforms needed for vocoder if it's integrated.
        self.setup_mel_transform()

    def load_model(self):
        """Instantiates the model from the config."""
        model = instantiate_from_config(self.cfg['model']).to(self.device)
        print(f"模型已在设备上实例化: {self.device}")
        return model

    def load_weights(self, weights_path):
        """Loads weights into the model and handles EMA."""
        print(f"正在从以下路径加载权重: {weights_path}")
        ckpt = torch.load(weights_path, map_location=self.device)
        
        if "ema" in ckpt and ckpt["ema"]:
            print("正在加载 EMA 权重...")
            ema_shadow_params = ckpt["ema"]
            
            new_ema_state_dict = {}
            for name, p_ema in ema_shadow_params.items():
                if name.startswith('model.'):
                    internal_name = name[len('model.'):]
                    buffer_name = internal_name.replace('.', '')
                    new_ema_state_dict[buffer_name] = p_ema

            if not new_ema_state_dict:
                print("警告: 在检查点中找到EMA权重，但模型的EMA模块中没有匹配的参数。将回退到标准权重。")
                self.model.load_state_dict(ckpt["state_dict"], strict=False)
            else:
                missing, unexpected = self.model.model_ema.load_state_dict(new_ema_state_dict, strict=False)
                print(f"EMA 权重已加载。缺失键: {len(missing)}, 意外键: {len(unexpected)}")
                self.model.model_ema.copy_to(self.model.model)

        elif "state_dict" in ckpt:
            print("正在加载标准 state_dict...")
            self.model.load_state_dict(ckpt["state_dict"], strict=False)
        else:
            print("正在加载原始权重...")
            self.model.load_state_dict(ckpt, strict=False)
        
        self.model.eval()
        print("权重加载成功，模型已进入评估模式。")

    def setup_mel_transform(self):
        """Sets up only the mel spectrogram transformation."""
        data_cfg = self.cfg['data']
        self.mel_transform = torchaudio.transforms.MelSpectrogram(
            sample_rate=data_cfg['sample_rate'],
            n_fft=data_cfg['n_fft'],
            win_length=data_cfg['win_length'],
            hop_length=data_cfg['hop_length'],
            f_min=data_cfg.get('fmin', 0),
            f_max=data_cfg.get('fmax', None),
            n_mels=data_cfg['n_mels'],
            power=2.0,
            norm='slaney',
            mel_scale='slaney'
        ).to(self.device)

    def _preprocess_audio(self, audio_path):
        """Loads, resamples, and creates a low-pass version of the audio."""
        sr = self.cfg['data']['sample_rate']
        audio, orig_sr = torchaudio.load(audio_path)
        if audio.shape[0] > 1:
            audio = torch.mean(audio, dim=0, keepdim=True)
        if orig_sr != sr:
            audio = torchaudio.functional.resample(audio, orig_freq=orig_sr, new_freq=sr)
        
        window = torch.hann_window(2048, device=self.device).to(torch.float32)
        
        stft_for_cutoff = torch.stft(audio.to(self.device).squeeze(0), n_fft=2048, hop_length=480, win_length=2048, window=window, return_complex=True, center=True)
        
        cutoff_freq = (_locate_cutoff_freq(stft_for_cutoff.abs(), percentile=0.985) / 1024) * (sr / 2)
        
        nyquist = sr / 2
        cutoff_freq = np.clip(cutoff_freq, 10.0, nyquist - 1.0)
        
        low_quality_waveform = lowpass(audio.cpu().numpy().squeeze(), highcut=cutoff_freq, fs=sr, order=8, _type="butter")
        
        return torch.from_numpy(low_quality_waveform.copy()).to(self.device).unsqueeze(0), audio.squeeze()

    @torch.no_grad()
    def _ddim_sample(self, start_noise, cond, steps=100, eta=1.0):
        """Performs DDIM sampling to generate audio from noise."""
        shape = start_noise.shape
        b = shape[0]
        t_total = self.model.num_timesteps
        
        timesteps = torch.linspace(t_total - 1, 0, steps, device=self.device).long()
        x_t = start_noise
        
        for i, t_curr in enumerate(timesteps):
            t_prev = timesteps[i + 1] if i < steps - 1 else torch.tensor(-1, device=self.device)
            model_output = self.model.apply_model(x_t, t_curr.expand(b), cond)
            
            if self.model.parameterization == "v":
                sqrt_alpha_prod = self.model.sqrt_alphas_cumprod[t_curr]
                sqrt_one_minus_alpha_prod = self.model.sqrt_one_minus_alphas_cumprod[t_curr]
                x0_pred = sqrt_alpha_prod * x_t - sqrt_one_minus_alpha_prod * model_output
            else:
                x0_pred = (x_t - self.model.sqrt_one_minus_alphas_cumprod[t_curr] * model_output) / self.model.sqrt_alphas_cumprod[t_curr]
            
            x0_pred.clamp_(-1., 1.)
            
            if t_prev < 0:
                x_t = x0_pred
                continue
            
            alpha_prod_t_prev = self.model.alphas_cumprod_prev[t_curr]
            dir_xt = (1. - alpha_prod_t_prev - (eta * self.model.posterior_variance[t_curr])).sqrt() * model_output
            x_prev = alpha_prod_t_prev.sqrt() * x0_pred + dir_xt
            x_t = x_prev
            
        return x_t

    def run_inference(self, audio_path, output_dir, progress_callback, **kwargs):
        """Main inference function with chunking, stitching, and HiFi-GAN vocoder."""
        try:
            # --- 1. Preprocessing ---
            print("\n--- 步骤 1: 预处理音频 ---")
            sr = self.cfg['data']['sample_rate']
            low_quality_waveform, original_waveform = self._preprocess_audio(audio_path)
            print("音频已加载并预处理。")
            
            # --- 2. Chunking Setup ---
            print("\n--- 步骤 2: 音频分块 ---")
            chunk_seconds = self.cfg['data']['segment_seconds']
            chunk_samples = int(chunk_seconds * sr)
            overlap_samples = int(0.2 * sr)
            fade_samples = overlap_samples
            
            chunks = []
            current_pos = 0
            total_len = low_quality_waveform.shape[1]
            step = chunk_samples - overlap_samples

            while current_pos + chunk_samples <= total_len:
                chunk = low_quality_waveform[:, current_pos : current_pos + chunk_samples]
                chunks.append(chunk)
                current_pos += step
            
            if current_pos < total_len:
                last_chunk = low_quality_waveform[:, -chunk_samples:]
                chunks.append(last_chunk)

            print(f"音频被分割成 {len(chunks)} 个块。")

            processed_chunks = []
            
            # --- 3. Inference over Chunks ---
            print("\n--- 步骤 3: 运行模型推理 ---")
            
            for i, chunk in enumerate(chunks):
                progress_callback(i + 1, len(chunks)) # Update GUI progress
                print(f"\n{'='*20} 正在处理块 {i+1}/{len(chunks)} {'='*20}")
                
                with torch.no_grad():
                    mel = self.mel_transform(chunk.to(torch.float32))
                    log_mel = torch.log(torch.clamp(mel, min=1e-5)).unsqueeze(0)
                    
                    cond_tensor = self.model.cond_stage_models[0](log_mel)
                    cond = {"concat": cond_tensor}
                    
                    start_noise = torch.randn_like(cond_tensor)
                    latent = self._ddim_sample(start_noise, cond, steps=kwargs.get('ddim_steps', 100))
                    
                    # Use the integrated HiFi-GAN vocoder via decode_to_waveform
                    waveform_chunk = self.model.first_stage_model.decode_to_waveform(latent)
                    
                    # Move to CPU immediately to free GPU memory and convert to numpy
                    processed_chunks.append(waveform_chunk.squeeze().detach().cpu().numpy())
                    
                    # --- Clean up GPU memory ---
                    del mel, log_mel, cond_tensor, cond, start_noise, latent, waveform_chunk
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()

            # --- 4. Stitching with Crossfade ---
            print("\n--- 步骤 4: 拼接音频块 ---")
            fade_in = np.linspace(0, 1, fade_samples)
            fade_out = np.linspace(1, 0, fade_samples)
            final_audio = np.zeros(len(original_waveform) + chunk_samples)
            current_pos = 0
            
            for i, chunk in enumerate(processed_chunks):
                chunk_len = chunk.shape[0]
                if i == 0:
                    final_audio[current_pos : current_pos + chunk_len] += chunk
                else:
                    chunk[:fade_samples] *= fade_in
                    final_audio[current_pos : current_pos + fade_samples] *= fade_out
                    final_audio[current_pos : current_pos + chunk_len] += chunk
                current_pos += chunk_len - overlap_samples

            final_audio = final_audio[:len(original_waveform)]
            print("音频块拼接成功。")
            
            # --- 5. Advanced Post-processing ---
            print("\n--- 步骤 5: 后期处理 ---")
            if kwargs.get('use_clipping', False):
                final_audio = self._frequency_clipping(original_waveform.cpu().numpy(), final_audio, sr, 12000)
            if kwargs.get('use_filter', False):
                print("正在应用 12kHz 低通滤波器...")
                b, a = butter(8, 12000, btype='low', fs=sr)
                final_audio = lfilter(b, a, final_audio)
            
            max_abs_val = np.max(np.abs(final_audio))
            if max_abs_val > 0:
                print("正在标准化音频...")
                final_audio /= max_abs_val
            else:
                print("警告: 生成的音频为完全静音，跳过标准化。")

            
            # --- 6. Save Outputs ---
            print("\n--- 步骤 6: 保存输出 ---")
            basename = os.path.splitext(os.path.basename(audio_path))[0]
            output_path = os.path.join(output_dir, f"{basename}_enhanced.wav")
            sf.write(output_path, final_audio, sr)
            print(f"增强后的音频已保存至: {output_path}")
            spec_path = os.path.join(output_dir, f"{basename}_spectrogram.png")
            self._save_spectrogram(final_audio, sr, spec_path)
            print(f"频谱图已保存至: {spec_path}")

            return output_path, spec_path
        except Exception as e:
            raise e

    def _frequency_clipping(self, original_wav, generated_wav, sr, cutoff_freq):
        """Combines low frequencies from original audio with high frequencies from generated audio."""
        print(f"正在应用频率剪切，分界线为 {cutoff_freq} Hz...")
        n_fft = self.cfg['data']['n_fft']
        min_len = min(len(original_wav), len(generated_wav))
        original_wav, generated_wav = original_wav[:min_len], generated_wav[:min_len]

        stft_orig = torch.stft(torch.from_numpy(original_wav), n_fft=n_fft, return_complex=True)
        stft_gen = torch.stft(torch.from_numpy(generated_wav), n_fft=n_fft, return_complex=True)
        
        freq_bins = torch.fft.rfftfreq(n_fft, d=1.0/sr)
        cutoff_bin = torch.where(freq_bins >= cutoff_freq)[0][0]
        
        mask = torch.ones_like(stft_orig)
        mask[cutoff_bin:, :] = 0
        
        stft_combined = stft_orig * mask + stft_gen * (1 - mask)
        
        combined_wav = torch.istft(stft_combined, n_fft=n_fft)
        return combined_wav.cpu().numpy()

    def _save_spectrogram(self, waveform, sr, save_path):
        """Generates and saves a spectrogram plot."""
        fig = Figure(figsize=(12, 6), dpi=100)
        ax = fig.add_subplot(111)
        ax.specgram(waveform, Fs=sr, cmap='viridis', NFFT=1024, noverlap=512)
        ax.set_title("Generated Audio Spectrogram")
        ax.set_xlabel("Time (s)")
        ax.set_ylabel("Frequency (Hz)")
        fig.tight_layout()
        fig.savefig(save_path)
        plt.close(fig)

# =====================================================================================
# SECTION: TKINTER GUI APPLICATION
# =====================================================================================

class App(tk.Tk):
    def __init__(self, cfg_path):
        super().__init__()
        self.title("Audio Super-Resolution Inference GUI")
        self.geometry("800x700")
        
        self.cfg_path = cfg_path
        self.engine = None
        self.thread = None

        # --- Style ---
        style = ttk.Style(self)
        style.theme_use('clam')
        style.configure('TButton', font=('Helvetica', 10))
        style.configure('Accent.TButton', font=('Helvetica', 12, 'bold'), foreground='white', background='#0078D7')
        style.configure('TLabel', font=('Helvetica', 10))
        style.configure('TFrame', padding=10)

        # --- Variables ---
        self.input_audio_var = tk.StringVar(value="/home/husrcf/Code/Python/ProjectLily_Z_III/data/valid/low/02 - 共同渡过.wav")
        self.weights_path_var = tk.StringVar(value="/home/husrcf/Code/Python/ProjectLily_Z_III/outputs/audiosr_ldm_train/checkpoints/step_30000.pt")
        self.output_dir_var = tk.StringVar(value="/home/husrcf/Code/Python/ProjectLily_Z_III/output")
        self.use_filter_var = tk.BooleanVar(value=True)
        self.use_clipping_var = tk.BooleanVar(value=True)
        self.ddim_steps_var = tk.IntVar(value=100)

        # --- Main Frame ---
        main_frame = ttk.Frame(self)
        main_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)

        self._create_io_widgets(main_frame)
        self._create_options_widgets(main_frame)
        self._create_control_widgets(main_frame)
        self._create_output_widgets(main_frame)

    def _create_io_widgets(self, parent):
        io_frame = ttk.LabelFrame(parent, text="输入 / 输出", padding=15)
        io_frame.pack(fill=tk.X, pady=5)

        ttk.Label(io_frame, text="输入音频:").grid(row=0, column=0, sticky='w', padx=5, pady=5)
        ttk.Entry(io_frame, textvariable=self.input_audio_var, width=60).grid(row=0, column=1, sticky='ew', padx=5)
        ttk.Button(io_frame, text="浏览...", command=lambda: self._browse_file(self.input_audio_var, "选择音频文件", (("Audio Files", "*.wav *.flac *.mp3"), ("All files", "*.*")))).grid(row=0, column=2, padx=5)

        ttk.Label(io_frame, text="模型权重:").grid(row=1, column=0, sticky='w', padx=5, pady=5)
        ttk.Entry(io_frame, textvariable=self.weights_path_var, width=60).grid(row=1, column=1, sticky='ew', padx=5)
        ttk.Button(io_frame, text="浏览...", command=lambda: self._browse_file(self.weights_path_var, "选择模型权重", (("PyTorch Checkpoints", "*.pt *.ckpt"), ("All files", "*.*")))).grid(row=1, column=2, padx=5)

        ttk.Label(io_frame, text="输出目录:").grid(row=2, column=0, sticky='w', padx=5, pady=5)
        ttk.Entry(io_frame, textvariable=self.output_dir_var, width=60).grid(row=2, column=1, sticky='ew', padx=5)
        ttk.Button(io_frame, text="浏览...", command=lambda: self._browse_dir(self.output_dir_var, "选择输出目录")).grid(row=2, column=2, padx=5)

        io_frame.columnconfigure(1, weight=1)

    def _create_options_widgets(self, parent):
        options_frame = ttk.LabelFrame(parent, text="推理选项", padding=15)
        options_frame.pack(fill=tk.X, pady=10)
        ttk.Checkbutton(options_frame, text="对输出应用 12kHz 低通滤波器", variable=self.use_filter_var).pack(anchor='w', padx=5)
        ttk.Checkbutton(options_frame, text="使用频率剪切 (低频来自原始音频, 高频来自AI)", variable=self.use_clipping_var).pack(anchor='w', padx=5, pady=5)
        ddim_frame = ttk.Frame(options_frame)
        ddim_frame.pack(anchor='w', padx=5, pady=5)
        ttk.Label(ddim_frame, text="DDIM 步数:").pack(side=tk.LEFT)
        ttk.Spinbox(ddim_frame, from_=10, to=1000, textvariable=self.ddim_steps_var, width=8).pack(side=tk.LEFT, padx=5)

    def _create_control_widgets(self, parent):
        control_frame = ttk.Frame(parent)
        control_frame.pack(fill=tk.X, pady=10)
        self.start_button = ttk.Button(control_frame, text="开始推理", command=self.start_inference, style='Accent.TButton', padding=10)
        self.start_button.pack(pady=10)
        self.progress_bar = ttk.Progressbar(control_frame, orient='horizontal', mode='determinate')
        self.progress_bar.pack(fill=tk.X, expand=True, pady=5)
        self.status_label = ttk.Label(control_frame, text="状态: 空闲", anchor='center')
        self.status_label.pack(fill=tk.X, expand=True)

    def _create_output_widgets(self, parent):
        output_frame = ttk.LabelFrame(parent, text="输出频谱图", padding=15)
        output_frame.pack(fill=tk.BOTH, expand=True, pady=5)
        self.fig = Figure(figsize=(5, 3), dpi=100)
        self.ax = self.fig.add_subplot(111)
        self.ax.set_facecolor("#f0f0f0")
        self.ax.tick_params(axis='x', colors='gray'); self.ax.tick_params(axis='y', colors='gray')
        self.ax.spines['bottom'].set_color('gray'); self.ax.spines['top'].set_color('gray') 
        self.ax.spines['right'].set_color('gray'); self.ax.spines['left'].set_color('gray')
        self.fig.tight_layout()
        self.canvas = FigureCanvasTkAgg(self.fig, master=output_frame)
        self.canvas.get_tk_widget().pack(side=tk.TOP, fill=tk.BOTH, expand=True)

    def _browse_file(self, var, title, filetypes):
        filepath = filedialog.askopenfilename(title=title, filetypes=filetypes)
        if filepath: var.set(filepath)

    def _browse_dir(self, var, title):
        dirpath = filedialog.askdirectory(title=title)
        if dirpath: var.set(dirpath)

    def start_inference(self):
        if not self.input_audio_var.get() or not self.weights_path_var.get() or not self.output_dir_var.get():
            messagebox.showerror("错误", "请指定输入音频、模型权重和输出目录。")
            return
        os.makedirs(self.output_dir_var.get(), exist_ok=True)
        self.start_button.config(state=tk.DISABLED, text="处理中...")
        self.progress_bar['value'] = 0
        self.update_status("状态: 初始化...")
        self.thread = threading.Thread(target=self._inference_thread_func, daemon=True)
        self.thread.start()

    def _inference_thread_func(self):
        try:
            print("正在初始化引擎...")
            if self.engine is None:
                self.update_status("状态: 加载配置并实例化模型...")
                with open(self.cfg_path, 'r', encoding='utf-8') as f:
                    cfg = yaml.safe_load(f)
                self.engine = InferenceEngine(cfg)
            
            self.update_status("状态: 加载模型权重...")
            self.engine.load_weights(self.weights_path_var.get())
            
            self.update_status("状态: 开始推理流程...")
            output_path, spec_path = self.engine.run_inference(
                audio_path=self.input_audio_var.get(), output_dir=self.output_dir_var.get(),
                progress_callback=self.update_progress, use_filter=self.use_filter_var.get(),
                use_clipping=self.use_clipping_var.get(), ddim_steps=self.ddim_steps_var.get()
            )
            self.after(0, self.on_inference_complete, output_path, spec_path)
        except BaseException:
            print("在推理线程中捕获到错误。正在向主线程报告...")
            exc_info = sys.exc_info()
            self.after(0, self.on_inference_error, exc_info)

    def update_progress(self, current, total):
        self.after(0, self._update_progress_gui, current, total)

    def _update_progress_gui(self, current, total):
        self.progress_bar['maximum'] = total
        self.progress_bar['value'] = current
        self.update_status(f"状态: 正在处理块 {current} / {total}...")

    def update_status(self, text):
        self.status_label.config(text=text)

    def on_inference_complete(self, output_path, spec_path):
        self.start_button.config(state=tk.NORMAL, text="开始推理")
        self.update_status(f"状态: 完成！音频已保存至 {output_path}")
        print(f"\n✅ 推理完成！音频已保存至: {output_path}")
        messagebox.showinfo("成功", f"推理完成！\n输出已保存至:\n{output_path}")
        
        img = plt.imread(spec_path)
        self.ax.clear()
        self.ax.imshow(img, aspect='auto', origin='lower')
        self.ax.axis('off')
        self.fig.tight_layout()
        self.canvas.draw()

    def on_inference_error(self, exc_info):
        self.start_button.config(state=tk.NORMAL, text="开始推理")
        self.update_status(f"状态: 发生错误！详情请查看控制台。")
        print("\n" + "="*50)
        print("推理过程中发生错误:")
        # Use the standard traceback module to print the exception
        traceback.print_exception(exc_info[0], exc_info[1], exc_info[2])
        print("="*50 + "\n")

# =====================================================================================
# SECTION: MAIN EXECUTION
# =====================================================================================

if __name__ == '__main__':
    CFG_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'config.yaml')
    if not os.path.exists(CFG_PATH):
        tk.Tk().withdraw()
        messagebox.showerror("配置错误", f"错误: 在脚本目录中未找到 'config.yaml'。\n请确保该文件存在。")
        sys.exit(1)
    app = App(CFG_PATH)
    app.mainloop()

```

## 文件: `backpack/modelORI.py`

```python
# This file is adapted from the official AudioSR repository to be compatible with the project structure.
# It now integrates the LatentDiffusion model, VAE, and UNet in one place with the correct naming hierarchy.
#
# --- MODIFICATIONS ---
# 1. Replaced placeholder CLAP and Vocoder classes with detailed architectural skeletons
#    that precisely match the keys in the pre-trained checkpoint file.
# 2. Registered 'scale_factor' as a buffer in the main LatentDiffusion class.
# 3. Kept the custom EMA handler and the corrected UNet architecture from previous versions.
# 4. Corrected the shape of 'logit_scale_a' and 'logit_scale_t' to be scalars to match the checkpoint.
# 5. Fully built out the ClapWrapper's audio_branch with a SwinTransformer skeleton to match all checkpoint keys.
# 6. Set bias=False for STFT conv layers as per the checkpoint's structure.
# 7. Corrected a SyntaxError in the UNetModel's __init__ method.
# 8. Corrected an UnboundLocalError in the LatentDiffusion's register_schedule method.
# 9. Corrected the SwinTransformer's window_size and attn_mask shapes to match the checkpoint.
# 10. Re-instated the 'text_branch' and added the correct 'text_transform' and 'audio_transform' modules.
# 11. FINAL FIX: Added the '.sequential' nesting to the transform modules to match the checkpoint keys.

import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange
from contextlib import contextmanager
from functools import partial
import numpy as np

# --- Official AudioSR Imports (or their equivalents) ---
from audiosr.latent_diffusion.util import (
    exists,
    default,
    instantiate_from_config,
)
from audiosr.latent_diffusion.modules.distributions.distributions import (
    DiagonalGaussianDistribution,
)
from audiosr.latent_diffusion.modules.diffusionmodules.util import (
    make_beta_schedule,
    extract_into_tensor,
    noise_like,
    checkpoint,
    conv_nd,
    linear,
    zero_module,
    normalization,
    timestep_embedding,
)
from audiosr.latent_diffusion.modules.attention import SpatialTransformer
from audiosr.latent_diffusion.modules.diffusionmodules.model import Encoder, Decoder # VAE components

# --- Detailed Placeholder for Vocoder ---
class ResBlockModule(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size):
        super().__init__()
        self.convs1 = nn.ModuleList([
            nn.Conv1d(in_channels, out_channels, kernel_size, padding=kernel_size//2, dilation=1),
            nn.Conv1d(in_channels, out_channels, kernel_size, padding=kernel_size//2, dilation=3),
            nn.Conv1d(in_channels, out_channels, kernel_size, padding=kernel_size//2, dilation=5),
        ])
        self.convs2 = nn.ModuleList([
            nn.Conv1d(out_channels, out_channels, kernel_size, padding=kernel_size//2),
            nn.Conv1d(out_channels, out_channels, kernel_size, padding=kernel_size//2),
            nn.Conv1d(out_channels, out_channels, kernel_size, padding=kernel_size//2),
        ])

class Vocoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv_pre = nn.Conv1d(256, 1536, 7, 1, padding=3)
        self.ups = nn.ModuleList([
            nn.ConvTranspose1d(1536, 768, 12, 6, padding=3),
            nn.ConvTranspose1d(768, 384, 10, 5, padding=2, output_padding=1),
            nn.ConvTranspose1d(384, 192, 8, 4, padding=2),
            nn.ConvTranspose1d(192, 96, 4, 2, padding=1),
            nn.ConvTranspose1d(96, 48, 4, 2, padding=1),
        ])
        self.resblocks = nn.ModuleList([
            ResBlockModule(768, 768, 3), ResBlockModule(768, 768, 7), ResBlockModule(768, 768, 11), ResBlockModule(768, 768, 15),
            ResBlockModule(384, 384, 3), ResBlockModule(384, 384, 7), ResBlockModule(384, 384, 11), ResBlockModule(384, 384, 15),
            ResBlockModule(192, 192, 3), ResBlockModule(192, 192, 7), ResBlockModule(192, 192, 11), ResBlockModule(192, 192, 15),
            ResBlockModule(96, 96, 3), ResBlockModule(96, 96, 7), ResBlockModule(96, 96, 11), ResBlockModule(96, 96, 15),
            ResBlockModule(48, 48, 3), ResBlockModule(48, 48, 7), ResBlockModule(48, 48, 11), ResBlockModule(48, 48, 15),
        ])
        self.conv_post = nn.Conv1d(48, 1, 7, 1, padding=3)

# --- Detailed Placeholders for CLAP ---
class Mlp(nn.Module):
    def __init__(self, in_features, hidden_features, out_features, act_layer=nn.GELU, drop=0.):
        super().__init__()
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

class WindowAttention(nn.Module):
    def __init__(self, dim, num_heads):
        super().__init__()
        self.qkv = nn.Linear(dim, dim * 3, bias=True)
        self.proj = nn.Linear(dim, dim)
        window_size = 8
        self.relative_position_bias_table = nn.Parameter(
            torch.zeros((2 * window_size - 1) * (2 * window_size - 1), num_heads))
        self.relative_position_index = nn.Parameter(torch.zeros(64, 64), requires_grad=False)

class SwinTransformerBlock(nn.Module):
    def __init__(self, dim, num_heads):
        super().__init__()
        self.norm1 = nn.LayerNorm(dim)
        self.attn = WindowAttention(dim, num_heads)
        self.norm2 = nn.LayerNorm(dim)
        self.mlp = Mlp(dim, dim * 4, dim)
        self.register_parameter("attn_mask", None)

class PatchMerging(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.reduction = nn.Linear(4 * in_channels, out_channels, bias=False)
        self.norm = nn.LayerNorm(4 * in_channels)

class BasicLayer(nn.Module):
    def __init__(self, dim, out_dim, num_blocks, num_heads, mask_shape=None):
        super().__init__()
        self.blocks = nn.ModuleList([SwinTransformerBlock(dim, num_heads) for _ in range(num_blocks)])
        if mask_shape is not None:
            for i in range(num_blocks):
                if i % 2 != 0:
                    self.blocks[i].attn_mask = nn.Parameter(torch.zeros(mask_shape), requires_grad=False)
        if out_dim is not None:
            self.downsample = PatchMerging(dim, out_dim)
        else:
            self.downsample = None

class PatchEmbed(nn.Module):
    def __init__(self):
        super().__init__()
        self.proj = nn.Conv2d(1, 128, kernel_size=4, stride=4)
        self.norm = nn.LayerNorm(128)

class ClapAudioBranch(nn.Module):
    def __init__(self):
        super().__init__()
        self.spectrogram_extractor = nn.Module()
        self.spectrogram_extractor.stft = nn.Module()
        self.spectrogram_extractor.stft.conv_real = nn.Conv1d(1, 513, 1024, bias=False)
        self.spectrogram_extractor.stft.conv_imag = nn.Conv1d(1, 513, 1024, bias=False)
        self.logmel_extractor = nn.Module()
        self.logmel_extractor.melW = nn.Parameter(torch.zeros(513, 64))
        self.bn0 = nn.BatchNorm1d(64)
        self.patch_embed = PatchEmbed()
        self.layers = nn.ModuleList([
            BasicLayer(128, 256, 2, 4, mask_shape=(64, 64, 64)),
            BasicLayer(256, 512, 2, 8, mask_shape=(16, 64, 64)),
            BasicLayer(512, 1024, 12, 16, mask_shape=(4, 64, 64)),
            BasicLayer(1024, None, 2, 32) 
        ])
        self.norm = nn.LayerNorm(1024)
        self.tscam_conv = nn.Conv2d(1024, 527, kernel_size=(2,3), padding=(0,1))
        self.head = nn.Linear(527, 527)

class BertEmbeddings(nn.Module):
    def __init__(self):
        super().__init__()
        self.word_embeddings = nn.Embedding(50265, 768)
        self.position_embeddings = nn.Embedding(514, 768)
        self.token_type_embeddings = nn.Embedding(1, 768)
        self.LayerNorm = nn.LayerNorm(768, eps=1e-12)
        self.register_buffer("position_ids", torch.arange(514).expand((1, -1)))

class BertSelfAttention(nn.Module):
    def __init__(self):
        super().__init__()
        self.query = nn.Linear(768, 768)
        self.key = nn.Linear(768, 768)
        self.value = nn.Linear(768, 768)

class BertAttention(nn.Module):
    def __init__(self):
        super().__init__()
        self.self = BertSelfAttention()
        self.output = nn.Module()
        self.output.dense = nn.Linear(768, 768)
        self.output.LayerNorm = nn.LayerNorm(768, eps=1e-12)

class BertIntermediate(nn.Module):
    def __init__(self):
        super().__init__()
        self.dense = nn.Linear(768, 3072)

class BertOutput(nn.Module):
    def __init__(self):
        super().__init__()
        self.dense = nn.Linear(3072, 768)
        self.LayerNorm = nn.LayerNorm(768, eps=1e-12)

class BertLayer(nn.Module):
    def __init__(self):
        super().__init__()
        self.attention = BertAttention()
        self.intermediate = BertIntermediate()
        self.output = BertOutput()

class BertEncoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer = nn.ModuleList([BertLayer() for _ in range(12)])

class BertPooler(nn.Module):
    def __init__(self):
        super().__init__()
        self.dense = nn.Linear(768, 768)

class ClapTextBranch(nn.Module):
    def __init__(self):
        super().__init__()
        self.embeddings = BertEmbeddings()
        self.encoder = BertEncoder()
        self.pooler = BertPooler()

class ClapWrapper(nn.Module):
    def __init__(self):
        super().__init__()
        self.model = nn.Module()
        self.model.logit_scale_a = nn.Parameter(torch.tensor(0.0))
        self.model.logit_scale_t = nn.Parameter(torch.tensor(0.0))
        self.model.audio_branch = ClapAudioBranch()
        self.model.text_branch = ClapTextBranch()
        
        self.model.text_projection = nn.Sequential(nn.Linear(768, 512), nn.ReLU(), nn.Linear(512, 512))
        self.model.audio_projection = nn.Sequential(nn.Linear(1024, 512), nn.ReLU(), nn.Linear(512, 512))
        
        # --- FIX: Added the '.sequential' nesting to match the checkpoint keys ---
        self.model.text_transform = nn.Module()
        self.model.text_transform.sequential = nn.Sequential(
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.GELU(),
            nn.Linear(512, 512)
        )
        self.model.audio_transform = nn.Module()
        self.model.audio_transform.sequential = nn.Sequential(
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.GELU(),
            nn.Linear(512, 512)
        )
        
        self.mel_transform = nn.Module()
        self.mel_transform.spectrogram = nn.Module()
        self.mel_transform.spectrogram.window = nn.Parameter(torch.zeros(1024))
        self.mel_transform.mel_scale = nn.Module()
        self.mel_transform.mel_scale.fb = nn.Parameter(torch.zeros(513, 64))

# --- Custom EMA Handler ---
class CustomLitEma(nn.Module):
    def __init__(self, model, decay=0.9999, use_num_updates=True):
        super().__init__()
        if decay < 0.0 or decay > 1.0:
            raise ValueError('Decay must be between 0 and 1')
        self.m_name2s_name = {name: name.replace('.', '') for name, p in model.named_parameters()}
        self.register_buffer('decay', torch.tensor(decay, dtype=torch.float32))
        self.register_buffer('num_updates', torch.tensor(0, dtype=torch.int64) if use_num_updates else torch.tensor(-1, dtype=torch.int64))
        for name, p in model.named_parameters():
            if p.requires_grad:
                s_name = self.m_name2s_name[name]
                self.register_buffer(s_name, p.clone().detach().data)
        self.collected_params = []
    @torch.no_grad()
    def forward(self, model):
        self.num_updates += 1
        for name, p in model.named_parameters():
            if p.requires_grad:
                s_name = self.m_name2s_name[name]
                s_param = self.get_buffer(s_name)
                s_param.sub_((1 - self.decay) * (s_param - p.data))
    def store(self, params): self.collected_params = [p.clone() for p in params]
    def restore(self, params):
        for p_old, p_new in zip(params, self.collected_params): p_old.data.copy_(p_new.data)
        self.collected_params = []
    def copy_to(self, model):
        for name, p in model.named_parameters():
            if p.requires_grad: p.data.copy_(self.get_buffer(self.m_name2s_name[name]))

# --- Building Blocks for UNet (ResBlock, Upsample, etc.) ---
class TimestepBlock(nn.Module):
    @staticmethod
    def forward(x, emb): raise NotImplementedError
class TimestepEmbedSequential(nn.Sequential, TimestepBlock):
    def forward(self, x, emb, context=None):
        for layer in self:
            if isinstance(layer, TimestepBlock): x = layer(x, emb)
            elif isinstance(layer, SpatialTransformer): x = layer(x, context)
            else: x = layer(x)
        return x
class Upsample(nn.Module):
    def __init__(self, channels, use_conv, dims=2, out_channels=None):
        super().__init__()
        self.channels, self.out_channels, self.use_conv, self.dims = channels, out_channels or channels, use_conv, dims
        if use_conv: self.conv = conv_nd(dims, self.channels, self.out_channels, 3, padding=1)
    def forward(self, x):
        assert x.shape[1] == self.channels
        x = F.interpolate(x, scale_factor=2, mode="nearest")
        if self.use_conv: x = self.conv(x)
        return x
class Downsample(nn.Module):
    def __init__(self, channels, use_conv, dims=2, out_channels=None):
        super().__init__()
        self.channels, self.out_channels, self.use_conv, self.dims = channels, out_channels or channels, use_conv, dims
        stride = 2
        if use_conv: self.op = conv_nd(dims, self.channels, self.out_channels, 3, stride=stride, padding=1)
        else: self.op = nn.AvgPool2d(kernel_size=stride, stride=stride)
    def forward(self, x):
        assert x.shape[1] == self.channels
        return self.op(x)
class ResBlock(TimestepBlock):
    def __init__(self, channels, emb_channels, dropout, out_channels=None, dims=2, use_checkpoint=False, use_scale_shift_norm=False):
        super().__init__()
        self.channels, self.emb_channels, self.dropout, self.out_channels, self.use_checkpoint, self.use_scale_shift_norm = channels, emb_channels, dropout, out_channels or channels, use_checkpoint, use_scale_shift_norm
        self.in_layers = nn.Sequential(normalization(channels), nn.SiLU(), conv_nd(dims, channels, self.out_channels, 3, padding=1))
        self.emb_layers = nn.Sequential(nn.SiLU(), linear(emb_channels, 2 * self.out_channels if use_scale_shift_norm else self.out_channels))
        self.out_layers = nn.Sequential(normalization(self.out_channels), nn.SiLU(), nn.Dropout(p=dropout), zero_module(conv_nd(dims, self.out_channels, self.out_channels, 3, padding=1)))
        self.skip_connection = nn.Identity() if self.out_channels == channels else conv_nd(dims, channels, self.out_channels, 1)
    def forward(self, x, emb): return checkpoint(self._forward, (x, emb), self.parameters(), self.use_checkpoint)
    def _forward(self, x, emb):
        h = self.in_layers(x)
        emb_out = self.emb_layers(emb).type(h.dtype)
        while len(emb_out.shape) < len(h.shape): emb_out = emb_out[..., None]
        if self.use_scale_shift_norm:
            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]
            scale, shift = torch.chunk(emb_out, 2, dim=1)
            h = out_norm(h) * (1 + scale) + shift
            h = out_rest(h)
        else:
            h = h + emb_out
            h = self.out_layers(h)
        return self.skip_connection(x) + h

# --- Main U-Net Model (Corrected Architecture) ---
class UNetModel(nn.Module):
    def __init__(self, image_size, in_channels, model_channels, out_channels, num_res_blocks, attention_resolutions, dropout=0, channel_mult=(1, 2, 4, 8), conv_resample=True, dims=2, num_classes=None, use_checkpoint=False, num_heads=-1, num_head_channels=-1, use_scale_shift_norm=False, resblock_updown=False, use_spatial_transformer=False, transformer_depth=1, context_dim=None, **kwargs):
        super().__init__()
        self.image_size, self.in_channels, self.model_channels, self.out_channels, self.num_res_blocks, self.attention_resolutions, self.dropout, self.channel_mult, self.conv_resample, self.num_classes, self.use_checkpoint, self.num_heads, self.num_head_channels = image_size, in_channels, model_channels, out_channels, num_res_blocks, attention_resolutions, dropout, channel_mult, conv_resample, num_classes, use_checkpoint, num_heads, num_head_channels
        time_embed_dim = model_channels * 4
        self.time_embed = nn.Sequential(linear(model_channels, time_embed_dim), nn.SiLU(), linear(time_embed_dim, time_embed_dim))
        if self.num_classes is not None: self.label_emb = nn.Embedding(num_classes, time_embed_dim)
        self.input_blocks = nn.ModuleList([TimestepEmbedSequential(conv_nd(dims, in_channels, model_channels, 3, padding=1))])
        self._feature_size, input_block_chans, ch, ds = model_channels, [model_channels], model_channels, 1
        for level, mult in enumerate(channel_mult):
            for i in range(num_res_blocks):
                layers = [ResBlock(ch, time_embed_dim, dropout, out_channels=mult * model_channels, dims=dims, use_checkpoint=use_checkpoint, use_scale_shift_norm=use_scale_shift_norm)]
                ch = mult * model_channels
                if ds in attention_resolutions:
                    num_heads = ch // num_head_channels
                    dim_head = num_head_channels
                    layers.append(SpatialTransformer(ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim))
                    layers.append(SpatialTransformer(ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim))
                self.input_blocks.append(TimestepEmbedSequential(*layers))
                self._feature_size += ch
                input_block_chans.append(ch)
            if level != len(channel_mult) - 1:
                out_ch = ch
                self.input_blocks.append(
                    TimestepEmbedSequential(
                        Downsample(ch, conv_resample, dims=dims, out_channels=out_ch)
                    )
                )
                ch = out_ch
                input_block_chans.append(ch)
                ds *= 2
                self._feature_size += ch
        
        num_heads = ch // num_head_channels
        dim_head = num_head_channels
        self.middle_block = TimestepEmbedSequential(ResBlock(ch, time_embed_dim, dropout, dims=dims, use_checkpoint=use_checkpoint, use_scale_shift_norm=use_scale_shift_norm), SpatialTransformer(ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim), SpatialTransformer(ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim), ResBlock(ch, time_embed_dim, dropout, dims=dims, use_checkpoint=use_checkpoint, use_scale_shift_norm=use_scale_shift_norm))
        self._feature_size += ch
        self.output_blocks = nn.ModuleList([])
        for level, mult in list(enumerate(channel_mult))[::-1]:
            for i in range(num_res_blocks + 1):
                ich = input_block_chans.pop()
                layers = [ResBlock(ch + ich, time_embed_dim, dropout, out_channels=model_channels * mult, dims=dims, use_checkpoint=use_checkpoint, use_scale_shift_norm=use_scale_shift_norm)]
                ch = model_channels * mult
                if ds in attention_resolutions:
                    num_heads = ch // num_head_channels
                    dim_head = num_head_channels
                    layers.append(SpatialTransformer(ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim))
                    layers.append(SpatialTransformer(ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim))
                if level and i == num_res_blocks:
                    out_ch = ch
                    layers.append(Upsample(ch, conv_resample, dims=dims, out_channels=out_ch))
                    ds //= 2
                self.output_blocks.append(TimestepEmbedSequential(*layers))
                self._feature_size += ch
        self.out = nn.Sequential(normalization(ch), nn.SiLU(), zero_module(conv_nd(dims, model_channels, out_channels, 3, padding=1)))
    def forward(self, x, timesteps, context=None, y=None, **kwargs):
        hs, t_emb = [], timestep_embedding(timesteps, self.model_channels, repeat_only=False)
        emb = self.time_embed(t_emb)
        if self.num_classes is not None: emb = emb + self.label_emb(y)
        h = x
        for module in self.input_blocks:
            h = module(h, emb, context)
            hs.append(h)
        h = self.middle_block(h, emb, context)
        for module in self.output_blocks:
            h = module(torch.cat([h, hs.pop()], dim=1), emb, context)
        return self.out(h)

# --- VAE and Conditioner definitions ---
class AutoencoderKL(nn.Module):
    def __init__(self, ddconfig, embed_dim, ckpt_path=None, ignore_keys=[]):
        super().__init__()
        self.encoder, self.decoder, self.vocoder = Encoder(**ddconfig), Decoder(**ddconfig), Vocoder()
        assert ddconfig["double_z"]
        self.quant_conv = torch.nn.Conv2d(2 * ddconfig["z_channels"], 2 * embed_dim, 1)
        self.post_quant_conv = torch.nn.Conv2d(embed_dim, ddconfig["z_channels"], 1)
        self.embed_dim = embed_dim
    def encode(self, x): return DiagonalGaussianDistribution(self.quant_conv(self.encoder(x)))
    def decode(self, z): return self.decoder(self.post_quant_conv(z))
class VAEFeatureExtract(nn.Module):
    def __init__(self, first_stage_config):
        super().__init__()
        self.vae = instantiate_from_config(first_stage_config)
        self.vae.eval()
        for p in self.vae.parameters(): p.requires_grad = False
    def forward(self, batch):
        with torch.no_grad(): vae_embed = self.vae.encode(batch).sample()
        return vae_embed.detach()

# --- Main LDM Class ---
class LatentDiffusion(nn.Module):
    def __init__(self, first_stage_config, cond_stage_config, unet_config, beta_schedule="linear", timesteps=1000, loss_type="l2", parameterization="v", scale_factor=1.0, scale_by_std=False, use_ema=True, **kwargs):
        super().__init__()
        self.scale_by_std, self.parameterization, self.model = scale_by_std, parameterization, DiffusionWrapper(unet_config)
        if use_ema: self.model_ema = CustomLitEma(self.model)
        self.first_stage_model = instantiate_from_config(first_stage_config)
        self.cond_stage_models, self.cond_stage_model_metadata = nn.ModuleList(), {}
        self.instantiate_cond_stage(cond_stage_config)
        self.clap = ClapWrapper()
        self.register_schedule(beta_schedule=beta_schedule, timesteps=timesteps, scale_factor=scale_factor)
        self.loss_type = loss_type
    def register_schedule(self, beta_schedule, timesteps, linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3, scale_factor=1.0):
        self.register_buffer('scale_factor', torch.tensor(scale_factor))
        betas = make_beta_schedule(beta_schedule, timesteps, linear_start=linear_start, linear_end=linear_end, cosine_s=cosine_s)
        alphas = 1. - betas
        alphas_cumprod = np.cumprod(alphas, axis=0)
        
        # --- FIX: Clip alphas_cumprod to prevent log(0) and division by zero warnings ---
        alphas_cumprod = np.clip(alphas_cumprod, a_min=0.0, a_max=1.0 - 1e-8)

        alphas_cumprod_prev = np.append(1., alphas_cumprod[:-1])
        to_torch = partial(torch.tensor, dtype=torch.float32)
        self.num_timesteps = int(timesteps)
        self.register_buffer('betas', to_torch(betas)); self.register_buffer('alphas_cumprod', to_torch(alphas_cumprod)); self.register_buffer('alphas_cumprod_prev', to_torch(alphas_cumprod_prev))
        self.register_buffer('sqrt_alphas_cumprod', to_torch(np.sqrt(alphas_cumprod))); self.register_buffer('sqrt_one_minus_alphas_cumprod', to_torch(np.sqrt(1. - alphas_cumprod)))
        self.register_buffer('log_one_minus_alphas_cumprod', to_torch(np.log(1. - alphas_cumprod))); self.register_buffer('sqrt_recip_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod)))
        self.register_buffer('sqrt_recipm1_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod - 1)))
        posterior_variance = (1 - self.scale_by_std) * betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)
        self.register_buffer('posterior_variance', to_torch(posterior_variance)); self.register_buffer('posterior_log_variance_clipped', to_torch(np.log(np.maximum(posterior_variance, 1e-20))))
        self.register_buffer('posterior_mean_coef1', to_torch(betas * np.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod)))
        self.register_buffer('posterior_mean_coef2', to_torch((1. - alphas_cumprod_prev) * np.sqrt(alphas) / (1. - alphas_cumprod)))
        self.register_buffer('logvar', torch.zeros(timesteps))
    def instantiate_cond_stage(self, config):
        for key, cfg in config.items():
            model = instantiate_from_config(cfg)
            self.cond_stage_models.append(model)
            self.cond_stage_model_metadata[key] = {"model_idx": len(self.cond_stage_models) - 1, "cond_stage_key": cfg.get("cond_stage_key"), "conditioning_key": cfg.get("conditioning_key")}
    @contextmanager
    def ema_scope(self, context=None):
        if hasattr(self, 'model_ema'):
            self.model_ema.store([p.data for p in self.model.parameters()]); self.model_ema.copy_to(self.model)
        try: yield None
        finally:
            if hasattr(self, 'model_ema'): self.model_ema.restore([p.data for p in self.model.parameters()])
    @torch.no_grad()
    def get_input(self, batch):
        x, c_concat_data = batch["fbank"], batch["lowpass_mel"]
        z = self.get_first_stage_encoding(self.first_stage_model.encode(x))
        cond = {}
        for key, meta in self.cond_stage_model_metadata.items():
            if meta["conditioning_key"] == "concat": cond[key] = self.cond_stage_models[meta["model_idx"]](c_concat_data)
        return z, cond
    def get_first_stage_encoding(self, encoder_posterior):
        z = encoder_posterior.sample() if isinstance(encoder_posterior, DiagonalGaussianDistribution) else encoder_posterior
        return self.scale_factor * z
    def q_sample(self, x_start, t, noise=None):
        noise = default(noise, lambda: torch.randn_like(x_start))
        return (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start + extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise)
    def get_v(self, x, noise, t): return (extract_into_tensor(self.sqrt_alphas_cumprod, t, x.shape) * noise - extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x.shape) * x)
    def apply_model(self, x_noisy, t, cond): return self.model(x_noisy, t, cond)
    def get_loss(self, pred, target):
        if self.loss_type == 'l1': return F.l1_loss(pred, target)
        elif self.loss_type == 'l2': return F.mse_loss(pred, target)
        else: raise NotImplementedError()
    def p_losses(self, x_start, cond, t, noise=None):
        noise = default(noise, lambda: torch.randn_like(x_start))
        x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)
        model_output = self.apply_model(x_noisy, t, cond)
        target = self.get_v(x_start, noise, t) if self.parameterization == "v" else noise
        loss = self.get_loss(model_output, target)
        return loss, {"loss": loss}
    def forward(self, batch):
        z, c = self.get_input(batch)
        t = torch.randint(0, self.num_timesteps, (z.shape[0],), device=z.device).long()
        return self.p_losses(z, c, t)
class DiffusionWrapper(nn.Module):
    def __init__(self, unet_config):
        super().__init__()
        self.diffusion_model = instantiate_from_config(unet_config)
    def forward(self, x, t, cond_dict={}):
        xc, context = x, None
        for key, value in cond_dict.items():
            if "concat" in key: xc = torch.cat([x, value], dim=1)
        return self.diffusion_model(xc, t, context=context)
```

## 文件: `backpack/dealer.py`

```python
import tkinter as tk
from tkinter import filedialog, messagebox, ttk
import os
from pydub import AudioSegment
import threading
import multiprocessing
import queue
import time
from tqdm import tqdm

# --- Core Audio Processing Logic (for worker processes) ---

def worker_process_folder(task_args):
    """
    This function runs in a separate process. It processes a single folder and
    displays its own tqdm progress bar.

    Args:
        task_args (tuple): A tuple containing (input_folder, output_file, position).
                           'position' is for placing the tqdm bar correctly.

    Returns:
        tuple: A tuple containing (status, message).
    """
    input_folder, output_file, position = task_args
    folder_name = os.path.basename(input_folder)
    pid = os.getpid()
    
    supported_formats = ('.wav', '.flac')
    
    try:
        audio_files = sorted(
            [f for f in os.listdir(input_folder) if f.lower().endswith(supported_formats)]
        )

        if not audio_files:
            return ('SKIPPED', f"文件夹 '{folder_name}' 中无音频文件。")

        combined_audio = None
        
        # Create a tqdm progress bar for this specific worker process
        progress_bar = tqdm(
            total=len(audio_files),
            desc=f"PID {pid} | {folder_name[:25]:<25}", # Truncate and pad folder name
            position=position + 1, # Position 0 is for the main progress bar
            leave=False,
            bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt}'
        )

        for filename in audio_files:
            filepath = os.path.join(input_folder, filename)
            file_format = os.path.splitext(filename)[1][1:].lower()
            try:
                audio_segment = AudioSegment.from_file(filepath, format=file_format)
                if combined_audio is None:
                    combined_audio = audio_segment
                else:
                    combined_audio += audio_segment
            except Exception:
                continue
            finally:
                progress_bar.update(1)
        
        progress_bar.close()

        if combined_audio:
            combined_audio.export(output_file, format="wav")
            result = ('SUCCESS', f"已处理 '{folder_name}' -> {os.path.basename(output_file)}")
        else:
            result = ('FAILURE', f"未能从 '{folder_name}' 加载任何音频。")

    except Exception as e:
        result = ('ERROR', f"处理 '{folder_name}' 时出错: {e}")

    return result


# --- Process Management (runs in a separate thread) ---

def start_processing_manager(is_batch_mode, input_path, output_path, update_queue):
    """
    Manages the processing. For batch mode, it sets up a multiprocessing Pool
    and a main tqdm progress bar.
    """
    print("\n" + "="*80)
    print("--- [Manager] Processing Manager Thread Started ---")
    start_time = time.time()

    if not is_batch_mode:
        print(f"--- [Manager] Running in Single Folder Mode for: {input_path}")
        # Single mode doesn't need a worker position
        status, message = worker_process_folder((input_path, output_path, 0))
        if status == 'SUCCESS':
            update_queue.put(('STATUS', f"成功！音频已拼接并保存至: {output_path}"))
            update_queue.put(('DONE_SINGLE', output_path))
        else:
            update_queue.put(('STATUS', f"错误: {message}"))
            update_queue.put(('DONE_ERROR', message))
    else:
        print("--- [Manager] Running in Batch Processing Mode ---")
        update_queue.put(('STATUS', "正在扫描子文件夹..."))
        
        raw_tasks = []
        for subdir, _, _ in os.walk(input_path):
            if subdir == input_path: continue
            if any(f.lower().endswith(('.wav', '.flac')) for f in os.listdir(subdir)):
                folder_name = os.path.basename(subdir)
                output_filename = f"{folder_name}.wav"
                output_filepath = os.path.join(output_path, output_filename)
                raw_tasks.append((subdir, output_filepath))
        
        if not raw_tasks:
            update_queue.put(('STATUS', "错误: 未在任何子文件夹中找到可处理的音频文件。"))
            update_queue.put(('DONE_ERROR', "无可处理文件")); return

        num_processes = multiprocessing.cpu_count()
        # Assign a position to each task for its progress bar
        tasks_with_pos = [(task[0], task[1], i % num_processes) for i, task in enumerate(raw_tasks)]
        
        print(f"--- [Manager] Found {len(tasks_with_pos)} tasks. Creating a Pool with {num_processes} workers. ---")
        update_queue.put(('MAX_PROGRESS', len(tasks_with_pos)))
        update_queue.put(('STATUS', f"开始使用 {num_processes} 个CPU核心进行并行处理..."))
        
        results = []
        # Create a main progress bar for overall progress
        with tqdm(total=len(tasks_with_pos), desc="Overall Progress", position=0) as main_pbar:
            with multiprocessing.Pool(processes=num_processes) as pool:
                # Use imap_unordered to get results as they complete
                for result in pool.imap_unordered(worker_process_folder, tasks_with_pos):
                    results.append(result)
                    main_pbar.update(1)
        
        print("\n--- [Manager] All worker processes have finished. Final Summary: ---")
        for res in results: print(f"  - Status: {res[0]}, Details: {res[1]}")
        
        success_count = sum(1 for r in results if r[0] == 'SUCCESS')
        update_queue.put(('FINAL_RESULTS', (success_count, len(tasks_with_pos), output_path)))

    end_time = time.time()
    print(f"--- [Manager] Processing Finished. Total time: {end_time - start_time:.2f} seconds. ---")
    print("="*80 + "\n")


# --- GUI Application Class ---

class AudioSplicerApp:
    def __init__(self, root):
        self.root = root
        self.root.title("音频拼接工具 (多进程加速版)")
        self.root.geometry("600x450")
        self.root.resizable(False, False)

        # Variables
        self.input_folder_path = tk.StringVar(value="尚未选择文件夹")
        self.output_path = tk.StringVar(value="尚未选择输出路径")
        self.batch_mode = tk.BooleanVar(value=False)
        self.status_var = tk.StringVar(value="欢迎使用音频拼接工具！")
        self.cpu_count = multiprocessing.cpu_count()
        self.update_queue = queue.Queue()

        # --- GUI Layout ---
        main_frame = tk.Frame(root, padx=15, pady=15)
        main_frame.pack(fill=tk.BOTH, expand=True)

        # Input/Output frames
        self.create_io_widgets(main_frame)

        # Batch mode checkbox and CPU info
        batch_frame = tk.Frame(main_frame)
        batch_frame.pack(fill=tk.X, pady=10)
        batch_check = tk.Checkbutton(batch_frame, text="启用批量处理模式 (并行处理子文件夹)", variable=self.batch_mode, command=self.toggle_batch_mode)
        batch_check.pack(side=tk.LEFT)
        self.cpu_label = tk.Label(batch_frame, text=f"CPU核心数: {self.cpu_count}", fg="grey")
        self.cpu_label.pack(side=tk.RIGHT)
        
        # Progress Bar
        self.progress_bar = ttk.Progressbar(main_frame, orient='horizontal', mode='determinate')
        self.progress_bar.pack(fill=tk.X, pady=5)

        # Start Button
        self.start_button = tk.Button(main_frame, text="开始拼接", command=self.start_splicing_thread, font=("Helvetica", 12, "bold"), bg="#4CAF50", fg="white")
        self.start_button.pack(pady=20, ipadx=20, ipady=5)

        # Status Bar
        status_bar = tk.Label(root, textvariable=self.status_var, bd=1, relief=tk.SUNKEN, anchor=tk.W, padx=10)
        status_bar.pack(side=tk.BOTTOM, fill=tk.X)

    def create_io_widgets(self, parent):
        input_frame = tk.Frame(parent)
        input_frame.pack(fill=tk.X, pady=5)
        tk.Button(input_frame, text="选择根文件夹", command=self.select_input_folder, width=15).pack(side=tk.LEFT, padx=(0, 10))
        tk.Label(input_frame, textvariable=self.input_folder_path, relief=tk.SUNKEN, bg="white", anchor="w").pack(side=tk.LEFT, fill=tk.X, expand=True)

        output_frame = tk.Frame(parent)
        output_frame.pack(fill=tk.X, pady=5)
        self.output_btn = tk.Button(output_frame, text="选择输出文件", command=self.select_output_path, width=15)
        self.output_btn.pack(side=tk.LEFT, padx=(0, 10))
        tk.Label(output_frame, textvariable=self.output_path, relief=tk.SUNKEN, bg="white", anchor="w").pack(side=tk.LEFT, fill=tk.X, expand=True)

    def toggle_batch_mode(self):
        if self.batch_mode.get():
            self.output_btn.config(text="选择输出文件夹")
            self.output_path.set("尚未选择输出文件夹")
            self.status_var.set("批量处理模式已启用。")
        else:
            self.output_btn.config(text="选择输出文件")
            self.output_path.set("尚未选择输出路径")
            self.status_var.set("批量处理模式已禁用。")

    def select_input_folder(self):
        folder = filedialog.askdirectory(title="选择包含音频的根文件夹")
        if folder: self.input_folder_path.set(folder)

    def select_output_path(self):
        if self.batch_mode.get():
            path = filedialog.askdirectory(title="选择保存拼接文件的文件夹")
        else:
            path = filedialog.asksaveasfilename(title="选择输出文件", defaultextension=".wav", filetypes=[("WAV files", "*.wav")])
        if path: self.output_path.set(path)

    def start_splicing_thread(self):
        input_p, output_p = self.input_folder_path.get(), self.output_path.get()
        is_batch = self.batch_mode.get()
        
        # --- Input Validation ---
        if not os.path.isdir(input_p):
            messagebox.showerror("输入错误", "请输入一个有效的根文件夹路径。"); return
        if is_batch and not os.path.isdir(output_p):
            messagebox.showerror("输出错误", "批量模式下，请选择一个有效的输出文件夹。"); return
        if not is_batch and (not output_p or "尚未选择" in output_p):
            messagebox.showerror("输出错误", "请选择一个有效的输出文件路径。"); return

        self.start_button.config(state=tk.DISABLED, text="正在处理...")
        self.progress_bar['value'] = 0

        # --- Start the manager thread ---
        self.processing_thread = threading.Thread(
            target=start_processing_manager,
            args=(is_batch, input_p, output_p, self.update_queue)
        )
        self.processing_thread.daemon = True
        self.processing_thread.start()
        
        # --- Start polling the queue for updates ---
        self.root.after(100, self.process_queue)

    def process_queue(self):
        """ Periodically checks the queue for messages from the processing thread/processes. """
        try:
            message_type, data = self.update_queue.get_nowait()
            
            if message_type == 'STATUS':
                self.status_var.set(data)
            elif message_type == 'MAX_PROGRESS':
                self.progress_bar['maximum'] = data
                self.progress_bar['value'] = 0 # Reset progress on new task
            elif message_type == 'UPDATE_PROGRESS':
                self.progress_bar['value'] = data
            elif message_type == 'DONE_SINGLE':
                self.progress_bar['value'] = 1
                self.progress_bar['maximum'] = 1
                messagebox.showinfo("完成", f"音频拼接完成！\n文件已保存到:\n{data}")
                self.reset_ui()
            elif message_type == 'DONE_ERROR':
                messagebox.showerror("错误", data)
                self.reset_ui()
            elif message_type == 'FINAL_RESULTS':
                success_count, total_tasks, output_path = data
                self.progress_bar['value'] = self.progress_bar['maximum']
                final_message = f"批量处理完成！\n成功处理了 {success_count} / {total_tasks} 个子文件夹。"
                self.status_var.set(final_message)
                messagebox.showinfo("批量完成", f"{final_message}\n文件已保存到:\n{output_path}")
                self.reset_ui()

        except queue.Empty:
            pass # No new messages
        
        if self.processing_thread.is_alive():
            # Update GUI progress bar from main thread
            if self.batch_mode.get() and self.progress_bar['maximum'] > 0:
                 # This is tricky as results come unordered. We just count completed tasks.
                 # The terminal provides the real-time feedback.
                 pass
            self.root.after(100, self.process_queue)
        else:
            if self.start_button['state'] == tk.DISABLED:
                 try:
                    # Check one last time for any remaining messages
                    while not self.update_queue.empty():
                        self.process_queue()
                 except queue.Empty:
                    self.reset_ui()

    def reset_ui(self):
        """Resets the button and progress bar to their initial state."""
        self.start_button.config(state=tk.NORMAL, text="开始拼接")
        self.progress_bar['value'] = 0


# --- Main Execution ---
if __name__ == "__main__":
    multiprocessing.freeze_support()
    
    main_root = tk.Tk()
    app = AudioSplicerApp(main_root)
    main_root.mainloop()

```

